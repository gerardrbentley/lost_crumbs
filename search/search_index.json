{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":""},{"location":"#welcome","title":"Welcome \ud83c\udf7b","text":"<p>I'm Gerard Bentley, a software engineer interested in the web, machine learning, and how both impact us. This is my blog and technical notes; feel free to poke around.</p> <p>Currently working at Sensible Weather with a focus on Golang, Python, and Postgres webservices.</p> <p>Most excited about time series data for environmental and social impact.</p> <ul> <li>Github</li> <li>Fediverse</li> <li>Linkedin if that's how you connect</li> <li>Resume</li> </ul> <p>This site is generated from Markdown files with MkDocs along with MkDocs Material.</p>"},{"location":"#some-demos","title":"Some Demos","text":"<p>Here are some side projects I worked on in the recent year or so.</p>"},{"location":"#darts-api-playground","title":"Darts API Playground","text":"<p>Explore the Datasets, Metrics, and Models of the Darts Time Series library.</p> <p>See: Github Repo</p>"},{"location":"#url-scanner","title":"URL Scanner","text":"<p>Using AWS Rekognition + Streamlit to provide interactive OCR URL Scanner / Text Extraction on real world images.</p> <p>See: Github Repo</p>"},{"location":"#wsgi-stack-vs-streamlit","title":"\ud83e\udd5e WSGI Stack vs Streamlit","text":"<p>Comparing an interactive web app built with <code>bottle</code> + <code>htmx</code> to the same idea built with <code>streamlit</code>.</p> <p>In folder <code>wsgi_comparison</code></p> <p>\ud83c\udfa5 Watch: Youtube Breakdown \u270d\ud83c\udffb Read: Blog Post</p> <p>Left: ~50 lines of Python and HTML</p> <p>Right: ~15 lines of Python</p>"},{"location":"#guitar-tuner","title":"\ud83c\udfb8 Guitar Tuner","text":"<p>Simple guitar tuner powered by <code>streamlit-webrtc</code></p>"},{"location":"#streamlit-full-stack-3-ways","title":"Streamlit Full Stack 3 Ways","text":"<p>Demo of Full Stack Streamlit Concept. Deployed with 3 increasingly complicated backends.</p> <p>See: Github Repo</p>"},{"location":"#littlest","title":"Littlest","text":""},{"location":"#postgres-version","title":"Postgres Version","text":""},{"location":"#go-backend-version","title":"Go Backend Version","text":""},{"location":"#fidelity-personal-stock-account-dashboard","title":"Fidelity / Personal Stock Account Dashboard","text":"<p>Upload a CSV export from Fidelity investment account(s) and visualize profits and losses from select tickers and accounts.</p> <p>See: Github Repo</p>"},{"location":"#pipreqs-api-streamlit-frontend","title":"\ud83d\udcbe Pipreqs API + Streamlit Frontend","text":"<p>FastAPI backend hosted on Heroku to parse repos and use <code>pipreqs</code> to spit out a minimal <code>requirements.txt</code>!</p> <p>See: Github Repo</p>"},{"location":"#lightgbm-water-pump-predictions","title":"\ud83d\udeb0 LightGBM Water Pump Predictions","text":"<p>Data Science project based on DrivenData \"Pump It Up\" competition. Includes Data Exploration, Feature Engineering, and training and predicting functionality of water pumps with LightGBM</p> <p>See: Github Repo</p>"},{"location":"#personal-spending-dashboard","title":"\ud83d\udcb0 Personal Spending Dashboard","text":"<p>Upload a CSV or excel with at least a date column and spending amount column to analyze maximum and average spending over different time periods.</p>"},{"location":"#peak-weather-nh-4000-footers","title":"Peak Weather: NH 4,000 Footers","text":"<p>Use async http request library <code>httpx</code> to make 48 api calls roughly simultaneously in one Python process. Feed a dashboard of weather for all 4,000 foot mountains in New Hampshire.</p> <p>See: Github Repo</p>"},{"location":"#pandas-power","title":"\ud83d\udc3c Pandas Power","text":"<p>Demoing useful functionalities of Pandas library in a web app.</p> <p>Currently:</p> <ul> <li><code>read_html</code>: Parse dataframes from html (url, upload, or raw copy+paste)</li> </ul>"},{"location":"#text-recognition-dataset-generator-app","title":"\u270d\ud83c\udffb Text Recognition Dataset Generator App","text":"<p>Putting a frontend on TRDG CLI tool. Primary goal: creating classic videogame text screenshots with known ground truth labels</p>"},{"location":"#github-lines-of-code-analyzer","title":"\ud83d\udc19 Github Lines of Code Analyzer","text":"<p>Shallow clone a repo then use unix + pandas tools to count how many lines of each file type are present</p> <p><code>streamlit run github_code_analyze.py</code></p>"},{"location":"#aws-textract-document-text-scan","title":"AWS Textract Document Text Scan","text":"<p>Using AWS Textract + S3 + Streamlit to provide interactive OCR Web App.</p> <p>See: Github Repo</p>"},{"location":"tags/","title":"Tags","text":"<p>Following is a list of used tags:</p>"},{"location":"tags/#api","title":"API","text":"<ul> <li>GraphQL</li> </ul>"},{"location":"tags/#backend","title":"Backend","text":"<ul> <li>GraphQL</li> <li>Javascript</li> <li>Node JS</li> </ul>"},{"location":"tags/#cicd","title":"CI/CD","text":"<ul> <li>CI/CD</li> <li>CircleCI</li> </ul>"},{"location":"tags/#circleci","title":"CircleCI","text":"<ul> <li>CircleCI</li> </ul>"},{"location":"tags/#database","title":"Database","text":"<ul> <li>GraphQL</li> </ul>"},{"location":"tags/#frontend","title":"Frontend","text":"<ul> <li>Gatsby</li> <li>Javascript</li> <li>React JS</li> </ul>"},{"location":"tags/#full-stack","title":"Full Stack","text":"<ul> <li>Node JS</li> </ul>"},{"location":"tags/#general","title":"General","text":"<ul> <li>Open Source</li> <li>Windows</li> </ul>"},{"location":"tags/#gitlab","title":"Gitlab","text":"<ul> <li>Gitlab</li> </ul>"},{"location":"tags/#graphql","title":"GraphQL","text":"<ul> <li>GraphQL</li> </ul>"},{"location":"tags/#heroku","title":"Heroku","text":"<ul> <li>Heroku</li> </ul>"},{"location":"tags/#js","title":"JS","text":"<ul> <li>React JS</li> </ul>"},{"location":"tags/#javascript","title":"Javascript","text":"<ul> <li>Javascript</li> </ul>"},{"location":"tags/#linux","title":"Linux","text":"<ul> <li>Linux</li> </ul>"},{"location":"tags/#machine-learning","title":"Machine Learning","text":"<ul> <li>Machine Learning</li> </ul>"},{"location":"tags/#markdown","title":"Markdown","text":"<ul> <li>Markdown</li> </ul>"},{"location":"tags/#node","title":"Node","text":"<ul> <li>Node JS</li> </ul>"},{"location":"tags/#os","title":"OS","text":"<ul> <li>Windows</li> </ul>"},{"location":"tags/#open-source","title":"Open Source","text":"<ul> <li>Open Source</li> </ul>"},{"location":"tags/#react","title":"React","text":"<ul> <li>Gatsby</li> <li>React JS</li> </ul>"},{"location":"tags/#ssr","title":"SSR","text":"<ul> <li>Gatsby</li> </ul>"},{"location":"tags/#web","title":"Web","text":"<ul> <li>Gatsby</li> <li>Javascript</li> <li>Node JS</li> <li>React JS</li> </ul>"},{"location":"tags/#windows","title":"Windows","text":"<ul> <li>Windows</li> </ul>"},{"location":"tags/#advanced","title":"advanced","text":"<ul> <li>Computer Vision Part 3: Train U-Net Architecture to predict UI segmentation</li> </ul>"},{"location":"tags/#async","title":"async","text":"<ul> <li>Checking 48 Mountain Weather Locations at Once</li> </ul>"},{"location":"tags/#beginner","title":"beginner","text":"<ul> <li>Bottle + HTMX vs Streamlit</li> <li>Computer Vision Part 1: Loading Images with Python</li> <li>CTE vs Subquery: Who gives a \ud83e\udd86uck?</li> <li>Gitlab Workflow Project Management</li> <li>How I Edit Text Files (VS Code Settings)</li> <li>My Favorite Python One Liner Sucks</li> <li>Share a website to the world in less than a day (Free for Students!)</li> <li>3 Ways to Run Python Code</li> <li>Streamlit Full Stack Part 1: Python Web App in One File</li> <li>The Terminal (/ Shell / Command Line / Console)</li> <li>Time Series Data Part 1: What is a Time Series?</li> <li>Git</li> </ul>"},{"location":"tags/#curl","title":"curl","text":"<ul> <li>curl</li> </ul>"},{"location":"tags/#darts","title":"darts","text":"<ul> <li>Time Series Data Part 2: Darts and Streamlit</li> <li>Time Series Data Part 3: Custom Model Tuning</li> <li>Time Series Data Part 4: A Full Stack Use Case</li> </ul>"},{"location":"tags/#data","title":"data","text":"<ul> <li>CTE vs Subquery: Who gives a \ud83e\udd86uck?</li> <li>Holy \ud83e\udd86uck! Fast Analysis with DuckDB + Pyarrow</li> </ul>"},{"location":"tags/#deployment","title":"deployment","text":"<ul> <li>Share a website to the world in less than a day (Free for Students!)</li> </ul>"},{"location":"tags/#drivendata","title":"drivendata","text":"<ul> <li>LightGBM Predicting Water Pump Functionality</li> </ul>"},{"location":"tags/#fastapi","title":"fastapi","text":"<ul> <li>Pipreqs FastAPI Server + Github Bot</li> </ul>"},{"location":"tags/#frontend_1","title":"frontend","text":"<ul> <li>Frontend</li> </ul>"},{"location":"tags/#full-stack_1","title":"full-stack","text":"<ul> <li>Frontend</li> </ul>"},{"location":"tags/#general-coding","title":"general-coding","text":"<ul> <li>Gitlab Workflow Project Management</li> <li>How I Edit Text Files (VS Code Settings)</li> </ul>"},{"location":"tags/#git","title":"git","text":"<ul> <li>Gitlab Workflow Project Management</li> <li>Git</li> </ul>"},{"location":"tags/#guide","title":"guide","text":"<ul> <li>Computer Vision Part 1: Loading Images with Python</li> <li>Computer Vision Part 2: Load Tagged Data With Pytorch</li> <li>Computer Vision Part 3: Train U-Net Architecture to predict UI segmentation</li> <li>Gitlab Workflow Project Management</li> <li>Share a website to the world in less than a day (Free for Students!)</li> </ul>"},{"location":"tags/#habits","title":"habits","text":"<ul> <li>How I Edit Text Files (VS Code Settings)</li> </ul>"},{"location":"tags/#images","title":"images","text":"<ul> <li>Computer Vision Part 1: Loading Images with Python</li> </ul>"},{"location":"tags/#intermediate","title":"intermediate","text":"<ul> <li>Holy \ud83e\udd86uck! Fast Analysis with DuckDB + Pyarrow</li> <li>LightGBM Predicting Water Pump Functionality</li> <li>Pipreqs FastAPI Server + Github Bot</li> <li>Python Form Generator (JSON to Streamlit Form!)</li> <li>Time Series Data Part 3: Custom Model Tuning</li> <li>Time Series Data Part 4: A Full Stack Use Case</li> <li>Checking 48 Mountain Weather Locations at Once</li> </ul>"},{"location":"tags/#machine-learning_1","title":"machine-learning","text":"<ul> <li>Computer Vision Part 2: Load Tagged Data With Pytorch</li> <li>Computer Vision Part 3: Train U-Net Architecture to predict UI segmentation</li> </ul>"},{"location":"tags/#meta","title":"meta","text":"<ul> <li>Brain Dump 2022</li> </ul>"},{"location":"tags/#moderate","title":"moderate","text":"<ul> <li>Computer Vision Part 2: Load Tagged Data With Pytorch</li> </ul>"},{"location":"tags/#networks","title":"networks","text":"<ul> <li>Local Network</li> <li>Local Server</li> </ul>"},{"location":"tags/#opencv","title":"opencv","text":"<ul> <li>Computer Vision Part 1: Loading Images with Python</li> <li>Custom Data Tagging with Python OpenCV</li> </ul>"},{"location":"tags/#projects","title":"projects","text":"<ul> <li>Custom Data Tagging with Python OpenCV</li> </ul>"},{"location":"tags/#python","title":"python","text":"<ul> <li>Bottle + HTMX vs Streamlit</li> <li>Computer Vision Part 1: Loading Images with Python</li> <li>Computer Vision Part 2: Load Tagged Data With Pytorch</li> <li>Computer Vision Part 3: Train U-Net Architecture to predict UI segmentation</li> <li>Holy \ud83e\udd86uck! Fast Analysis with DuckDB + Pyarrow</li> <li>LightGBM Predicting Water Pump Functionality</li> <li>My Favorite Python One Liner Sucks</li> <li>Pipreqs FastAPI Server + Github Bot</li> <li>Python Form Generator (JSON to Streamlit Form!)</li> <li>3 Ways to Run Python Code</li> <li>Streamlit Full Stack Part 1: Python Web App in One File</li> <li>Time Series Data Part 1: What is a Time Series?</li> <li>Time Series Data Part 2: Darts and Streamlit</li> <li>Time Series Data Part 3: Custom Model Tuning</li> <li>Time Series Data Part 4: A Full Stack Use Case</li> <li>Custom Data Tagging with Python OpenCV</li> <li>Checking 48 Mountain Weather Locations at Once</li> </ul>"},{"location":"tags/#pytorch","title":"pytorch","text":"<ul> <li>Computer Vision Part 1: Loading Images with Python</li> <li>Computer Vision Part 2: Load Tagged Data With Pytorch</li> <li>Computer Vision Part 3: Train U-Net Architecture to predict UI segmentation</li> </ul>"},{"location":"tags/#resources","title":"resources","text":"<ul> <li>Time Series</li> </ul>"},{"location":"tags/#sql","title":"sql","text":"<ul> <li>CTE vs Subquery: Who gives a \ud83e\udd86uck?</li> </ul>"},{"location":"tags/#streamlit","title":"streamlit","text":"<ul> <li>LightGBM Predicting Water Pump Functionality</li> <li>Python Form Generator (JSON to Streamlit Form!)</li> <li>Streamlit Full Stack Part 1: Python Web App in One File</li> <li>Time Series Data Part 2: Darts and Streamlit</li> <li>Time Series Data Part 3: Custom Model Tuning</li> <li>Time Series Data Part 4: A Full Stack Use Case</li> <li>Checking 48 Mountain Weather Locations at Once</li> </ul>"},{"location":"tags/#time-series","title":"time-series","text":"<ul> <li>Time Series Data Part 2: Darts and Streamlit</li> <li>Time Series Data Part 4: A Full Stack Use Case</li> </ul>"},{"location":"tags/#timeseries","title":"timeseries","text":"<ul> <li>Time Series Data Part 1: What is a Time Series?</li> <li>Time Series Data Part 3: Custom Model Tuning</li> <li>Time Series</li> </ul>"},{"location":"tags/#vgac","title":"vgac","text":"<ul> <li>Custom Data Tagging with Python OpenCV</li> </ul>"},{"location":"tags/#web_1","title":"web","text":"<ul> <li>Frontend</li> </ul>"},{"location":"tags/#web-dev","title":"web-dev","text":"<ul> <li>Share a website to the world in less than a day (Free for Students!)</li> <li>Custom Data Tagging with Python OpenCV</li> </ul>"},{"location":"blog/","title":"Blog","text":"<p>Links on the left.</p> <p>These posts are my longer form thoughts and side projects.</p> <p>Currently a mash of Python, Data, and a few Workflow topics.</p>"},{"location":"blog/bottle-htmx-streamlit/","title":"Bottle + HTMX vs Streamlit","text":"<p>NOTE: This isn't a debate that real people have.</p> <p>So why compare them...?</p> <p>It's a comparison of workflows for quickly building small, interactive web apps.</p> <p><code>streamlit</code> is the new hotness in Data Science and Machine Learning. It basically turns Python scripts into websites, pre-styled and pre-packaged with a good looking component library. But its install size is not small!</p> <p>The Python web framework <code>bottle</code> and the Javascript/HTML power tool <code>htmx</code> are each dependency free and each is contained in a single <code>.py</code> / <code>.js</code> file (js can be loaded from cdn).</p> <p><code>bottle</code> is ~4500 lines of Python and <code>htmx</code> is ~3000 lines of Javascript (unminified). Combining the two allows for writing interactive apps with just Python and HTML.</p> <p>But first some context for people who haven't heard of <code>bottle</code> and <code>htmx</code>.</p>","tags":["python","beginner"]},{"location":"blog/bottle-htmx-streamlit/#python-full-stack-development","title":"Python Full Stack Development","text":"<p>Before I found <code>streamlit</code> I played with different Python web frameworks (<code>flask</code>, <code>klein</code>, <code>twisted</code>, <code>django</code>, etc.). They all try to make spinning up a web server easier (some with a heavier hand than others cough django cough).</p> <p>They all have some templating mechanism (ex. <code>jinja2</code>) for sending html to the frontend. But many modern \"full-stack\" tutorials won't bother with this feature.</p> <p>The flavor of the moment I've seen is JSON Rest Api + React/Vue/Angular Frontend. So as an aspiring developer I've hacked away at Vue projects (like a timer app for daily tasks) to figure out web-workers and browser storage work. And I've gone through courses such as Django Channels + Angular + Docker to learn wtf a websocket is. I even did some minor work on a <code>react</code> + <code>express</code> admin app for a startup.</p> <p>But I have never been able to get invested in the javascript ecosystem &amp; accept the size of <code>node_modules</code>. (Saying this fully aware of Python's packaging woes...)</p>","tags":["python","beginner"]},{"location":"blog/bottle-htmx-streamlit/#a-simpler-web-stack","title":"A Simpler Web Stack","text":"<p>If you've run code like <code>django-admin startproject mysite</code> then fired up a webserver without understanding the magic of the background that is totally fine.</p> <p>To get a grip on why <code>django</code> does what it does, you can spend some time with <code>bottle</code>.</p> <p>A simple bottle app that returns <code>hello world</code> on a GET request might look like the following:</p> Python<pre><code>from bottle import route, run\n\n@route('/')\ndef index():\n    return 'hello world'\n\nrun(host='localhost', port=8080)\n</code></pre> <p>Running this code in a normal Python script (i.e. <code>python my_bottle_app.py</code>) will run a server on your local machine. Going to a browser with url http://localhost:8080/ should show you 'hello world'. You can also use a program like <code>curl</code> (or Python <code>requests</code>) to fetch the data as an API request.</p> <p>HOLD UP</p> <p>WTF is a <code>route</code> or a <code>localhost</code> or a <code>port</code>?</p>","tags":["python","beginner"]},{"location":"blog/bottle-htmx-streamlit/#an-easier-web-stack","title":"An Easier Web Stack","text":"<p>Let's compare this to a <code>streamlit</code> hello world.</p> Python<pre><code>import streamlit as st\nst.write('hello world')\n</code></pre> <p>To run this we need to use <code>streamlit run my_streamlit_app.py</code> (you may need <code>python -m streamlit run ...</code> to specify the current Python interpreter)</p> <p>Then we should automagically get a browser window open with our 'hello world' displayed, styled, and part of a larger app with a hamburger menu that can change the theme and do some other things.</p> <p>One major drawback is we don't get access to our code via API (see this github issue for my hack on adding API routes to your Streamlit app's <code>tornado</code> server).</p>","tags":["python","beginner"]},{"location":"blog/bottle-htmx-streamlit/#bottle-htmx","title":"Bottle + HTMX","text":"<p>To actually get cooking with gas, let's demo an interactive <code>bottle</code> webpage. If you don't want to see raw HTML, avert your attention now.</p> Python<pre><code>from bottle import Bottle, static_file, request\n\napp = Bottle()\n\n@app.route(\"/\")\ndef hello():\n    return static_file(\"index.html\", '.')\n\n@app.route(\"/htmx.js\")\ndef htmx():\n    return static_file(\"htmx.js\", '.')\n\n@app.route(\"/fun\", method=\"GET\")\ndef fun_template():\n    return \"\"\"&lt;textarea placeholder=\"Type some nonsense then tab / click away!\" \n        hx-post=\"/fun\" hx-target=\"#fun-outputs\" hx-swap=\"innerHTML\" \n        name=\"fun-input\" id=\"fun-input\" cols=\"60\" rows=\"20\"&gt;&lt;/textarea&gt;\n        &lt;div id=\"fun-outputs\"&gt;&lt;/div&gt;\"\"\"\n\n\n@app.route(\"/fun\", method=\"POST\")\ndef fun_handler():\n    fun_input = request.forms.get(\"fun-input\", \"\")\n    if len(fun_input):\n        return f\"\"\"\n            &lt;h1&gt;{len(fun_input)} Characters Processed! REVERSE REVERSE!&lt;/h1&gt;\n            &lt;div&gt;{fun_input[::-1]}&lt;/div&gt;\"\"\"\n\n\napp.run(host=\"localhost\", port=8080, debug=True, reloader=True)\n</code></pre> <p>If you have played around with <code>flask</code>, this app creation pattern and adding routes with decorators might be a little familiar.</p> <p>Returning HTML strings with random <code>hx-</code> element attributes is probably not familiar at all.</p>","tags":["python","beginner"]},{"location":"blog/bottle-htmx-streamlit/#static-files","title":"Static Files","text":"<p>To be explicit, I've included 2 routes to serve the 2 necessary static files. Bottle does have the ability to serve static files from a directory on a wildcard route.</p> <p><code>index.html</code> holds the skeleton of the app. Just including the interesting bits here:</p> HTML<pre><code>&lt;head&gt;\n    &lt;script src=\"htmx.js\"&gt;&lt;/script&gt;\n&lt;/head&gt;\n\n&lt;body&gt;\n    &lt;button hx-get=\"/fun\" \n            hx-target=\"#app\" \n            hx-swap=\"innerHTML\"&gt;\ud83c\udf89 Do Something Fun!&lt;/button&gt;\n    &lt;div id=\"app\"&gt;&lt;/div&gt;\n&lt;/body&gt;\n</code></pre> <p>At the top we have a standard <code>&lt;script&gt;</code> tag to load in the file at <code>/htmx.js</code>, which is the raw <code>htmx</code> library (included as raw file for showcase, serve it gzipped / from cdn in production).</p> <p>The <code>htmx</code> magic comes in the <code>hx-get</code>, <code>hx-target</code> and <code>hx-swap</code> attributes.</p> <p>When the <code>button</code> is clicked, it will make a GET request to <code>/fun</code>. It then takes the response from <code>/fun</code> and injects it into the <code>innerHTML</code> of the <code>#app</code> element (the <code>div</code> in this case).</p> <p>All powered by <code>htmx</code> using 3 HTML attributes!</p>","tags":["python","beginner"]},{"location":"blog/bottle-htmx-streamlit/#get-routes-in-bottle-htmx","title":"Get Routes in Bottle + HTMX","text":"<p>Next we have a route that handles GET requests to <code>/fun</code>.</p> <p>On the <code>bottle</code> side of things we just want to return the HTML template for the \"fun zone\" (tm \ud83c\udf89). Which with syntax highlighting looks like:</p> HTML<pre><code>&lt;textarea placeholder=\"Type some nonsense then tab / click away!\" \n    hx-post=\"/fun\" hx-target=\"#fun-outputs\" hx-swap=\"innerHTML\" \n    name=\"fun-input\" id=\"fun-input\" cols=\"60\" rows=\"20\"&gt;&lt;/textarea&gt;\n&lt;div id=\"fun-outputs\"&gt;&lt;/div&gt;\n</code></pre> <p>The first element returned is a <code>textarea</code> for inputting random text. When the content of this <code>textarea</code> changes, a POST request will be sent to <code>/fun</code> (note the <code>hx-post=\"/fun\"</code>), and the response will be injected into the <code>innerHTML</code> of the <code>fun-outputs</code> <code>div</code>.</p>","tags":["python","beginner"]},{"location":"blog/bottle-htmx-streamlit/#post-routes-in-bottle-htmx","title":"Post Routes in Bottle + HTMX","text":"<p><code>htmx</code> can send data from our html widgets in a post request (see the docs for more understanding of request triggers such as with HTML forms).</p> <p><code>bottle</code> can then parse this request data and do something with it!</p> Python<pre><code>fun_input = request.forms.get(\"fun-input\", \"\")\n</code></pre> <p>In this case, we'll return the message in reverse and announce how long the string is. By returning HTML, it will get displayed nicely in the element specified by <code>hx-target</code>.</p> <p>In the case where there isn't any actual text, it will implicitly return None and won't update the display.</p> Python<pre><code>if len(fun_input):\n    return f\"\"\"\n        &lt;h1&gt;{len(fun_input)} Characters Processed! REVERSE REVERSE!&lt;/h1&gt;\n        &lt;div&gt;{fun_input[::-1]}&lt;/div&gt;\"\"\"\n</code></pre> <p>NOTE this doesn't use built in <code>bottle</code> templating for simplicity.</p> <p>Admittedly, <code>bottle</code> uses a similarly confusing global request object to <code>flask</code>, but \"Pay no attention to that man behind the curtain!\"</p>","tags":["python","beginner"]},{"location":"blog/bottle-htmx-streamlit/#doing-it-with-streamlit","title":"Doing it with Streamlit","text":"<p>Already the <code>bottle</code> example has grown in complexity and \"things to know about\".</p> <p>So far the concepts outside of Python (that all require coding attention!) are:</p> <ul> <li>HTML</li> <li>Ports / Hostnames</li> <li>Server / wsgi loops</li> <li>URL routes</li> <li>HTTP request methods</li> <li>How <code>htmx</code> works</li> </ul> <p>And if we wanted to deploy this to the world we'd also want to know (and spend time on):</p> <ul> <li>CSS styling</li> <li>Static file deployment / bundling</li> <li>Gunicorn / Gevent / production wsgi serving</li> <li>Testing interaction between HTML and routes</li> </ul> <p>What if we could shove all of that mountain of stuff onto the shoulders of <code>streamlit</code>...?</p> <p>NOTE These are all awesome things to learn! The Odin Project is an awesome resource and community for learning the web parts that Python learners usually don't come into web apps with.</p> <p>Porting the example to streamlit, we can reduce it from 2 files (<code>index.html</code> and <code>wsgi.py</code>) to 1 (<code>streamlit_app.py</code>) and can reduce the lines of code we need to understand from ~50 to ~15.</p> Python<pre><code>import streamlit as st\n\nif st.button(\"Do Something Fun \ud83c\udf89!\"):\n    st.session_state['show_fun'] = True\nelif 'show_fun' not in st.session_state:\n    st.session_state['show_fun'] = False\n\nif st.session_state.show_fun:\n    fun_input = st.text_area('', placeholder='Type some nonsense then hit cmd/ctrl + enter')\n    if len(fun_input):\n        st.header(f\"{len(fun_input)} Characters Processed! REVERSE REVERSE!\")\n        st.write(fun_input[::-1])\n</code></pre> <p>No more HTML... Replaced with:</p> <ul> <li><code>st.button</code></li> <li><code>st.text_area</code></li> <li><code>st.header</code></li> <li><code>st.write</code></li> </ul> <p>No more Ports / server / routes... Replaced with:</p> <p><code>streamlit run streamlit_app.py</code></p> <p>No more HTTP requests or <code>htmx</code>... Replaced with:</p> <ul> <li><code>if st.button(\"Do Something Fun \ud83c\udf89!\"):</code></li> <li><code>fun_input = st.text_area('')</code></li> </ul> <p>This example didn't even show any effort to CSS on the <code>bottle</code> app, but <code>streamlit</code> gives it to us for free!</p> <p>Additionally, the communication between frontend components and backend updates has to be tested by you. And then you have to test the business logic.</p> <p><code>streamlit</code> has over a dozen widgets in their input library with a team and community of developeres working on their speed, UI/UX, and resiliency.</p> <p>Finally, deploying to streamlit with <code>streamlit run streamlit_app.py</code> is a valid way to run the server. Deploying with Streamlit Sharing in the cloud is even less to deal with, netting a free Continuous Deployment pipeline from github to the cloud.</p>","tags":["python","beginner"]},{"location":"blog/bottle-htmx-streamlit/#conclusion","title":"Conclusion","text":"<p>While <code>streamlit</code> isn't the simplest interactive web stack out there, it very well might be the easiest.</p> <p>With a growing community, new web features are added all the time to <code>streamlit</code>, but are accessible in a Pythonic way.</p> <p>Are you with the majority of people who hear the word \"state\" and think about political geographies? Spend some time with session_state since the <code>streamlit</code> execution model is unique. Soon your understanding of frontend <code>state</code> will carry over to other projects!</p> <p>Want to learn what query params / arguments are for? There's experimental support for getting and setting them with Python.</p> <p>All that being said, if you need barebones interactivity with no dependencies, it is acheivable.</p>","tags":["python","beginner"]},{"location":"blog/brain-dump-2022/","title":"Brain Dump 2022","text":"","tags":["meta"]},{"location":"blog/brain-dump-2022/#looking-back-on-2022","title":"Looking back on 2022","text":"","tags":["meta"]},{"location":"blog/brain-dump-2022/#old-job","title":"Old Job","text":"<p>Working in mortgage finance industry showed me the reasons I don't want to work with Windows, or Excel, or mainframes... But it did give me a bunch of Python data pipelining skills and exposure to what it takes to convert an organization to a data warehouse over Postgres.</p>","tags":["meta"]},{"location":"blog/brain-dump-2022/#new-job","title":"New Job","text":"<p>Moved into the climatetech / fintech space with Sensible Weather and enjoying contributing to a profit center rather than cost center. Stretching me to learn Golang for web services, which has been a good addition to the toolkit.</p>","tags":["meta"]},{"location":"blog/brain-dump-2022/#old-blog","title":"Old Blog","text":"<p>To give me confidence in job searching I started a new blog. This version was with Fastpages, which has some special handling for converting jupyter notebooks to Jekyll blog pages.</p> <p>I converted a few old markdown blog entries and wanted to write new posts for side projects and explorations.</p> <p>The past years have been a lot about finding what niches of software really excite me (which is a variety!). Python as a backend and data engineering and scripting and pseudocoding language is my current comfort. While a language is only a tool, it's been great to be familiar with it as I've explored Time Series and Geospatial data this year.</p>","tags":["meta"]},{"location":"blog/brain-dump-2022/#new-blog","title":"New Blog","text":"<p>New Year, New Blog Framework. Going with Mkdocs Material and will likely join the sponsor program to get insider access to the official blog plugin.</p> <p>Fastpages has a bit too much surrounding setup for me to feel in control and mkdocs has some fantastic support for customization and extension out of the box. I'll probably still use something to convert Jupyter notebooks into markdown, but that's still up in the air.</p> <p>I'm also interested in a strong search, since I'd still like to accumulate knowledge of frontend topics and general know-how on how to set up and deploy systems in different environments. But I want my blog topics to have a more focused curation. Maybe. Who knows where it'll go in a year.</p>","tags":["meta"]},{"location":"blog/brain-dump-2022/#old-side-projects","title":"Old Side Projects","text":"<p>While I'm beginning to appreciate the usecases for splitting webservices up into separate deployments, I think I learned to create applications in a generation of extreme over-engineering. I thought to have a shot at getting a job I had to be able to:</p> <ul> <li>Write and test a Javascript frontend in React / Vue / Angular</li> <li>Write and test a backend web framework to provide a JSON REST(-ish) API that is consumed by the frontend</li> <li>Create and maintain a database to provide persistence for the application</li> <li>Write and understand a proprietary YAML config description for deploying each piece in a horizontally scalable way</li> </ul> <p>So struggling through side projects came with a lot of cognitive thrashing trying to learn multiple things at once and not a lot of feeling of progress.</p>","tags":["meta"]},{"location":"blog/brain-dump-2022/#new-side-projects","title":"New Side Projects","text":"<p>I think I signed up for Streamlit beta after I saw it on reddit, but didn't open it until a mentor suggested I use it for teaching students. After some success with students creating impressive demos, I got used to using it on my own.</p> <p>A handful of gif demos are on my github page, but the main point is I decided to abandon complexity and focus on learning.</p> <p>I made an OCR app backed by AWS text services, then later discovered EasyOCR and plugged that in instead. I wouldn't want to make changes like that in a highly complicated project, but when working on one or two files it is easy to experiment.</p> <p>I started working with more tabular and time series data and explored DuckDB and Apache Arrow, then since joining a climatetech company I've been playing with Spatialite, PostGIS, and GeoPandas.</p>","tags":["meta"]},{"location":"blog/brain-dump-2022/#old-achievements","title":"Old Achievements","text":"<p>I put up 17 blog posts on my Fastpages based blog this past year and 3 videos on Youtube. I shared 13 projects on Twitter and the Streamlit Forum and probably hacked away at a dozen others. I joined the open source Streamlit Creators group and got an article published on the Streamlit blog.</p>","tags":["meta"]},{"location":"blog/brain-dump-2022/#new-achievements","title":"New Achievements","text":"<p>I plan to create more videos this year on Youtube. If short videos seem to help the next generation of engineers I want to see what niches are relying on TikTok.</p> <p>I want to blog more about geospatial topics as I learn more. Geoparquet and DuckDB-Geospatial support are still in very early days. Any climate or energy or population modeling relies on these kinds of computations.</p> <p>I hope to stick with this single mkdocs deployment for my blog for at least a year. I do have a fun idea to combine it with a Pocketbase based user / event store for making a lightweight Learning Management System of sorts.</p>","tags":["meta"]},{"location":"blog/computer-vision-part-1/","title":"Computer Vision Part 1: Loading Images with Python","text":"<p>Intro to Loading Images with OpenCV and Matplotlib in a Python Notebook (+ Bonus using PIL / Pillow)</p>","tags":["beginner","images","opencv","python","pytorch","guide"]},{"location":"blog/computer-vision-part-1/#1-import-and-inline-setup","title":"1. Import and inline setup","text":"<p>At the start of our notebook we put <code>%matplotlib inline</code>, this isn't necessary in normal python because there is no inline viewing.</p> <p>We import <code>matplotlib.pyplot</code> as <code>plt</code> for ease of typing.</p> <p>We also import opencv (<code>cv2</code>) to load images. Matplotlib uses Numpy arrays (ndarray) to display, so does opencv. Later we'll using PIL / Pillow to load images, which will require converting to numpy anyway</p> <p>Use <code>ctrl</code> + <code>enter</code> to run cells more easily, or use the <code>play</code> button above or to the left of cells.</p> <p>Once we've run both of the following cells we'll be able to view images. If this fails then your virtual environment probably doesn't have both matplotlib and cv2 installed, so you'll need to <code>pip install...</code> or <code>conda install...</code> based on your setup.</p> <p>Also note that PIL has inline image viewing, I'm not sure if it's supported in all notebooks. There's also HTML5 viewing.</p> <p>Refernce from SO</p> <p>OpenCV docs for version 3.4.2 (Relatively similar between versions 2.x and 4.x, the different pip and conda and conda-forge sources will give different verisions)</p> Python<pre><code>%matplotlib inline\n</code></pre> Python<pre><code>import matplotlib\nimport matplotlib.pyplot as plt\nimport cv2\nprint(f\"Matplotlib version: {matplotlib.__version__}\")\nprint(f\"OpenCV version: {cv2.__version__}\")\n</code></pre> Text Only<pre><code>Matplotlib version: 3.2.1\nOpenCV version: 4.2.0\n</code></pre>","tags":["beginner","images","opencv","python","pytorch","guide"]},{"location":"blog/computer-vision-part-1/#2-fetching-some-image-files","title":"2. Fetching some image files","text":"<p>In this step we do a basic folder / directory search with <code>glob.glob</code>, which is a lot like <code>ls</code> in the terminal. I'll use a String to specify the directory and an asteriks <code>*.png</code> to indicate a wildcard (i.e. it selects any filename that ends in '.png' in the directory. (We have to import it to use it, but it's built into python)</p> <p>Where your files are may vary, in this case I use a relative path ('./start_images') since the <code>file.ipynb</code> is in the same folder as my data folder. You could also use an absolute path ('/Users/username/datasets/mycooldata')</p> <p>Reference for glob Reference for directories</p> Python<pre><code>import glob\n</code></pre> Python<pre><code>directory_wildcard_png = \"./start_/images/*.png\"\nfile_names = glob.glob(directory_wildcard_png)\nprint(f\"Found {len(file_names)} .png files in the folder {directory_wildcard_png}\")\nprint(\"First 3 file_paths:\")\n\nfirst_3 = file_names[:3]\nfor file_path in first_3:\n    print(file_path)\n</code></pre> Text Only<pre><code>Found 6 .png files in the folder ./start_/images/*.png\nFirst 3 file_paths:\n./start_/images/6_poke.png\n./start_/images/4_poke.png\n./start_/images/2_poke.png\n</code></pre>","tags":["beginner","images","opencv","python","pytorch","guide"]},{"location":"blog/computer-vision-part-1/#3-use-opencv-to-load-image-files-into-numpy-ndarrays","title":"3. Use OpenCV to Load image files into numpy ndarrays","text":"<p>Now that we have the file paths that tell where each image is, we can load them.</p> <p>We'll make an empty list that we'll append with loaded images. We 'Image Read' (<code>imread</code>) in the images and specify OpenCV to give us a 3-channel image (Red, Green, Blue), which is what we generally want to work with (This is included because if you want to use a texture / sprite with empty alpha you should use <code>cv2.IMREAD_UNCHANGED</code> to load in a 4-channel image with alpha transparency). </p> <p>OpenCV (and by extension Numpy in most cases) follows the order of (ROWS, COLUMNS, CHANNELS) for the array shape. So when we use the <code>.shape</code> attribute on an OpenCV image we get a 3-tuple of (Height, Width, Channels). If it is 1-channel it will be a 2-tuple of (Height, Width) If working in PIL see 9.</p> <p>You can do this with as many files as you want, but if you try to load too many your computer may slow down / python will Out Of Memory error.</p> <p>If you have an image in the same folder a non-absolute path will probably work: \"myimage.png\", \"./myimage.png\" </p> <p>Reference for OpenCV non-notebook</p> Python<pre><code>open_cv_images = []\nfor file_path in file_names:\n    loaded_image = cv2.imread(file_path, cv2.IMREAD_COLOR)\n    open_cv_images.append(loaded_image)\nprint(f\"First image type: {type(open_cv_images[0])}, shape: {open_cv_images[0].shape}, dtype: {open_cv_images[0].dtype}\")\n</code></pre> Text Only<pre><code>First image type: &lt;class 'numpy.ndarray'&gt;, shape: (224, 256, 3), dtype: uint8\n</code></pre>","tags":["beginner","images","opencv","python","pytorch","guide"]},{"location":"blog/computer-vision-part-1/#4-dislpay-a-single-image-using-matplotlib","title":"4. Dislpay a single Image using Matplotlib","text":"<p>At this point you've got all the images loaded and ready to display, now it's just up to you whether you to spread out the calls or make a grid with matplotlib</p> <p>If we use more than one <code>plt.imshow()</code> call in the same cell one will not actually get displayed (there may be a workaround to this, we'll use a grid at step 6). You can try commenting out the second <code>plt.imshow()</code> call, you should get a different image when you run it again.</p> Python<pre><code># This image won't display, gets overwritten by next one\nplt.imshow(open_cv_images[1])\n# Single Image\nplt.imshow(open_cv_images[0])\n</code></pre> Text Only<pre><code>&lt;matplotlib.image.AxesImage at 0x7f52a7f9a390&gt;\n</code></pre>","tags":["beginner","images","opencv","python","pytorch","guide"]},{"location":"blog/computer-vision-part-1/#5-opencv-color-caveat","title":"5. OpenCV Color Caveat","text":"<p>OpenCV loads images in <code>B-G-R</code> order (Blue, Green, Red), as opposed to <code>R-G-B</code> which I'm prety sure most of the rest of the world uses (Except people really into color, who use HSV or HSL or whatever)</p> <p>What this implies: - We could swap the first and last channel to display it as RGB easily with matplotlib. - When saving a file OpenCV assumes the image is in BGR.</p> <p>We'll focus on the first implication for now and discuss the second when we use PIL / Pillow.</p> <p>We use OpenCV's <code>cvtColor</code> ('Convert Color') in the following way: <code>recolored_image = cv2.cvtColor(original_image, cv2.TYPE_OF_CONVERSION)</code>.</p> <p>Conversions you'll probably use the most:  - <code>cv2.COLOR_BGR2RGB</code>: convert BGR to RGB (essentially the same as <code>cv2.COLOR_RGB2BGR</code>, swaps first and third channel) - <code>cv2.COLOR_BGR2GRAY</code>: convert BGR to Grayscale (1-channel) (<code>RGB2GRAY</code> also available) - <code>cv2.COLOR_GRAY2BGR</code>: convert Grayscale to BGR 3-channel (<code>GRAY2RGB</code> also available)</p> <p>Reference for changing colors (Bonus Object Tracking demo using HSV color space)</p> <p>All Conversion FLAGS</p> Python<pre><code>rgb_images = []\ngrayscale_images = []\nfor bgr_image in open_cv_images:\n    rgb_image = cv2.cvtColor(bgr_image, cv2.COLOR_BGR2RGB)\n    rgb_images.append(rgb_image)\n    gray_image = cv2.cvtColor(bgr_image, cv2.COLOR_BGR2GRAY)\n    grayscale_images.append(gray_image)\n\nplt.imshow(rgb_images[0])\n</code></pre> Text Only<pre><code>&lt;matplotlib.image.AxesImage at 0x7f52a8a70910&gt;\n</code></pre>","tags":["beginner","images","opencv","python","pytorch","guide"]},{"location":"blog/computer-vision-part-1/#6-displaying-multiple-images-with-matplotlib","title":"6. Displaying multiple Images with Matplotlib","text":"<p>Now our RGB images can display nicely, so let's move on to displaying multiple images in a grid with matplotlib.</p> <p>We'll use matplotlib <code>subplots()</code>, but you can find many implementations of 'make grid of images' with matplotlibt using  other functions</p> <p>Reference for matplotlib subplots (also see figures keywords link)</p> <p>HTML colornames that are valid for matplotlib</p> Python<pre><code>NUM_ROWS = 2\nNUM_COLS = 3\n# Scale and Dimensions to easier view things in your browser. 16:9 is common screen ratio\nSCALE = 1.5\nFIG_DIMENSIONS = (16 * SCALE, 9 * SCALE)\nBG_COLOR = 'beige'\n\n_, axes = plt.subplots(NUM_ROWS, NUM_COLS, figsize=FIG_DIMENSIONS, facecolor=BG_COLOR)\n# Turn 2d-array of axes into 1d list to select each (row, col) index\naxes = axes.flatten()\nfor i, axis in enumerate(axes):\n    axis.imshow(grayscale_images[i])\nplt.show()\n</code></pre>","tags":["beginner","images","opencv","python","pytorch","guide"]},{"location":"blog/computer-vision-part-1/#7-matplotlib-grayscale-color-caveat","title":"7. Matplotlib Grayscale Color Caveat","text":"<p>Well we've run into another display issue. Our grayscale images are 1 channel, but why does matplotlib show these colors? Grayscale / 1-channel images can be used in many domains, and matplotlib isn't just for plotting images, so their default <code>colormap</code> from values to colors is not <code>Grayscale</code>, but instead it's something called <code>viridis</code> and used to be <code>jet</code>. (I dont' know anything about the uses of these, but there's a whole talk on why they make <code>viridis</code>, so there's theory behind it).</p> <p>We'll just make sure to specify to matplotlib that we want to use the <code>gray</code> colormap whenever we <code>imshow</code> an image. Or use another one of matplotlib colormaps.</p> <p>We also will treat this as a whole new figure (a new set of subplots), so we'll use the <code>plt.subplots()</code> call again</p> <p>Reference for default map in matplotlib</p> Python<pre><code>_, axes = plt.subplots(NUM_ROWS, NUM_COLS, figsize=FIG_DIMENSIONS, facecolor=BG_COLOR)\n#Turn 2d-array of axes into 1d list to select each (row, col) index\naxes = axes.flatten()\nfor i, axis in enumerate(axes):\n    axis.imshow(grayscale_images[i], cmap='gray')\nplt.show()\n</code></pre>","tags":["beginner","images","opencv","python","pytorch","guide"]},{"location":"blog/computer-vision-part-1/#bonuses","title":"Bonuses!","text":"<p>Those are the most important points about using Python Glob to get files, OpenCV to load, MatplotLib to display.</p> <p>We'll now go into more specific use cases - Plotting image changes on multiple images in a grid - Using PIL / Pillow and the differences with OpenCV - Saving Images</p>","tags":["beginner","images","opencv","python","pytorch","guide"]},{"location":"blog/computer-vision-part-1/#8-function-for-plotting-image-changes-on-multiple-images-in-a-grid","title":"8. Function for plotting image changes on multiple images in a grid","text":"<p>Commonly when we do image processing tasks we'll have an <code>input</code>, perform some operation (often using a <code>mask</code> or another <code>image</code>), and then have some <code>output</code> image that's different from the original. Maybe we want to perform multiple operations and view all of them, then we'd have an <code>extra_output</code> for each <code>input</code></p> <p>We usually want to compare the <code>output</code> with the <code>input</code> to make sure our operation is doing what we want, and we usually want to ensure it's working on multiple images.</p> <p>What this implies: - If we maintain a list of inputs and a list of outputs that correspond to each input we can display each <code>input</code> in one row, and each <code>output</code> on the next to compare  - If <code>inputs</code> and <code>outputs</code> have a one-to-one relationship, it'll be simple to go through both in one list but display in multiple rows - We can treat any transformation or step of our program as an operation, and might want to check that each one works as we're intending</p> <p>We'll make a function that takes a list of ALL the image lists concatenated and the number of <code>inputs</code> (length of our <code>inputs</code> list) to display each list row by row.</p>","tags":["beginner","images","opencv","python","pytorch","guide"]},{"location":"blog/computer-vision-part-1/#note-the-only-error-checking-we-do-is-for-single-channel-image-vs-rgba-so-the-list-best-be-divisible-by-the-number-of-samples","title":"Note: The only error checking we do is for single channel image vs RGB(A), so the list best be divisible by the number of samples","text":"<p>(We'll also only use 3 images for size constraints)</p> Python<pre><code>def display_image_comparison(list_of_inputs_and_outputs, number_of_samples, fig_dimensions=(24, 13), bg_color='beige'):\n    num_rows = len(list_of_inputs_and_outputs) // number_of_samples \n    _, axes = plt.subplots(num_rows, number_of_samples, figsize=fig_dimensions, facecolor=bg_color)\n    axes = axes.flatten()\n    for i, axis in enumerate(axes):\n        image_to_display = list_of_inputs_and_outputs[i]\n        if len(image_to_display.shape) == 2:\n            axis.imshow(image_to_display, cmap='gray')\n        else:\n            axis.imshow(image_to_display)\n    plt.show()\n\n# Imagine our task is comparing grayscale and rgb images, we've already done the transformations and maintained the images\n# Load in input images\ninput_images = open_cv_images[:3]\n# Do some operation to them, saving each in a list\nrecolor_operation_outputs = rgb_images[:3]\n# Do some other operation to the intermediate step or inputs\ngray_operation_outputs = grayscale_images[:3]\n\n# We can concatenate a few ways, python lists are interesting\nall_images = []\nall_images.extend(input_images)\nall_images.extend(recolor_operation_outputs)\nall_images.extend(gray_operation_outputs)\n\n# One-liner expand each list into elements\n# all_images = [*input_images, *recolor_operation_outputs, *gray_operation_outputs]\n\nprint(f\"Num inputs: {len(input_images)}, num rgb outputs: {len(recolor_operation_outputs)}, num grayscale images: {len(gray_operation_outputs)}\")\nprint(f\"Total: {len(all_images)}\")\ndisplay_image_comparison(all_images, len(input_images))\n</code></pre> Text Only<pre><code>Num inputs: 3, num rgb outputs: 3, num grayscale images: 3\nTotal: 9\n</code></pre>","tags":["beginner","images","opencv","python","pytorch","guide"]},{"location":"blog/computer-vision-part-1/#9a-using-pil-pillow-image-library","title":"9a. Using PIL / Pillow Image library","text":"<p>PIL / Pillow / Python Imaging Library is also super popular, but the images are in a different format than OpenCV and can't be displayed immediately by Matplotlib, so we'll go over some of the differences. I'll make notes in the code that show where to be careful when dealing with Pillow vs OpenCV.</p> <p>Installing this library has changed over the years. Using <code>pip install Pillow</code> should get you the correct version. Alternatively, the pytorch extension <code>torchvision</code> has Pillow as a dependency, so if you <code>pip install torchvision</code> you should get a working Pillow.</p> <p>I'll mostly only refer to the <code>Image</code> Module (Documentation), ImageColor and ImageChops may also be relevant to your tasks.</p> <p>We'll also need direct access to Numpy for conversions and displaying, we import as <code>np</code> for ease of typing</p>","tags":["beginner","images","opencv","python","pytorch","guide"]},{"location":"blog/computer-vision-part-1/#note-older-guides-on-pil-import-it-differently-you-probably-want-to-use-from-pil-import-image","title":"NOTE Older guides on PIL import it differently. You probably want to use <code>from PIL import Image</code>","text":"<p>Reference</p> Python<pre><code>import PIL\nfrom PIL import Image\nimport numpy as np\nprint(PIL.__version__)\nprint(np.__version__)\n</code></pre> Text Only<pre><code>7.1.1\n1.18.1\n</code></pre>","tags":["beginner","images","opencv","python","pytorch","guide"]},{"location":"blog/computer-vision-part-1/#9b-loading-images-with-pil","title":"9b. Loading Images with PIL","text":"<p>Not much different from OpenCV, just a different command</p> Python<pre><code>pil_images = []\nfor file_path in file_names:\n    # Use open() instead of imread()\n    loaded_image = Image.open(file_path)\n    pil_images.append(loaded_image)\n\n# Use .size instead of .shape, which yields (Width, Height), unlike numpy ndarray.shape, which is (Height, Width)\nprint(f\"First PIL image type: {type(pil_images[0])}, size: {pil_images[0].size}, Image Mode: {pil_images[0].mode}\")\nprint(f\"OpenCV image type: {type(open_cv_images[0])}, size: {open_cv_images[0].shape}\")\n\n# In most cases we should be able to view PIL images just by calling the variable. I'm not sure what notebook setups this works in\npil_images[0]\n</code></pre> Text Only<pre><code>First PIL image type: &lt;class 'PIL.PngImagePlugin.PngImageFile'&gt;, size: (256, 224), Image Mode: RGB\nOpenCV image type: &lt;class 'numpy.ndarray'&gt;, size: (224, 256, 3)\n</code></pre>","tags":["beginner","images","opencv","python","pytorch","guide"]},{"location":"blog/computer-vision-part-1/#9c-display-pil-images-in-matplotlib","title":"9c. Display PIL images in matplotlib","text":"<p>One way to compare with OpenCV images is using matplotlib, but we need to convert. So we'll use the numpy conversion <code>asarray()</code> to get something matplotlib can use.</p> <p>As you should see, PIL loads images in RGB format by default.</p> Python<pre><code>numpy_from_pil = np.asarray(pil_images[0])\nprint(f\"PIL size: {pil_images[0].size}, numpy shape: {numpy_from_pil.shape}\")\nplt.imshow(numpy_from_pil)\n</code></pre> Text Only<pre><code>PIL size: (256, 224), numpy shape: (224, 256, 3)\n\n\n\n\n\n&lt;matplotlib.image.AxesImage at 0x7f52a579de90&gt;\n</code></pre>","tags":["beginner","images","opencv","python","pytorch","guide"]},{"location":"blog/computer-vision-part-1/#9d-convert-pil-image-colors","title":"9d. Convert PIL image colors","text":"<p>This is a good time to talk about <code>in-place</code> vs <code>out-of-place</code> operations. In-Place means something affects the object directly, and thus will have some effect on the object. Out of Place means the original object isn't altered during the operation and a brand new object is returned by the function. </p> <p>Most operations we're using here are Out of Place, so be sure to save the result in a new variable assignment (as in <code>gray_pil = pil_image.convert()</code>) or append the result to a list.</p> <p>But be on the look out for In-Place operations, especially in Numpy. They use less memory because they don't save two copies of the thing you're working with, but they'll affect your original data, so side effect bugs may occur in your program.</p> <p>For displaying, we'll convert to numpy pre-emptively</p> <p>See Pillow image modes for some more info</p> <ul> <li>\"1\" is 1-bit pixels, black or white</li> <li>\"L\" is 8-bit black and white (0...255 scale)</li> <li>\"RGB\" is 8-bit RGB</li> </ul> Python<pre><code>input_images = []\ngray_images = []\nfor pil_image in pil_images:\n    numpy_from_pil = np.asarray(pil_image)\n    input_images.append(numpy_from_pil)\n\n    gray_pil = pil_image.convert(mode=\"L\")\n    gray_numpy = np.asarray(gray_pil)\n    gray_images.append(gray_numpy)\n\nall_images = [*input_images, *gray_images]\ndisplay_image_comparison(all_images, len(input_images))\n</code></pre>","tags":["beginner","images","opencv","python","pytorch","guide"]},{"location":"blog/computer-vision-part-1/#10a-saving-your-images","title":"10a. Saving your images","text":"<p>After a long hard day of image processing and converting, you probably want to save the results so that your computer doesn't have to do all that hard work all over again.</p> <p>NOTE The middle image that displays correctly is actually backwards to OpenCV</p> <p>We'll use a directory that is at the same level as our <code>.ipynb</code> in our filesystem, so we can reach it with <code>./out_images</code> as opposed to a full absolute path (ex. <code>/home/username/projects/segmentation/outputs</code>)</p> Python<pre><code>import os\noutput_dir = './out_images'\nos.makedirs(output_dir, exist_ok=True)\n</code></pre> Python<pre><code>image_to_save = open_cv_images[0]\nbgr_image_to_save = rgb_images[0]\npil_image_to_save = pil_images[0]\nprint(f\"opencv image type: {type(image_to_save)}, shape: {image_to_save.shape}\")\nprint(f\"rgb converted opencv image type: {type(bgr_image_to_save)}, shape: {bgr_image_to_save.shape}\")\nprint(f\"pil image type: {type(pil_image_to_save)}, size: {pil_image_to_save.size}\")\ndisplay_images = [image_to_save, bgr_image_to_save, np.asarray(pil_image_to_save)]\ndisplay_image_comparison(display_images, 1)\n</code></pre> Text Only<pre><code>opencv image type: &lt;class 'numpy.ndarray'&gt;, shape: (224, 256, 3)\nrgb converted opencv image type: &lt;class 'numpy.ndarray'&gt;, shape: (224, 256, 3)\npil image type: &lt;class 'PIL.PngImagePlugin.PngImageFile'&gt;, size: (256, 224)\n</code></pre>  Python<pre><code># Using `os.path.join` because it makes sure the `/` dividers are correct in file_paths\n\n# cv2.imwrite(filename_string, img_ndarray)\ncv2.imwrite(os.path.join(output_dir, \"loaded_w_cv.png\"), image_to_save)\ncv2.imwrite(os.path.join(output_dir, \"loaded_and_convert_w_cv.png\"), bgr_image_to_save)\n\n# PIL my_image_variable.save(filename_string)\npil_image_to_save.save(os.path.join(output_dir, \"loaded_w_pil.png\"))\n\n# Validate our work\nprint(glob.glob(os.path.join(output_dir, \"*.png\")))\n</code></pre> Text Only<pre><code>['./out_/images/loaded_and_convert_w_cv.png', './out_/images/loaded_w_pil.png', './out_/images/loaded_w_cv.png']\n</code></pre>","tags":["beginner","images","opencv","python","pytorch","guide"]},{"location":"blog/computer-vision-part-1/#10b-saving-colors-correctly","title":"10b. Saving Colors Correctly","text":"<p>Again, OpenCV uses BGR by default on save and load, PIL uses RGB. If you converted to RGB with OpenCV you should convert color back before you save.</p> <p>So when we saved the OpenCV BGR Image <code>loaded_w_cv.png</code> it saved as a png that will be read as RGB (except by OpenCV).</p> <p>When we saved the PIL RGB Image <code>loaded_w_pil.png</code> it also saved as a png that will be read as RGB (except by OpenCV)</p> <p>To demonstrate, we'll load all 3 with PIL again, since that defaults to RGB loading</p> Python<pre><code># Using an `f` formatted string is also fine, so long as you know whether the variables have `/`'s\nsaved_files = glob.glob(f\"{output_dir}/*.png\")\n\nimages_to_display = []\nfor file_path in saved_files:\n    print(file_path)\n    raw_pil = Image.open(file_path)\n    np_img = np.asarray(raw_pil)\n    images_to_display.append(np_img)\n\ndisplay_image_comparison(images_to_display, 1)\n</code></pre> Text Only<pre><code>./out_/images/loaded_and_convert_w_cv.png\n./out_/images/loaded_w_pil.png\n./out_/images/loaded_w_cv.png\n</code></pre>","tags":["beginner","images","opencv","python","pytorch","guide"]},{"location":"blog/computer-vision-part-1/#11-wrap-up","title":"11. Wrap-up","text":"<p>Those are all the basic loading, converting, and saving operations that I've needed to use with OpenCV and PIL, working in an ipython notebook or in regular <code>.py</code> scripts</p> <p>As a final code snippet, we'll go from OpenCV Image (numpy ndarray) to PIL Image, as we haven't actually needed to do that without saving first. You'd use this if you want to pass numpy loaded arrays into a PIL-based image processing pipeline (ex. torchvision transforms)</p> <p>NOTE Again check your color conversions. <code>Image.fromarray()</code> expects an array in RGB form and will treat whatever 3-channel array you give it as such.</p> Python<pre><code>cv_image_to_convert = open_cv_images[0]\nprint(f\"cv image: {type(cv_image_to_convert)}, shape {cv_image_to_convert.shape}\")\n\nnew_pil_image = Image.fromarray(cv_image_to_convert)\npil_from_rgb_cv = Image.fromarray(rgb_images[0])\nprint(f\"converted to pil: {type(new_pil_image)}, size: {new_pil_image.size}, mode: {new_pil_image.mode}\")\nprint(f\"rgb_cv image converted to pil: {type(pil_from_rgb_cv)}, size: {pil_from_rgb_cv.size}, mode: {pil_from_rgb_cv.mode}\")\nto_display = [cv_image_to_convert, np.asarray(new_pil_image), np.asarray(pil_from_rgb_cv)]\ndisplay_image_comparison(to_display, 1)\n</code></pre> Text Only<pre><code>cv image: &lt;class 'numpy.ndarray'&gt;, shape (224, 256, 3)\nconverted to pil: &lt;class 'PIL.Image.Image'&gt;, size: (256, 224), mode: RGB\nrgb_cv image converted to pil: &lt;class 'PIL.Image.Image'&gt;, size: (256, 224), mode: RGB\n</code></pre>","tags":["beginner","images","opencv","python","pytorch","guide"]},{"location":"blog/computer-vision-part-2/","title":"Computer Vision Part 2: Load Tagged Data With Pytorch","text":"<p>From Images and Masks to Loading Data</p> <p>In this we'll go take a set of Images and Target labels (black and white mask images), transform them into Pytorch Tensors, and set them up in a Dataloader to be fed into a Neural Net model</p> <p>We'll be using PIL / Pillow to load images, as torchvision has built in support for that in its transforms.</p> <p>Since Masks are images we won't need to load them with numpy</p>","tags":["moderate","machine-learning","python","pytorch","guide"]},{"location":"blog/computer-vision-part-2/#0-python-setup","title":"0. Python Setup","text":"<p>Imports</p> Python<pre><code>#Jupyter Notebook Specific\n%matplotlib inline\nimport matplotlib\nimport matplotlib.pyplot as plt\nprint(f\"matplotlib loaded: {matplotlib.__version__}\")\n</code></pre> Text Only<pre><code>matplotlib loaded: 3.2.1\n</code></pre> Python<pre><code>import os\nimport glob\nfrom collections import namedtuple\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nimport torchvision\n\nimport PIL\nfrom PIL import Image\nprint(f\"torch {torch.__version__}, torchvision {torchvision.__version__}, PIL {PIL.__version__}, loaded\")\n</code></pre> Text Only<pre><code>torch 1.4.0, torchvision 0.5.0, PIL 7.1.1, loaded\n</code></pre>","tags":["moderate","machine-learning","python","pytorch","guide"]},{"location":"blog/computer-vision-part-2/#1-image-and-target-loading","title":"1. Image and Target Loading","text":"<p>We also want to make sure we're loading our images and targets as we intend, so let's double check that first</p> <p>Our example will be to segment what is UI and what is not UI, so we can get away with just one color to signal UI. </p> <p>We're treating the target as binary, so 'L' or '1' Mode in PIL should work. We most care about it being binary 0 and 1 after transforming to torch tensor</p> <p>SIDE-NOTE In a multi-class segmentation problem (e.x. self-driving cars segmenting 'road', 'sky', 'people', 'cars', etc.) we often use different colors to represent different classes in our target data, which requires an additional step to our data-loading pipeline. The colors used don't matter, so long as they are unique and consistent; since the color values are known, we make a pytorch tensor of all 0's for each class (which is the same width and height as our target segmentation image), then fill each with a 1 only where the target segmentation pixels are equal to that specific class color value.</p> <p>One caveat to this is that you cannot have overlapping segmentations. You'd need to either save a segmentation image for each class, save the segmentations as multi-dimensional (as many dimensions as you have classes) tensors / arrays, or perform some kind of bit shifting or mod operations on pixel values to allow multiple class-colors to overlap and add together.</p> Python<pre><code>test_image_path = os.path.join('.', 'test_inputs', '1_input.png')\ntest_target_path = os.path.join('.', 'test_inputs', '1_target.jpg')\n\nimage = Image.open(test_image_path).convert('RGB')\ntarget = Image.open(test_target_path).convert('1')\n\nwidth, height = image.size\n# Hacky image grid with PIL, torchvision has a decent built in function for use with tensors, and of course matplotlib has subplots()\ntest_image = Image.new('RGB', (width*2, height))\ntest_image.paste(image, (0,0))\ntest_image.paste(target, (width,1))\n\nprint(f\"(Image, Target) loaded with pil: {type(image), type(target)}, size: {(image.size), (target.size)}, mode: {(image.mode), (target.mode)}\")\n# Check our scales and colors at load. We'll need to check this again after converting to tensors\nprint(f\"Image extrema R, G, B: {image.getextrema()}, Target extrema and palette B&amp;W: {target.getextrema()}, {target.histogram()}\")\ntest_image\n</code></pre> Text Only<pre><code>(Image, Target) loaded with pil: (&lt;class 'PIL.Image.Image'&gt;, &lt;class 'PIL.Image.Image'&gt;), size: ((256, 224), (256, 224)), mode: ('RGB', '1')\nImage extrema R, G, B: ((0, 255), (0, 251), (0, 255)), Target extrema and palette B&amp;W: (0, 255), [43956, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 13388]\n</code></pre>","tags":["moderate","machine-learning","python","pytorch","guide"]},{"location":"blog/computer-vision-part-2/#2-pytorch-dataset","title":"2. Pytorch Dataset","text":"<p>Pytorch has 2 tools that make feeding data into your deep learning model easier and rather generalizable to different tasks, data, and domains. These are the <code>Dataset</code> and <code>Dataloader</code> in the <code>torch.utils.data</code> module</p> <p>The Dataset Class handles the length of our data and how we receive each sample item. Note that the type of a sample <code>dataset[x_index]</code> depends on the <code>__getitem__</code> function, but also on the <code>transform</code> that is passed in. Also note this sample in the dataset will be a single sample and not have a Batch dimension, even if it is a tensor (this matters at the DataLoader step).</p> <p>We'll start with the dataset and provide plenty of comments.</p> <p>NOTE If you're just loading Images (and not targets, e.x. in an auto-encoding task), or are loading Images by class (e.x. predicting animal or pokemon type or something), then torchvision imagefolder is probably the easiest thing to use, it returns a tuple. (sidenote it expects a root directory filled with at least 1 directory of images, that's how it seperates by class)</p> <p>Dataloading Tutorial</p> Python<pre><code># Optional, i prefer this syntax for accessing the data with data.image and data.target or data[0], data[1]\nSample = namedtuple('Sample', ['image', 'target'])\n\nclass UIImageMaskDataset(torch.utils.data.Dataset):\n    \"\"\"\n    Expects images to be .pngs in data_dir folder of form `1_input.png`, target ui masks to be .jpg files with same number as image and target `1_target.jpg`\n    Initialize\n    Param: data_dir (String) path to directory with images and targets\n    Transform: Torchvision transform to be applied on each image &amp; target pair when fetched from the dataset\n    \"\"\"\n    def __init__(self, data_dir='./test_inputs', transform=None):\n        self.data_dir = data_dir\n\n        # Get file paths\n        # We could use a more clear wildcard (like *_input.png and *_target.jpg if we set up our filenames like that).\n        self.image_list = glob.glob(f'{self.data_dir}/*.png')\n        self.target_list = glob.glob(f'{self.data_dir}/*.jpg')\n\n        # For use in methods\n        self.length = len(self.image_list)\n        self.transform = transform\n\n        # Basic Error checking, just for testing\n        if self.transform is None:\n            print('No transform on data, you probably need at least ToTensor()')\n        if len(self.image_list) != len(self.target_list):\n            print('Image list and Target list not same length')\n\n    def __len__(self):\n        \"\"\"\n        Required by Pytorch. Return the amount of samples. \n        (For overfitting you can make this function return a fixed number and __getitem__ return one image always. \n            It can even made infinite with generators, but that's an advanced topic)\n        \"\"\"\n        return self.length\n\n    def __getitem__(self, idx):\n        \"\"\"\n        Also required by Pytorch. \n        Loads input and target\n        Do transformation if necessary\n        Return a `sample` / `item`, meaning an input and target pair.\n        For efficiency usually you do the actual loading here and not in the __init__.\n            The Dataloader uses multiple processes to call __getitem__\n            (With larger amounts of RAM you can preload images)\n        Return type and format will vary from person to person, I prefer using a NamedTuple. \n            I find this to be the most compatible with the Dataloader default Collation,\n            also it's readable: you can access with `data.image` and `data.target` instead of `data[0]` and `data[1]` (which you can also still do)\n        Other Return types are typically plain tuple (usually input first, target second), and dictionary\n            dictionary and namedtuple allow you to expand your targets for doing more complex tasks (e.x. bounding boxes and segmentations and class labels)\n            without having tuples with many numbered elements\n        \"\"\"\n        screenshot_file = self.image_list[idx]\n        target_file = self.target_list[idx]\n\n        image = Image.open(screenshot_file).convert('RGB')\n        target = Image.open(target_file).convert('1')\n\n        if self.transform:\n            image, target = self.transform(image, target)\n\n        # sample = {'image': image, 'target': target}\n        return Sample(image, target)\n\ntest_dataset = UIImageMaskDataset()\nprint(f\"Created test Data: {type(test_dataset)}.\") \nprint(f\"Len: {len(test_dataset)}\")\nprint(f\"Type of first item: {type(test_dataset[0])}. \")\nprint(f\"Type of first item.image: {type(test_dataset[0].image)}\")\nprint(f\"Type of first item.target: {type(test_dataset[0].target)}\")\n</code></pre> Text Only<pre><code>No transform on data, you probably need at least ToTensor()\nCreated test Data: &lt;class '__main__.UIImageMaskDataset'&gt;.\nLen: 1\nType of first item: &lt;class '__main__.Sample'&gt;. \nType of first item.image: &lt;class 'PIL.Image.Image'&gt;\nType of first item.target: &lt;class 'PIL.Image.Image'&gt;\n</code></pre>","tags":["moderate","machine-learning","python","pytorch","guide"]},{"location":"blog/computer-vision-part-2/#3-load-the-folder-into-a-dataset","title":"3. Load the folder into a Dataset","text":"<p>Depending on your task you may not need to transform it at all (ex. a class name like 'cheese')</p> <p>In our case we need to make sure whatever we do to the Image we also do to the Target mask. Most importantly, we need to convert both from PIL images to Pytorch Tensors so that they can be ingested in the model / network.</p> <p>We'll also perform a resize, because reducing image size reduces the amount of data passing through the network and also to demonstrate Composing a chain of transforms</p> <p>We use transform <code>ToTensor()</code> from torchvision transforms because it's the simplest conversion from PIL to Tensor. You usually want to do this last, depending on the other transforms of course</p> <p>Always remember your target image!</p> <p>But this function only takes one input... So we can either use the 'Functional' version or write our own class. We'll take the class route, which lets you chain more easily.</p> <p>If using random transforms on images and image targets make sure the same randomness is applied to both (you'll probably need to write your own transforms)</p> <p>In a more advanced case we can chain together more transforms like crops, flips, and rotations to vary the training data. Typically we don't test / validate on data with as many transforms though. </p> <p>One rather important transform we're ignoring for now is image normalization, which brings all of your dataset images within the same range using the entire dataset mean and standard deviation.</p> <p>Torchvision <code>ToTensor()</code> normalizes to a 0-1 scale which will keep our model's gradients in check. To go further we'd get the mean and std of our overfit image or dataset</p> <p>NOTE many people import torchvision.transforms as T or similar. Feel free to do the same. I usually write my own transforms (ex. <code>input_target_transforms.py</code> for auto encoders, and import as <code>TT</code> or <code>Transforms</code> for clarity)</p> Python<pre><code>class ImageTargetToTensor(object):\n    def __call__(self, image, target):\n        # Transform Classes built with __call__ act kind of like a function once you assign an instance to a variable\n        tv_tt = torchvision.transforms.ToTensor()\n        # transform image to tensor with torchvision transform\n        image = tv_tt(image)\n        # same with target\n        target = tv_tt(target)\n        return (image, target)\n\n\nclass ImageTargetResize(object):\n    def __init__(self, size):\n        \"\"\"\n        size: int will reduce shorter side to size param\n             tuple of (int, int) will resize to (height, width) from size param\n        returns tuple of (resized_image, resized_target)\n        \"\"\"\n        self.size = size\n\n    def __call__(self, image, target):\n        resize_transform = torchvision.transforms.Resize(self.size)\n        image = resize_transform(image)\n        target = resize_transform(target)\n        return (image, target)\n\nclass ImageTargetCompose(object):\n    def __init__(self, transforms):\n        \"\"\"\n        transforms: list of transforms that take in image and target and return tuples of (transformed_image, transformed_target)\n        returns tuple of (image, target) after going through each transform in list\n        \"\"\"\n        self.transforms = transforms\n\n    def __call__(self, image, target):\n        for t in self.transforms:\n            image, target = t(image, target)\n        return image, target\n\ntraining_transform = ImageTargetCompose([\n    ImageTargetResize((256,256)),\n    ImageTargetToTensor()\n])\n\nUI_DATASET = UIImageMaskDataset(data_dir=\"./test_inputs\", transform=training_transform)\nsample = UI_DATASET[0]\n\nprint(f\"Created Ui dataset: {type(UI_DATASET)}.\") \nprint(f\"Len: {len(UI_DATASET)}\")\nprint(f\"Type of first item: {type(sample)}. len {len(sample)} \")\nprint(f\"item.image  Type: {type(sample.image)}. Datatype: {sample.image.dtype}.\")\nprint(f\"            Shape: {sample.image.shape}, Extrema: ({sample.image.min()}, {sample.image.max()}), Unique: ({len(sample.image.unique())} unique colors)\")\nprint(f\"item.target Type: {type(sample.target)}. Datatype: {sample.target.dtype}.\")\nprint(f\"            Shape: {sample.target.shape}, Extrema: ({sample.target.min()}, {sample.target.max()}), Unique: ({sample.target.unique()})\")\n</code></pre> Text Only<pre><code>Created Ui dataset: &lt;class '__main__.UIImageMaskDataset'&gt;.\nLen: 1\nType of first item: &lt;class '__main__.Sample'&gt;. len 2 \nitem.image  Type: &lt;class 'torch.Tensor'&gt;. Datatype: torch.float32.\n            Shape: torch.Size([3, 256, 256]), Extrema: (0.0, 1.0), Unique: (247 unique colors)\nitem.target Type: &lt;class 'torch.Tensor'&gt;. Datatype: torch.float32.\n            Shape: torch.Size([1, 256, 256]), Extrema: (0.0, 1.0), Unique: (tensor([0., 1.]))\n</code></pre> Python<pre><code># As a double check, we should make sure our transforms do what we intend, especially if rotating / cropping image and target\n# We need to jump through one hoop by making the target seem like an RGB image to let torchvision see it\n#  We also need to jump through another hoop to display a torch tensor as an image, using torchvision transforms ToPILImage works, numpy and matplotlib also work\n\nviz_image = sample.image\nviz_target = sample.target\nviz_target = viz_target.repeat((3,1,1))\nprint(viz_target.shape)\ngrid = torchvision.utils.make_grid([viz_image, viz_target])\npil_grid = torchvision.transforms.ToPILImage()(grid)\npil_grid\n</code></pre> Text Only<pre><code>torch.Size([3, 256, 256])\n</code></pre>","tags":["moderate","machine-learning","python","pytorch","guide"]},{"location":"blog/computer-vision-part-2/#4-load-into-dataloader","title":"4. Load into DataLoader","text":"<p>This will be the last part we cover, as after this is running training samples through a model!</p> <p>The Pytorch Dataloader works pretty efficiently and takes a lot of effort out of batching your samples when looping through your dataset.</p> <p>In most cases you can use <code>shuffle=True</code> on training and <code>shuffle=False</code> on validation / test and not worry about a sampler / batch sampler.</p> <p><code>drop_last=True</code> will make it so you end on the last full batch of data (ex if you have 999 samples and batch size of 4 then the last batch could only have 3 samples, this flag ignores that un-full batch of 3)</p> <p><code>batch_size</code> tends to depend on the size of your GPU(s), <code>num_workers</code> on the other hand scales with CPU processor cores that you have (probably 4, 2, maybe 1, maybe 8, if you have 16 you probably know; depends on your machine).</p> <p>The dataloader gives results batched along a new dimension, meaning each tensor representing a whole batch of images has dimensions like this: <code>[ Batch_Size, Channels, Height, Width ]</code>. It does this using the <code>collate function</code>, which I generally leave default and is why I return tuples in Dataset implementations</p> <p>NOTE You generally only want to touch your dataloaders (i.e. get a batch from them) when you are looping through training, as they are implemented as generators, which will continue sampling forward by default whenever you get a <code>next()</code> item from it. In other words you can't get a specific index from them (you always can from the underlying dataset though), and if you access the first sample then the training may be a bit messed up because it will start from the second sample .</p> <p>Dataloader Documentation</p> Python<pre><code>TRAIN_DATALOADER = torch.utils.data.DataLoader(UI_DATASET, batch_size=1, shuffle=True, num_workers=1, drop_last=True)\nprint(f\"Length of Train loader: {len(TRAIN_DATALOADER)} batches\")\n\nfor batch_num, sample in enumerate(TRAIN_DATALOADER):\n    print(f\"batch number: {batch_num + 1}\")\n    batch_images = sample.image\n    batch_targets = sample.target\n    # one liner: batch_images, batch_targets = sample\n    print(f\"batch images  Type: {type(batch_images)}. Datatype: {batch_images.dtype}.\")\n    print(f\"            Shape: {batch_images.shape}, Extrema: ({batch_images.min()}, {batch_images.max()}), Unique: ({len(batch_images.unique())} unique colors)\")\n    print(f\"batch targets Type: {type(batch_targets)}. Datatype: {batch_targets.dtype}.\")\n    print(f\"            Shape: {batch_targets.shape}, Extrema: ({batch_targets.min()}, {batch_targets.max()}), Unique: ({batch_targets.unique()})\")\n</code></pre> Text Only<pre><code>Length of Train loader: 1 batches\nbatch number: 1\nbatch images  Type: &lt;class 'torch.Tensor'&gt;. Datatype: torch.float32.\n            Shape: torch.Size([1, 3, 256, 256]), Extrema: (0.0, 1.0), Unique: (247 unique colors)\nbatch targets Type: &lt;class 'torch.Tensor'&gt;. Datatype: torch.float32.\n            Shape: torch.Size([1, 1, 256, 256]), Extrema: (0.0, 1.0), Unique: (tensor([0., 1.]))\n</code></pre>","tags":["moderate","machine-learning","python","pytorch","guide"]},{"location":"blog/computer-vision-part-2/#5-bonus-overfit-dataset","title":"5. BONUS: Overfit Dataset","text":"<p>We've seen how to load and transform images, and this will almost definitely work with a larger folder of images, and a larger batch size, but lets make sure.</p> Python<pre><code>class OverfitDataset(torch.utils.data.Dataset):\n\n    def __init__(self, input_file='./test_inputs/1_input.png', target_file='./test_inputs/1_target.jpg', number_of_samples=2000, transform=None):\n        self.image = Image.open(input_file).convert('RGB')\n        self.target = Image.open(target_file).convert('1')\n\n        # For use in methods\n        self.length = number_of_samples\n        self.transform = transform\n\n        # Basic Error checking, just for testing\n        if self.transform is None:\n            print('No transform on data, you probably need at least ToTensor()')\n\n    def __len__(self):\n        return self.length\n\n    def __getitem__(self, idx):\n        \"\"\"\n        We apply transform in getitem in case you want to do some random transforms in the overfit training. \n            You could just do the transform originally in init for the overfit case\n            Shouldn't be a hit to performance when just using ToTensor anyway\n        \"\"\"\n        image, target = (self.image, self.target)\n        if self.transform:\n            image, target = self.transform(image, target)\n\n        # sample = {'image': image, 'target': target}\n        return Sample(image, target)\n\noverfit_dataset = OverfitDataset(input_file='./test_inputs/1_input.png', target_file='./test_inputs/1_target.jpg', number_of_samples=100, transform=training_transform)\nsample = overfit_dataset[0]\n\nprint(f\"Created overfit dataset: {type(overfit_dataset)}.\") \nprint(f\"Len: {len(overfit_dataset)}\")\nprint(f\"Type of first item: {type(sample)}. len {len(sample)} \")\nprint(f\"item.image  Type: {type(sample.image)}. Datatype: {sample.image.dtype}.\")\nprint(f\"            Shape: {sample.image.shape}, Extrema: ({sample.image.min()}, {sample.image.max()}), Unique: ({len(sample.image.unique())} unique colors)\")\nprint(f\"item.target Type: {type(sample.target)}. Datatype: {sample.target.dtype}.\")\nprint(f\"            Shape: {sample.target.shape}, Extrema: ({sample.target.min()}, {sample.target.max()}), Unique: ({sample.target.unique()})\")\n\nOVERFIT_LOADER = torch.utils.data.DataLoader(overfit_dataset, batch_size=16, shuffle=True, num_workers=1, drop_last=True)\nprint(f\"Length of overfit loader: {len(OVERFIT_LOADER)} batches\")\nfor batch_num, sample in enumerate(OVERFIT_LOADER):\n    print(\"----------------------------------\")\n    print(f\"batch number: {batch_num + 1}\")\n    batch_images = sample.image\n    batch_targets = sample.target\n    # one liner: batch_images, batch_targets = sample\n\n    print(f\"batch images  Type: {type(batch_images)}.\")\n    print(f\"            Shape: {batch_images.shape}\")\n    print(f\"batch targets Type: {type(batch_targets)}.\")\n    print(f\"            Shape: {batch_targets.shape}\")\n</code></pre> Text Only<pre><code>Created overfit dataset: &lt;class '__main__.OverfitDataset'&gt;.\nLen: 100\nType of first item: &lt;class '__main__.Sample'&gt;. len 2 \nitem.image  Type: &lt;class 'torch.Tensor'&gt;. Datatype: torch.float32.\n            Shape: torch.Size([3, 256, 256]), Extrema: (0.0, 1.0), Unique: (247 unique colors)\nitem.target Type: &lt;class 'torch.Tensor'&gt;. Datatype: torch.float32.\n            Shape: torch.Size([1, 256, 256]), Extrema: (0.0, 1.0), Unique: (tensor([0., 1.]))\nLength of overfit loader: 6 batches\n----------------------------------\nbatch number: 1\nbatch images  Type: &lt;class 'torch.Tensor'&gt;.\n            Shape: torch.Size([16, 3, 256, 256])\nbatch targets Type: &lt;class 'torch.Tensor'&gt;.\n            Shape: torch.Size([16, 1, 256, 256])\n----------------------------------\nbatch number: 2\nbatch images  Type: &lt;class 'torch.Tensor'&gt;.\n            Shape: torch.Size([16, 3, 256, 256])\nbatch targets Type: &lt;class 'torch.Tensor'&gt;.\n            Shape: torch.Size([16, 1, 256, 256])\n----------------------------------\nbatch number: 3\nbatch images  Type: &lt;class 'torch.Tensor'&gt;.\n            Shape: torch.Size([16, 3, 256, 256])\nbatch targets Type: &lt;class 'torch.Tensor'&gt;.\n            Shape: torch.Size([16, 1, 256, 256])\n----------------------------------\nbatch number: 4\nbatch images  Type: &lt;class 'torch.Tensor'&gt;.\n            Shape: torch.Size([16, 3, 256, 256])\nbatch targets Type: &lt;class 'torch.Tensor'&gt;.\n            Shape: torch.Size([16, 1, 256, 256])\n----------------------------------\nbatch number: 5\nbatch images  Type: &lt;class 'torch.Tensor'&gt;.\n            Shape: torch.Size([16, 3, 256, 256])\nbatch targets Type: &lt;class 'torch.Tensor'&gt;.\n            Shape: torch.Size([16, 1, 256, 256])\n----------------------------------\nbatch number: 6\nbatch images  Type: &lt;class 'torch.Tensor'&gt;.\n            Shape: torch.Size([16, 3, 256, 256])\nbatch targets Type: &lt;class 'torch.Tensor'&gt;.\n            Shape: torch.Size([16, 1, 256, 256])\n</code></pre> Python","tags":["moderate","machine-learning","python","pytorch","guide"]},{"location":"blog/computer-vision-part-3/","title":"Computer Vision Part 3: Train U-Net Architecture to predict UI segmentation","text":"<p>Now we've got data in a Dataset format ready to be processed as tensors by a neural net.</p> <p>The code for the full lightning module, model, dataset, and transforms can be found in the <code>ui_segmentation_system.py</code> file</p> <p>We'll be using pytorch lightning, an open-source package whose goal is to organize your pytorch code and take away a lot of the boiler plate code needed to run loops and do logging. </p> <p>It'll do a lot of things auto-magically for us, but we'll still be writing the core training logic of our prediction network. It also gives us a lot of powerful training flags out of the box, such as profiling, early stopping, gradient accumulation, model weights summary, and access to a handful of popular logging frameworks.</p> <p>All Pytorch Lightning traning flags</p> Python<pre><code>#Jupyter Notebook Specific\n%matplotlib inline\nimport matplotlib\nimport matplotlib.pyplot as plt\nprint(f\"matplotlib loaded: {matplotlib.__version__}\")\n</code></pre> Text Only<pre><code>matplotlib loaded: 3.2.1\n</code></pre> Python<pre><code>import os\nimport glob\nimport logging as log\nimport json\nimport time\nimport datetime\nfrom collections import namedtuple\nfrom typing import Tuple\nimport argparse\n\nimport PIL\nfrom PIL import Image\n\nimport numpy as np\n\nimport torch\nfrom torch import Tensor\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nimport torchvision\n\nimport pytorch_lightning\n\nimport ui_segmentation_system as UI_SEG\nprint(f\"torch {torch.__version__}, torchvision {torchvision.__version__}, PIL {PIL.__version__}, pytorch-lightning {pytorch_lightning.__version__} loaded\")\n</code></pre> Text Only<pre><code>torch 1.4.0, torchvision 0.5.0, PIL 7.1.1, pytorch-lightning 0.7.1 loaded\ntorch 1.4.0, torchvision 0.5.0, PIL 7.1.1, pytorch-lightning 0.7.1 loaded\n</code></pre>","tags":["advanced","machine-learning","python","pytorch","guide"]},{"location":"blog/computer-vision-part-3/#1-lightning-module","title":"1. Lightning Module","text":"<p>This contains all the information for what model to use, what dataset to use, how to treat each batch, and any important metrics or images to log</p> Python<pre><code>class UISegmentationSystem(pytorch_lightning.LightningModule):\n    \"\"\"\n    Predict UI segments of videogame screenshots using a Convolutional Neural Network.\n    This class can be used for training, validation, and prediction.\n    \"\"\"\n    def __init__(self, hparams):\n        \"\"\"\n        Requires args for batch size, data folder, output folder, etc.\n        \"\"\"\n        super(UISegmentationSystem, self).__init__()\n        log.info(\"Running __init__\")\n        self.hparams = hparams\n        # I usually initialize like this to use it as a boolean condition elsewhere\n        self.dataset = None\n        # We could use any kind of network architecture, or set it via an arg flag\n        self.network = UI_SEG.UNet(num_in_channels=3, num_out_channels=1, max_features=256)\n\n        self.models_dir = f\"{hparams.output_dir}/model_checkpoints\"\n        self.images_dir = f\"{hparams.output_dir}/step_images\"\n        os.makedirs(self.models_dir, exist_ok=True)\n        os.makedirs(self.images_dir, exist_ok=True)\n\n\n    def forward(self, input):\n        \"\"\"\n        Send an input image (tensor with batch dimension) through the prediction network and return the prediction (tensor with same batch dimension)\n        This is the same as the forward method of a pytorch nn module, we just have our network in a seperate file to have fewer lines here\n        \"\"\"\n        return self.network(input)\n\n    # ---------------------\n    # TRAINING SETUP\n    # ---------------------\n    def configure_optimizers(self):\n        \"\"\"\n        REQUIRED\n        Adam is a pretty popular optimizer, we'll use it\n        Lightning handles the zero_grad() and step() calls\n        :return: optimizer to use in training \n            (optionally return list of optimizers and a list of learning rate schedulers in a tuple)\n        \"\"\"\n        log.info(\"Configuring optimizer\")\n        optimizer = torch.optim.Adam(\n            self.network.parameters(),\n            lr=self.hparams.learning_rate\n        )\n        # scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer,\n        #     factor=0.1)\n        # return [optimizer], [scheduler]\n        return optimizer\n\n    def prepare_data(self):\n        \"\"\"\n        REQUIRED\n        Lightning calls this once at the beginning of training / inference, it gets called once no matter how many GPUs you're using\n        \"\"\"\n        # Resize to square and turn PIL image into torch Tensor\n        training_transform = UI_SEG.ImageTargetCompose([\n            UI_SEG.ImageTargetResize((self.hparams.image_size, self.hparams.image_size)),\n            UI_SEG.ImageTargetToTensor()\n        ])\n        log.info('Prepped training transform')\n        self.dataset = UI_SEG.UIImageMaskDataset(data_dir=self.hparams.dataset, transform=training_transform, overfit_num=self.hparams.overfit_num)\n\n        self.val_dataset = UI_SEG.UIImageMaskDataset(data_dir=self.hparams.dataset, transform=training_transform, overfit_num=self.hparams.overfit_num)\n\n        log.info(\n            f'Train ({len(self.dataset)} samples) and Val ({len(self.val_dataset)} samples) datasets loaded')\n\n        # Configure val set with 20% of full data. or do k-fold, up to you\n        # num_samples = len(self.dataset)\n        # num_train = int(0.8 * num_samples)\n        # indices = torch.randperm(num_samples).tolist()\n        # self.dataset = torch.utils.data.Subset(\n        #     self.dataset, indices[0:num_train])\n        # self.val_dataset = torch.utils.data.Subset(\n        #     self.val_dataset, indices[num_train:])\n        # log.info(f'Full Dataset loaded: len: {num_samples}')\n\n    def train_dataloader(self):\n        \"\"\"\n        REQUIRED. Return pytorch dataloader to be used in training loop\n        Lightning passes batches to the `training_step()` function below. \n        It also handles making a distributed sampler if you're using multiple GPUs.\n        \"\"\"\n        log.info(\"Fetching training dataloader\")\n        return torch.utils.data.DataLoader(self.dataset, batch_size=self.hparams.batch_size, drop_last=True, shuffle=True)\n\n    def val_dataloader(self):\n        \"\"\"\n        REQUIRED if doing val loop. Return pytorch dataloader to be used in validation loop\n        Lighting passes each batch to the `validation_step()` function below.\n        \"\"\"\n        log.info(\"Fetching validation dataloader\")\n        # Batch_size increase if using a full val set\n        return torch.utils.data.DataLoader(self.val_dataset, batch_size=1, shuffle=False)\n\n    def loss_function(self, targets, predictions):\n        \"\"\"\n        Used in training and validation to backpropogate into the network weights and assess how well our model is doing.\n        Combines sigmoid with Binary Cross Entropy for numerical stability.\n        Can include pos_weight to increase recall of underrepresented classes\n        :param targets: ground truth segmentation mask\n        :param predictions: output of the network\n        \"\"\"\n        bce_loss = F.binary_cross_entropy_with_logits(predictions, targets)\n        return bce_loss\n\n    # -----------------------------\n    # TRAINING LOOP\n    # -----------------------------\n    def training_step(self, batch, batch_idx):\n        \"\"\"\n        Lightning calls this inside the training loop\n        :param batch: Lightning yields a batch from the dataloader, in our case it is a tuple of (images, targets) already batched (tensors of dimesions [Batch, Channels, Height, Width])\n        :param batch_idx: batch_number in current epoch\n        :return: dict\n            - loss -&gt; tensor scalar [REQUIRED]\n            - progress_bar -&gt; Dict for progress bar display. Must have only tensors\n            - log -&gt; Dict of metrics to add to logger. Must have only tensors (no images, etc)\n        \"\"\"\n        # forward pass\n        images, targets = batch\n        predictions = self.forward(images)\n        # calculate loss\n        loss_val = self.loss_function(targets, predictions)\n        to_log = {'training_loss': loss_val}\n        output = {\n            'loss': loss_val,  # required\n            'progress_bar': to_log,\n            'log': to_log,\n        }\n\n        return output\n\n    # ---------------------\n    # VALIDATION LOOP\n    # ---------------------\n\n    def validation_step(self, batch, batch_number):\n        \"\"\"\n        Called in validation loop with model in eval mode\n        :param batch: Lightning yields a batch from the dataloader, in our case it is a tuple of (images, targets) already batched (tensors of dimesions [Batch, Channels, Height, Width])\n        :param batch_number: Lightning tells us what batch number we're on. We'll use this to choose when to log some images\n        :return: dict\n            - val_loss -&gt; tensor scalar [Useful for Early Stopping]\n            - progress_bar -&gt; Dict for progress bar display. Must have only tensors\n            - log -&gt; Dict of metrics to add to logger. Must have only tensors (no images, etc)\n        \"\"\"\n        images, targets = batch\n        predictions = self.forward(images)\n        loss_val = self.loss_function(targets, predictions)\n        if batch_number == 0:\n            self.log_validation_images(images, predictions, targets, step=self.global_step)\n        to_log = {'val_loss': loss_val}\n        output = {\n            'val_loss': loss_val,\n            'progress_bar': to_log,\n            'log': to_log\n        }\n        return output\n\n    def log_validation_images(self, inputs, predictions, targets, step=0):\n        # Turn 1 channel binary masks into 3 channel 'rgb' \n        model_outputs = torch.cat((predictions, predictions, predictions), dim=1)\n        viz_targets = torch.cat((targets, targets, targets), dim=1)\n        img_grid = torchvision.utils.make_grid(torch.cat([inputs, model_outputs, viz_targets]), nrow=inputs.shape[0], padding=20, normalize=True, range=(0,1))\n\n        plt.imshow(np.asarray(img_grid.detach().permute((1,2,0)).cpu()))\n        torchvision.utils.save_image(img_grid, os.path.join(self.images_dir, f\"step_{step}_val_batch.png\"))\n</code></pre>","tags":["advanced","machine-learning","python","pytorch","guide"]},{"location":"blog/computer-vision-part-3/#2-setup-training","title":"2. Setup Training","text":"<p>We'll want to be able to set different arguments for our training with either the command line or programatically</p> <p>This is an overfit example using the Pytorch Lightning <code>Trainer</code> to do all the looping for us. </p> <p>Note that Lightning uses early stopping by default conditioned on <code>val_loss</code>, this can be changed or <code>max_epochs</code> can be limited.</p> Python<pre><code>from argparse import Namespace\ndef setup_and_train():\n    # ------------------------\n    # 0 SETUP\n    # ------------------------\n    log.getLogger().setLevel(log.INFO)\n\n    parser = argparse.ArgumentParser(\n        description='UI Segmentation Training')\n    parser = pytorch_lightning.Trainer.add_argparse_args(parser)\n    # data\n    parser.add_argument('-d', '--dataset',\n                        default=os.path.join('.', 'test_inputs'), type=str)\n    parser.add_argument(\"-o\", \"--output-dir\", type=str, default=\"./test_outputs\", help=\"where to save validation images from training\")\n\n    parser.add_argument('-b', '--batch-size', default=4, type=int)\n    parser.add_argument('-lr', '--learning-rate', default=0.001, type=float,\n                        help='initial learning rate')\n    parser.add_argument(\"--image-size\", type=int, default=256,\n                        help=\"size of training images, default is 256\")\n    parser.add_argument(\"--overfit-num\", type=int, default=0,\n                        help=\"Set to positive number to overfit on one image, default is 0 (no overfitting)\")\n\n    parser.add_argument(\"--seed\", type=int, default=4747, help=\"random seed\")\n\n    # args = parser.parse_args()\n    args = Namespace(**{\n        'dataset': './test_inputs',\n        'output_dir': './test_outputs',\n        'batch_size': 4,\n        'learning_rate': 0.001,\n        'image_size': 256,\n        'overfit_num': 100,\n        'seed': 4747\n    })\n    ui_predictor = UISegmentationSystem(args)\n    trainer = pytorch_lightning.Trainer(default_save_path=args.output_dir, progress_bar_refresh_rate=50, gpus='0', profiler=True, weights_summary='top', max_epochs=5)\n    trainer.fit(ui_predictor)\nsetup_and_train()\n</code></pre> Text Only<pre><code>INFO:root:Running __init__\nINFO:root:GPU available: True, used: True\nINFO:root:VISIBLE GPUS: 0\nINFO:root:Prepped training transform\nINFO:root:Train (100 samples) and Val (100 samples) datasets loaded\nINFO:root:Configuring optimizer\nINFO:root:\n  | Name    | Type | Params\n-----------------------------\n0 | network | UNet | 1 M   \nINFO:root:Fetching validation dataloader\nINFO:root:Fetching training dataloader\nINFO:root:Fetching validation dataloader\nEpoch 1:  40%|\u2588\u2588\u2588\u2588      | 50/125 [00:00&lt;00:00, 647.70it/s, loss=0.851, training_loss=0.851, v_num=1]\nEpoch 1:  80%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  | 100/125 [00:01&lt;00:00, 69.42it/s, loss=0.851, training_loss=0.851, v_num=1]\nEpoch 1: : 150it [00:01, 88.18it/s, loss=0.519, training_loss=0.413, v_num=1]\nEpoch 2:  40%|\u2588\u2588\u2588\u2588      | 50/125 [00:00&lt;00:00, 88.18it/s, loss=0.515, training_loss=0.41, v_num=1]\nEpoch 2:  80%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  | 100/125 [00:01&lt;00:00, 83.01it/s, loss=0.515, training_loss=0.41, v_num=1]\nEpoch 2: : 150it [00:01, 102.55it/s, loss=0.449, training_loss=0.349, v_num=1]\nEpoch 3:  40%|\u2588\u2588\u2588\u2588      | 50/125 [00:00&lt;00:00, 102.55it/s, loss=0.447, training_loss=0.347, v_num=1]\nEpoch 3:  80%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  | 100/125 [00:01&lt;00:00, 92.17it/s, loss=0.447, training_loss=0.347, v_num=1]\nEpoch 3: : 150it [00:01, 113.91it/s, loss=0.406, training_loss=0.297, v_num=1]\nEpoch 4:  40%|\u2588\u2588\u2588\u2588      | 50/125 [00:00&lt;00:00, 113.91it/s, loss=0.405, training_loss=0.295, v_num=1]\nEpoch 4:  80%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  | 100/125 [00:01&lt;00:00, 99.01it/s, loss=0.405, training_loss=0.295, v_num=1]\nEpoch 4: : 150it [00:01, 121.86it/s, loss=0.373, training_loss=0.251, v_num=1]\nEpoch 5:  40%|\u2588\u2588\u2588\u2588      | 50/125 [00:00&lt;00:00, 121.86it/s, loss=0.367, training_loss=0.249, v_num=1]\nEpoch 5:  80%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  | 100/125 [00:01&lt;00:00, 102.98it/s, loss=0.367, training_loss=0.249, v_num=1]\nEpoch 5: : 150it [00:01, 125.90it/s, loss=0.301, training_loss=0.212, v_num=1]\nEpoch 5: : 150it [00:01, 88.34it/s, loss=0.301, training_loss=0.212, v_num=1]INFO:root:\n\nProfiler Report\n\nAction                  |  Mean duration (s)    |  Total time (s) \n-----------------------------------------------------------------\non_train_start          |  4.932e-06        |  4.932e-06      \non_epoch_start          |  1.8017e-05       |  9.0087e-05     \nget_train_batch         |  0.005027         |  0.65351        \non_batch_start          |  8.6806e-06       |  0.0010851      \nmodel_forward           |  0.011722         |  1.4652         \nmodel_backward          |  0.0039355        |  0.49194        \non_after_backward       |  2.9894e-06       |  0.00037367     \noptimizer_step          |  0.0065461        |  0.81826        \non_batch_end            |  9.3449e-06       |  0.0011681      \non_epoch_end            |  7.7462e-06       |  3.8731e-05     \non_train_end            |  8.029e-06        |  8.029e-06\n</code></pre>","tags":["advanced","machine-learning","python","pytorch","guide"]},{"location":"blog/computer-vision-part-3/#3-visualize-results","title":"3. Visualize results","text":"<p>In the validation loop we set it to save a batch of pngs every epoch (when we get the first batch on an epoch) to a folder.</p> <p>Here we fetch all those files, sorted by modification time so that the first comes first and last comes last (glob doesn't sort things in a reliable order depending on OS), save a gif of the training and a copy of the last prediction to our outputs folder</p> Python<pre><code>import imageio\ngif_images = []\nfile_paths = glob.glob(f\"./test_outputs/step_images/*\")\nfile_paths = sorted(file_paths, key=os.path.getmtime)\nprint(file_paths)\nfor step_img in file_paths:\n    gif_images.append(imageio.imread(step_img))\nnum_frames = len(gif_images)\nduration = num_frames * 0.1\nimageio.mimsave(\"./test_outputs/val_outputs.gif\", gif_images, duration=duration)\nimageio.imsave(\"./test_outputs/final_output.png\", gif_images[-1])\n</code></pre> Text Only<pre><code>['./test_outputs/step_images/step_0_val_batch.png', './test_outputs/step_images/step_24_val_batch.png', './test_outputs/step_images/step_49_val_batch.png', './test_outputs/step_images/step_74_val_batch.png', './test_outputs/step_images/step_99_val_batch.png', './test_outputs/step_images/step_124_val_batch.png']\n</code></pre>","tags":["advanced","machine-learning","python","pytorch","guide"]},{"location":"blog/computer-vision-part-3/#using-html-to-display-gif","title":"Using HTML to display gif","text":"<p>If this doesn't work, you can view in the files</p>","tags":["advanced","machine-learning","python","pytorch","guide"]},{"location":"blog/cte-subquery-who-gives-a-duck/","title":"CTE vs Subquery: Who gives a \ud83e\udd86uck?","text":"","tags":["sql","data","beginner"]},{"location":"blog/cte-subquery-who-gives-a-duck/#inspiration","title":"Inspiration","text":"<ul> <li>https://duckdb.org/2023/05/26/correlated-subqueries-in-sql.html</li> <li>https://cs.emis.de/LNI/Proceedings/Proceedings241/383.pdf</li> <li>Blog Posts about CTEs vs Subqueries</li> </ul>","tags":["sql","data","beginner"]},{"location":"blog/cte-subquery-who-gives-a-duck/#tldr","title":"TL;DR","text":"<p>Rough performance numbers on M1 Macbook Pro 16GB. 100,000 students joined with 1,000,000 exam scores.</p> <p>Follow along from the github repo if you're comfortable with docker.</p>    Engine Query Structure Performance (seconds)     DuckDB Subquery 0.02   DuckDB CTE 0.02   DuckDB + cached Postgres Subquery 0.03   DuckDB + cached Postgres CTE 0.07   Postgres CTE 0.49   DuckDB + scanned Postgres Subquery 0.50   DuckDB + scanned Postgres CTE 0.50   Postgres Subquery ~6300","tags":["sql","data","beginner"]},{"location":"blog/cte-subquery-who-gives-a-duck/#why-it-matters","title":"Why it matters","text":"<p>This is an example of a query with a subquery:</p> SQL<pre><code>select students.name, \n    exams.course\nfrom students,\n    exams\nwhere students.id = exams.sid\n    and exams.grade = (\n        select min(e2.grade)\n        from exams e2\n        where students.id = e2.sid\n    );\n</code></pre> <p>It selects, for each student, the exam with the lowest grade they received out of all their own exams.</p> <p>If you have experience with SQL joins then you might recognize the <code>students.id = exams.sid</code> clause, which joins exams to students.</p> <p>The subquery utilizes each student's id to find their own minimum grade of their exams.</p> <p>The performance of this query in postgres is atrocious.</p> <p>On the other hand, the performance of this completely equivalent query using a Common Table Expression (CTE) is very passable in postgres:</p> SQL<pre><code>with \n    min_exam_grades as (\n        select e2.sid as id,\n            min(e2.grade) as best\n        from exams e2\n        group by e2.sid\n    )\nselect students.name,\n    exams.course\nfrom students,\n    exams,\n    min_exam_grades\nwhere students.id = exams.sid\n    and min_exam_grades.id = students.id\n    and exams.grade = min_exam_grades.best;\n</code></pre> <p>This is the CTE vs Subquery debate in a nutshell.</p> <p>You have to keep in mind that subqueries can produce poor query performance and you cannot write all subquery expressions as CTEs!</p> <p>In comes the OLAP-centric duckdb and de-correlated queries so we can have our cake and eat it too.</p>","tags":["sql","data","beginner"]},{"location":"blog/cte-subquery-who-gives-a-duck/#not-giving-a-flock","title":"Not giving a flock","text":"<p>We'll run these queries over a duckdb database and compare their performance. (HINT: they're the same in duckdb)</p> <p>You can follow along with this experience in your browser; visit https://shell.duckdb.org to open a duckdb shell.</p> <p>Then we'll show that you can bring these benefits over to a live postgres instance with the duckdb postgres scanner extension!</p>","tags":["sql","data","beginner"]},{"location":"blog/cte-subquery-who-gives-a-duck/#tables","title":"Tables","text":"<p>We'll generate some similar tables to the examples in the paper. Namely, a students and an exams table.</p> <p>Copy and paste the following into a duckdb shell.</p> <p>Follow along with the sql comments (<code>-- these lines with two dashes are comments</code>)</p> SQL<pre><code>-- cleanup existing tables and sequences if necessary\ndrop table if exists exams;\ndrop table if exists students;\ndrop sequence if exists seq_studentid;\ndrop sequence if exists seq_examid;\n\n-- generate tables for students and exams\ncreate table students (\n    id bigint, \n    name text\n);\ncreate table exams (\n    id bigint,\n    sid bigint,\n    grade int,\n    course text\n);\n\n-- create sequences to create ids for exams and students\ncreate sequence seq_studentid start 1;\ncreate sequence seq_examid start 1;\n\n-- generate 100000 students with random text names\ninsert into students (id, name)\nselect nextval('seq_studentid'), md5(random()::text)\nfrom generate_series(1, 100000);\n\n-- generate 10 exams per student with random grades between 0 and 100 and random text course names\ninsert into exams (id, sid, grade, course)\nselect nextval('seq_examid'),\n    students.id,\n    floor(random() * (101))::int,\n    md5(random()::text)\nfrom generate_series(1, 10),\n    students;\n</code></pre> <p>You should see something like the following in your shell:</p> Text Only<pre><code>| Count |\n---------\n| 1000000 |\n</code></pre>","tags":["sql","data","beginner"]},{"location":"blog/cte-subquery-who-gives-a-duck/#subquery","title":"Subquery","text":"<p>The subquery is the first version of the query I personally would write.</p> <p>Let's evaluate its performance:</p> SQL<pre><code>explain analyze\nselect s.name,\n    e.course\nfrom students s,\n    exams e\nwhere s.id = e.sid\n    and e.grade =(\n        select min(e2.grade)\n        from exams e2\n        where s.id = e2.sid\n    );\n</code></pre> <p>This takes about <code>0.02</code> seconds on my machine on the first run (about <code>0.09</code> in the duckdb browser shell).</p>","tags":["sql","data","beginner"]},{"location":"blog/cte-subquery-who-gives-a-duck/#cte","title":"CTE","text":"<p>Now for the CTE:</p> SQL<pre><code>explain analyze\nselect students.name,\n    exams.course\nfrom students,\n    exams,\n    (\n        select e2.sid as id,\n            min(e2.grade) as best\n        from exams e2\n        group by e2.sid\n    ) min_exam_grades\nwhere students.id = exams.sid\n    and min_exam_grades.id = students.id\n    and exams.grade = min_exam_grades.best;\n</code></pre> <p>This also takes about <code>0.02</code> seconds on my machine (about <code>0.09</code> in the duckdb browser shell).</p>","tags":["sql","data","beginner"]},{"location":"blog/cte-subquery-who-gives-a-duck/#so-what","title":"So What","text":"<p>Let's run the same experiment in postgres via a docker compose file:</p> YAML<pre><code>services:\n  database:\n    image: postgres:15.1\n    command: [\"postgres\", \"-c\", \"log_statement=all\", \"-c\", \"log_destination=stderr\"]\n    volumes:\n      - ./pg:/tmp/pg\n      - postgres_data:/var/lib/postgresql/data/pgdata\n    environment:\n      PGDATA: /var/lib/postgresql/data/pgdata/\n      POSTGRES_HOST: database\n      POSTGRES_PORT: 5432\n      POSTGRES_DB: demo\n      POSTGRES_USER: demo_user\n      POSTGRES_PASSWORD: demo_password\n    ports:\n      - \"5432:5432\"\n    restart: always\n\nvolumes:\n  postgres_data:\n</code></pre> <p>With <code>docker-compose up --build</code> and this <code>docker-compose.yml</code> we should get a live postgres container.</p>","tags":["sql","data","beginner"]},{"location":"blog/cte-subquery-who-gives-a-duck/#init-postgres-tables","title":"Init Postgres Tables","text":"<p>The following should get us a <code>psql</code> shell in the postgres instance:</p> Bash<pre><code>docker-compose exec database psql --username demo_user --dbname demo\n</code></pre> <p>(NOTE you will probably need to open another terminal)</p> <p>And then we can create the same demo tables:</p> SQL<pre><code>-- cleanup if necessary\ndrop table if exists exams;\ndrop table if exists students;\n\n-- generate tables\ncreate table students (\n    id bigserial, \n    name text\n);\ncreate table exams (\n    id bigserial,\n    sid bigint,\n    grade int,\n    course text\n);\n\n-- generate 100000 students\ninsert into students (name)\nselect md5(random()::text)\nfrom generate_series(1, 100000);\n\n-- generate 10 exams per student\ninsert into exams (sid, grade, course)\nselect students.id,\n    floor(random() * (101))::int,\n    md5(random()::text)\nfrom generate_series(1, 10),\n    students;\n</code></pre>","tags":["sql","data","beginner"]},{"location":"blog/cte-subquery-who-gives-a-duck/#pg-cte","title":"PG CTE","text":"<p>For Postgres we'll start with the CTE version of the query:</p> SQL<pre><code>explain analyze\nselect students.name,\n    exams.course\nfrom students,\n    exams,\n    (\n        select e2.sid as id,\n            min(e2.grade) as best\n        from exams e2\n        group by e2.sid\n    ) min_exam_grades\nwhere students.id = exams.sid\n    and min_exam_grades.id = students.id\n    and exams.grade = min_exam_grades.best;\n</code></pre> <p>Execution time <code>493 ms</code> or <code>0.493</code> seconds.</p> <p>Ok, not atrocious, but slower than duckdb running in WASM in the browser.</p> <p>So if our subquery query can be re-written as a CTE query then we're mostly safe.</p>","tags":["sql","data","beginner"]},{"location":"blog/cte-subquery-who-gives-a-duck/#pg-subquery","title":"PG Subquery","text":"<p>If you're brave, go ahead and try the following.</p> <p>In the same <code>psql</code> shell let's evaluate the subquery:</p> SQL<pre><code>explain analyze\nselect s.name,\n    e.course\nfrom students s,\n    exams e\nwhere s.id = e.sid\n    and e.grade =(\n        select min(e2.grade)\n        from exams e2\n        where s.id = e2.sid\n);\n</code></pre> <p>WARNING this took about an hour and 45 minutes on my machine</p>","tags":["sql","data","beginner"]},{"location":"blog/cte-subquery-who-gives-a-duck/#blending-it-together","title":"Blending It Together","text":"<p>But we can do even better!</p> <p>By combining the power of duckdb and postgres we can run these queries efficiently against a live postgres instance without thinking about the structure of our query!</p>","tags":["sql","data","beginner"]},{"location":"blog/cte-subquery-who-gives-a-duck/#duckdb-postgres-scan","title":"Duckdb Postgres Scan","text":"<p>We can scan live data with the benefits of the de-correlated algorithm with the original queries.</p> <p>We'll run duckdb from the CLI with <code>duckdb</code> and connect to the dockerized postgres instance.</p> Bash<pre><code>duckdb\n</code></pre> <p>And in the duckdb shell we can attach to the postgres instance:</p> SQL<pre><code>INSTALL postgres_scanner;\nLOAD postgres_scanner;\nCALL postgres_attach('dbname=demo user=demo_user password=demo_password host=localhost');\nPRAGMA show_tables;\n</code></pre>","tags":["sql","data","beginner"]},{"location":"blog/cte-subquery-who-gives-a-duck/#live-scan","title":"Live Scan","text":"<p>Using <code>.read</code> will execute the sql query in a given file. This might be more convenient for you than copy and pasting the whole query.</p> SQL<pre><code>-- explain analyze subquery version\n.read pg/subquery.sql\n\n-- explain analyze cte version\n.read pg/cte.sql\n</code></pre> <p>About <code>0.5</code> seconds for each of these queries.</p> <p>(NOTE you could take this a step further by isolating the postgres instance on another server. In this experiment they are literally running on the same machine sharing resources)</p> <p>Both had equivalent performance to the regular Postgres CTE version. Both had orders of magnitude better performance than the Postgres Subquery version.</p> <p>So write whatever query makes sense to you and your team / organization!</p>","tags":["sql","data","beginner"]},{"location":"blog/cte-subquery-who-gives-a-duck/#cached-scan","title":"Cached Scan","text":"<p>If data duplication is not an issue, then we can load the postgres table data into duckdb itself and get even better performance!</p> SQL<pre><code>-- copy data into duckdb tables. Only has to happen once\n.read pg/init_db_scan_cache.sql\n\n-- explain analyze cached subquery version\n.read pg/subquery_cached.sql\n\n-- explain analyze cached cte version\n.read pg/cte_cached.sql\n</code></pre> <p>Around <code>0.07</code> seconds. An order of magnitude better than the \"efficient\" CTE version on Postgres.</p> <p>Interestingly, the cached subquery duckdb version has the best performance around <code>0.03</code> seconds.</p> <p>(NOTE these queries only differ in the tables they reference: the cached tables. Start duckdb with a command like <code>duckdb students.db</code> to save the database file and not have to copy tables again)</p>","tags":["sql","data","beginner"]},{"location":"blog/cte-subquery-who-gives-a-duck/#duckdb-docker","title":"Duckdb Docker","text":"<p>This mimics a real production scenario where your duckdb instance is querying a live postgres instance either on RDS or some other host.</p> <p>Perhaps duckdb is running in a lambda or other type of workflow job.</p> <p>The Dockerfile chooses a basic OS and downloads necessary packages:</p> Docker<pre><code>FROM debian:bullseye-slim\n\nRUN apt-get update &amp;&amp; apt-get install -y \\\n    wget unzip gettext-base \\\n    &amp;&amp; rm -rf /var/lib/apt/lists/*\n\nRUN wget https://github.com/duckdb/duckdb/releases/download/v0.8.0/duckdb_cli-linux-amd64.zip \\\n    &amp;&amp; unzip duckdb_cli-linux-amd64.zip -d /usr/local/bin \\\n    &amp;&amp; rm duckdb_cli-linux-amd64.zip\n\n# Run as non-root user\nRUN useradd --create-home appuser\nWORKDIR /home/appuser\nUSER appuser\n\n# Duckdb startup helpers\nCOPY .duckdbrc /tmp/.duckdbrc\nCOPY entrypoint.sh ./entrypoint.sh\n\nENTRYPOINT [ \"/bin/bash\" ]\nCMD [ \"entrypoint.sh\", \"duckdb\" ]\n</code></pre>","tags":["sql","data","beginner"]},{"location":"blog/cte-subquery-who-gives-a-duck/#duckdbrc","title":".duckdbrc","text":"<p>The <code>.duckdbrc</code> file defines the initialization of duckdb shells for this image. It will instruct each duckdb shell you open to connect to the live postgres docker container:</p> Bash<pre><code>.prompt '\u26ab\u25d7 '\nINSTALL postgres_scanner;\nLOAD postgres_scanner;\nCALL postgres_attach('dbname=$POSTGRES_DB user=$POSTGRES_USER password=$POSTGRES_PASSWORD host=$POSTGRES_HOST');\nPRAGMA show_tables;\n</code></pre>","tags":["sql","data","beginner"]},{"location":"blog/cte-subquery-who-gives-a-duck/#entrypoint","title":"Entrypoint","text":"<p>But to set those postgres connection variables at runtime we'll need a small entrypoint bash script that will allow other commands to be entered after:</p> Bash<pre><code>#!/bin/sh\nenvsubst &lt; /tmp/.duckdbrc &gt; /home/appuser/.duckdbrc\nexec \"$@\"\n</code></pre>","tags":["sql","data","beginner"]},{"location":"blog/cte-subquery-who-gives-a-duck/#docker-compose","title":"Docker Compose","text":"<p>Let's add this other container / service to our docker-compose file:</p> YAML<pre><code>  duck-database:\n    build: duck\n    volumes:\n      - ./pg:/home/appuser/pg\n      - ./duck:/home/appuser/duck\n    environment:\n      POSTGRES_HOST: database\n      POSTGRES_PORT: 5432\n      POSTGRES_DB: demo\n      POSTGRES_USER: demo_user\n      POSTGRES_PASSWORD: demo_password\n    # Probably only necessary for M1 Mac\n    platform: linux/x86_64\n</code></pre> <p>The following should get us a duckdb shell that can connect to the live postgres instance:</p> Bash<pre><code>docker-compose run --build duck-database entrypoint.sh duckdb duck/students.db\n</code></pre> <p>Or we can run a query directly from the cli:</p> Bash<pre><code>docker-compose run --build duck-database entrypoint.sh duckdb -c \".read pg/subquery.sql\"\n</code></pre> <p>This method might cause a performance hit on your machine, but can be tested more rigorously before a cloud pipeline deployment.</p>","tags":["sql","data","beginner"]},{"location":"blog/cte-subquery-who-gives-a-duck/#story-time","title":"Story Time","text":"<p>If you've made it this far then you probably have some interest in the topic.</p> <p>Here's an example of how I learned about subquery performance in Postgres in the real world:</p> <ul> <li>Imagine a table full of account records for your customers</li> <li>Imagine this table has a column for the date the data was updated</li> <li>Imagine you want to select the most up to date information for a large set of accounts</li> </ul> <p>Maybe you would write a query like this:</p> SQL<pre><code>select accounts.id, accounts.balance\nfrom accounts\nwhere \n    accounts.balance &gt; 1\n    and accounts.updated_date = (\n        select max(a2.updated_date)\n        from accounts a2\n        where accounts.id = a2.id\n    );\n</code></pre> <p>And thus you fell into the trap. This query could take god-knows-how-long and that is not at all useful for developer experience.</p> <p>Would you rather spend the mental cycles re-writing it to a CTE or move on with your life?</p>","tags":["sql","data","beginner"]},{"location":"blog/cte-subquery-who-gives-a-duck/#resources","title":"Resources","text":"<p>Keep exploring!</p> <ul> <li>More on this example from @FranckPachot: dbfiddle.uk/wLZ4H496</li> <li>https://duckdb.org/2023/05/26/correlated-subqueries-in-sql.html</li> <li>https://duckdb.org/2022/09/30/postgres-scanner.html</li> <li>https://hakibenita.com/be-careful-with-cte-in-postgre-sql</li> <li>https://cs.emis.de/LNI/Proceedings/Proceedings241/383.pdf</li> </ul>","tags":["sql","data","beginner"]},{"location":"blog/gitlab-workflow/","title":"Gitlab Workflow Project Management","text":"<p>One goal of using Gitlab is to not clobber other contributers' work, even when working on different features and recombining new code. Using it to manage a personal project will help you keep a history of your code, plan out what you want to add, and share your work so others can contribute or work on their own version.</p> <p>For a fuller perspective on why we try to <code>branch</code> into development code to add features then <code>merge</code> that code back into our previous work, here is a post from gitlab with some nice diagrams.</p> <p>It's not important to understand every detail, but it highlights some best practices like using informative names and comments.</p> <p>For motivation on why a system like git makes sense to use, here's a brief, not-so-technical post 'the git parable' by Tom Preston-Werner</p>","tags":["beginner","general-coding","git","guide"]},{"location":"blog/gitlab-workflow/#action-item-clone-the-site","title":"Action Item: Clone the Site","text":"<p>Your first task, which will hopefully be a useful example to reference in the future, is to change up this website by adding your information to the main page.</p> <p>NOTE: You only have to do this once per project / repository.</p> <ul> <li>Navigate to the project website:</li> <li>Open a new Terminal window (or Git Bash / Powershell on Windows) and enter the following commands one by one (you can replace <code>web-resources</code> with whatever folder name and <code>dev-YOURNAMEHERE</code> with something like <code>dev-gb</code>, just make sure to use the same every time, <code>TYPSO ARE BAD</code>)</li> </ul> Bash<pre><code>git clone git@TODO:add LINK web-resources\n</code></pre> <p>Now you should have a folder with all the code for the site called <code>web-resources,</code> probably in your Home directory ('~/web-resources'). If you want to understand what's happening with these folders, check out my intro to the Terminal post</p>","tags":["beginner","general-coding","git","guide"]},{"location":"blog/gitlab-workflow/#start-a-feature-development-branch","title":"Start a feature / development branch","text":"<p>Make sure you Change Directories in the Terminal to your <code>/web-resources/</code> folder, then start a branch to work on (you can name it anything, easy to remember and spell is best)</p> Bash<pre><code>cd web-resources\ngit checkout -b dev-YOURBRANCHNAME\n</code></pre> <p>NOTE: If you've already started a branch and are returning to working on it, you might not need this step, but may want to update your branch with code that others have added to master.</p>","tags":["beginner","general-coding","git","guide"]},{"location":"blog/gitlab-workflow/#update-feature-branch-to-master","title":"Update feature branch to master","text":"<p>When returning to working on a project it's usually a good idea to use <code>git status</code> to find what branch you're on, <code>git fetch</code> to download any files that are new on the server, and <code>git pull origin BRANCHNAME</code> to update your local files if needed.</p> <p>To make future merges easier and access others' code contributions, it is good to occasionally fetch changes from master and merge them into our feature / dev branch. I'll copy commands from this gist, see the link for more explanation.  This is similar to 'rebasing' </p> Text Only<pre><code>git checkout master\ngit fetch -p origin\ngit merge origin/master\ngit checkout YOUR-FEATURE-BRANCH-NAME\ngit merge master\n</code></pre> <p>This may get you into Vim to complete the merge, see Common Issues below (or <code>esq</code> - <code>:wq</code> - <code>enter</code>)</p> <p>Once the merge is done you can <code>git checkout YOUR-FEATURE-BRANCH-NAME</code>, work on it, add, commit, and finally <code>git push origin YOUR-FEATURE-BRANCH-NAME</code></p>","tags":["beginner","general-coding","git","guide"]},{"location":"blog/gitlab-workflow/#record-what-youre-doing","title":"Record what you're doing","text":"<p>When working on a new feature, we want to tell everyone that we're working on it in Issues</p> <ul> <li> To tell the rest of the team what Issue / Task you're working on, go to the Issues -&gt; Boards tab</li> <li> Click the <code>+</code> Plus button on the top right of <code>To Do</code> or <code>Doing</code> and add a comment about what you're improving like \"Adding my name to the website\" (try to be specific)</li> <li> Everyone can see that you are / were working on that task and that it needs to get done</li> <li> BONUS: If you want to get real fancy, include the number of your issue in the branch name (<code>#5</code>, <code>#1</code>, <code>#47</code>, etc...) so that we can remember it and automatically set it to Done later</li> </ul>","tags":["beginner","general-coding","git","guide"]},{"location":"blog/gitlab-workflow/#make-some-edits","title":"Make some edits","text":"<ul> <li> Add a headshot image to the folder <code>images/headshots/</code>. For this example the filename will be <code>my_headshot.jpg</code></li> <li> Open up your Code Editor and open the file <code>web-resources/public/index.html</code></li> <li> Add your name to the main page</li> <li> Save the file</li> </ul>","tags":["beginner","general-coding","git","guide"]},{"location":"blog/gitlab-workflow/#send-your-changes-to-the-server","title":"Send your changes to the server","text":"<p>Now we want to make your changes go live.</p> <p>To do that, commit your code changes to the <code>dev-YOURBRANCHNAME</code> branch, push it to the online server to be saved, merge it into the master branch, and finally update the live website with the master branch</p> <p>If anything goes wrong in this process or to understand it better, see the references below</p>","tags":["beginner","general-coding","git","guide"]},{"location":"blog/gitlab-workflow/#bonus","title":"BONUS:","text":"<p>If you remembered your issue number above (let it be 47 for now), add <code>fixes #47</code> or <code>closes #47</code> to your git commit message below (in the double quotes) and it automatically goes from Doing / To Do to Done!</p> Bash<pre><code>git status\ngit add .\ngit commit -m \"Added my name to the site, closes #47\"\ngit push origin dev-YOURBRANCHNAME\n</code></pre>","tags":["beginner","general-coding","git","guide"]},{"location":"blog/gitlab-workflow/#merge-your-changes-with-the-live-master-branch","title":"Merge your changes with the live / master branch","text":"<p>Now you're changes are saved online, but on your development branch. When we merge it into the master branch there may be conflicts / overwrites that need to be resolved.</p> <p>NOTE: This is the traditional (command line) way to merge, using git commands in the Terminal. If any conflicts or errors arise, scroll down to check common issues</p> Text Only<pre><code>git checkout master\ngit merge dev-YOURBRANCHNAME\ngit push origin master\n</code></pre>","tags":["beginner","general-coding","git","guide"]},{"location":"blog/gitlab-workflow/#alternative-merge","title":"Alternative Merge","text":"<ul> <li>Using the gitlab site to might help you get fewer: <code>error: failed to push some refs to ......</code> hangups</li> <li>Instead of the above commands, after you push your dev-YOURBRANCHNAME, navigate to the project page</li> <li>You should see a banner near the top like the following with the branch that you just pushed to (in this example <code>add-styling</code> branch), click the <code>Create Merge Request</code>: [TODO IMAGE]</li> <li>On the next screen you should assign the task of merging to yourself and add comments if you wish. Near the bottom you can observe the files and changes affected by the merge. Continue creating the merge request.</li> <li>Once all conflicts are sorted out (if any), a testing pipeline will run (if in place), and you'll be able to press the green <code>Merge</code> button as below. (You don't need to check the <code>Delete Source branch</code> but you can always make a new branch): [TODO IMAGE]</li> </ul>","tags":["beginner","general-coding","git","guide"]},{"location":"blog/gitlab-workflow/#confirm-your-work-on-gitlab","title":"Confirm your work on gitlab","text":"<p>Now a <code>pipeline</code> will be triggered to test and deploy your changes, which you can see in the CI/CD -&gt; Pipelines tab</p> <p>You can click through to the <code>pages</code> job (either a blue semi circle or green check or red x) to see what the server is actually doing to execute your changes.</p> <p>Hopefully that won't take too long and the Blue semi circle becomes a Green check (and not a Red x).</p> <p>Now you can go back to the Issues -&gt; Boards tab and drag your TO Do / Doing item to the Done Bin!</p> <p>Check out the (main page)[TODO MAIN LINKE] to confirm your work.</p>","tags":["beginner","general-coding","git","guide"]},{"location":"blog/gitlab-workflow/#common-git-issues","title":"Common Git Issues","text":"<p>See Resources/Git</p>","tags":["beginner","general-coding","git","guide"]},{"location":"blog/holy-duck/","title":"Holy \ud83e\udd86uck! Fast Analysis with DuckDB + Pyarrow","text":"<p>Turning to DuckDB when you need to crunch more numbers faster than pandas in your Streamlit app \ud83c\udf88</p> <p>Inspired by \"DuckDB quacks Arrow\" blogpost cross-posted on duckdb and arrow</p>","tags":["python","data","intermediate"]},{"location":"blog/holy-duck/#background","title":"Background","text":"<p><code>streamlit</code> and Streamlit Cloud are fantastic for sharing your data exploration apps. A very common pattern uses csv files with <code>pandas</code> to accomplish the necessary steps of:</p> <ul> <li>Load the data into the program</li> <li>Filter data by certain columns or attributes</li> <li>Compute analyses on the data (averages, counts, etc.)</li> </ul>","tags":["python","data","intermediate"]},{"location":"blog/holy-duck/#nyc-uber-data","title":"NYC Uber Data","text":"<ul> <li>Streamlit DuckDB Uber NYC repo</li> <li>Includes 10 Year, 1.5 Billion row Taxi data example as well</li> <li>Streamlit Original Uber NYC repo</li> </ul> <p>Let's take this NYC Uber dataset example from Streamlit. We'll pay attention to:</p> <ul> <li>How much RAM / memory is used</li> <li>How long it takes to perform each step</li> </ul> Python<pre><code>import pandas as pd\nimport numpy as np\n# import streamlit as st\n\n# singleton ignored because we're not in streamlit anymore\n# @st.experimental_singleton\ndef load_data():\n    data = pd.read_csv(\n        \"uber-raw-data-sep14.csv.gz\",\n        nrows=100000,  # approx. 10% of data\n        names=[\n            \"date/time\",\n            \"lat\",\n            \"lon\",\n        ],  # specify names directly since they don't change\n        skiprows=1,  # don't read header since names specified directly\n        usecols=[0, 1, 2],  # doesn't load last column, constant value \"B02512\"\n        parse_dates=[\n            \"date/time\"\n        ],  # set as datetime instead of converting after the fact\n    )\n\n    return data\n</code></pre> Python<pre><code>%%time\ndata = load_data()\n</code></pre> Text Only<pre><code>CPU times: user 2.99 s, sys: 48.1 ms, total: 3.04 s\nWall time: 2.94 s\n</code></pre> Python<pre><code>data.info()\n</code></pre> Text Only<pre><code>&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 100000 entries, 0 to 99999\nData columns (total 3 columns):\n #   Column     Non-Null Count   Dtype         \n---  ------     --------------   -----         \n 0   date/time  100000 non-null  datetime64[ns]\n 1   lat        100000 non-null  float64       \n 2   lon        100000 non-null  float64       \ndtypes: datetime64[ns](1), float64(2)\nmemory usage: 2.3 MB\n</code></pre> <p>Feel free to reference the <code>read_csv</code> documentation, the focus of this post is on the <code>nrows=100000</code> argument though.</p> <p>This <code>nrows</code> is used to limit the number of rows that get loaded into our application. Taking in <code>100,000</code> rows landed us around <code>2.3 MB</code> of memory allocation for the data.</p> <p>It loaded on my computer in <code>~3</code> seconds.</p> <p>Let's see how that would go without our <code>nrows</code> limitation</p> Python<pre><code>def load_full_data():\n    data = pd.read_csv(\n        \"uber-raw-data-sep14.csv.gz\",\n        # nrows=100000,  # approx. 10% of data\n        names=[\n            \"date/time\",\n            \"lat\",\n            \"lon\",\n        ],  # specify names directly since they don't change\n        skiprows=1,  # don't read header since names specified directly\n        usecols=[0, 1, 2],  # doesn't load last column, constant value \"B02512\"\n        parse_dates=[\n            \"date/time\"\n        ],  # set as datetime instead of converting after the fact\n    )\n\n    return data\n</code></pre> Python<pre><code>%%time\nfull_data = load_full_data()\n</code></pre> Text Only<pre><code>CPU times: user 29.8 s, sys: 163 ms, total: 30 s\nWall time: 30 s\n</code></pre> Python<pre><code>full_data.info()\n</code></pre> Text Only<pre><code>&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 1028136 entries, 0 to 1028135\nData columns (total 3 columns):\n #   Column     Non-Null Count    Dtype         \n---  ------     --------------    -----         \n 0   date/time  1028136 non-null  datetime64[ns]\n 1   lat        1028136 non-null  float64       \n 2   lon        1028136 non-null  float64       \ndtypes: datetime64[ns](1), float64(2)\nmemory usage: 23.5 MB\n</code></pre> <p>Ok, so with <code>~10</code> times as much data (<code>1,028,136</code> vs <code>100,000</code>) we use:</p> <ul> <li><code>~10</code> times as much memory (<code>23.5 MB</code> vs <code>2.3 MB</code>)</li> <li><code>~10</code> times as much time (<code>30 s</code> vs <code>2.94 s</code>)</li> </ul> <p>The first time this app loads in <code>streamlit</code> will be a bit slow either way, but the <code>singleton</code> decorator is designed to prevent having to re-compute objects like this.</p> <p>(Also note that this is a single month of data... a year might include <code>~12,337,632</code> entries based on this september 2014 data)</p>","tags":["python","data","intermediate"]},{"location":"blog/holy-duck/#enter-the-duck","title":"Enter the Duck","text":"<p>Using <code>pyarrow</code> and <code>duckdb</code> let's see if we get any improvement</p> Python<pre><code>import duckdb\nimport pyarrow as pa\nfrom pyarrow import csv\nimport pyarrow.dataset as ds\n\ndef load_data_duckdb():\n    data = csv.read_csv('uber-raw-data-sep14.csv.gz', convert_options=csv.ConvertOptions(\n        include_columns=[\"Date/Time\",\"Lat\",\"Lon\"],\n        timestamp_parsers=['%m/%d/%Y %H:%M:%S']\n    )).rename_columns(['date/time', 'lat', 'lon'])\n\n    # `dataset` is for partitioning larger datasets. Can't include timestamp parsing directly though\n    # data = ds.dataset(\"uber-raw-data-sep14.csv.gz\", schema=pa.schema([\n    #     (\"Date/Time\", pa.timestamp('s')),\n    #     ('Lat', pa.float32()),\n    #     ('Lon', pa.float32())\n    # ]), format='csv')\n\n    # DuckDB can query Arrow tables, so we'll just return the table and a connection for flexible querying\n    return data, duckdb.connect(\":memory:\")\narrow_data, con = load_data_duckdb()\narrow_data[:5]\n</code></pre> Text Only<pre><code>pyarrow.Table\ndate/time: timestamp[s]\nlat: double\nlon: double\n----\ndate/time: [[2014-09-01 00:01:00,2014-09-01 00:01:00,2014-09-01 00:03:00,2014-09-01 00:06:00,2014-09-01 00:11:00]]\nlat: [[40.2201,40.75,40.7559,40.745,40.8145]]\nlon: [[-74.0021,-74.0027,-73.9864,-73.9889,-73.9444]]\n</code></pre> Python<pre><code>%%timeit\nload_data_duckdb()\n</code></pre> Text Only<pre><code>153 ms \u00b1 121 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 10 loops each)\n</code></pre> <p>Holy Smokes! Well that was fast and fun!</p> <p><code>pyarrow</code> read the whole dataset in <code>153 ms</code>. That's <code>0.153 s</code> compared to <code>30 s</code> with <code>pandas</code>!</p> <p>So how much memory are <code>pyarrow</code> and <code>duckdb</code> using?</p> Python<pre><code>def format_bytes(size):\n    \"\"\"from https://stackoverflow.com/a/49361727/15685218\"\"\"\n    # 2**10 = 1024\n    power = 2**10\n    n = 0\n    power_labels = {0 : '', 1: 'kilo', 2: 'mega', 3: 'giga', 4: 'tera'}\n    while size &gt; power:\n        size /= power\n        n += 1\n    return size, power_labels[n]+'bytes'\n</code></pre> Python<pre><code>format_bytes(arrow_data.nbytes)\n</code></pre> Text Only<pre><code>(23.53216552734375, 'megabytes')\n</code></pre> <p>Ok, the <code>pyarrow</code> table has roughly the same size as the full <code>pandas</code> Dataframe</p> Python<pre><code>con.execute('PRAGMA database_size;')\n\"\"\"\ndatabase_size VARCHAR, -- total block count times the block size\nblock_size BIGINT,     -- database block size\ntotal_blocks BIGINT,   -- total blocks in the database\nused_blocks BIGINT,    -- used blocks in the database\nfree_blocks BIGINT,    -- free blocks in the database\nwal_size VARCHAR,      -- write ahead log size\nmemory_usage VARCHAR,  -- memory used by the database buffer manager\nmemory_limit VARCHAR   -- maximum memory allowed for the database\n\"\"\"\ndatabase_size, block_size, total_blocks, used_blocks, free_blocks, wal_size, memory_usage, memory_limit = con.fetchall()[0]\nmemory_usage\n</code></pre> Text Only<pre><code>'0 bytes'\n</code></pre> <p>We haven't told <code>duckdb</code> to load anything into its own tables, so it still has no memory usage. Nevertheless, <code>duckdb</code> can query the <code>arrow_data</code> since it's a <code>pyarrow</code> table. (<code>duckdb</code> can also load directly from csv).</p> <p>So where does that leave us on loading the full <code>1,000,000</code> row dataset?</p> <ul> <li><code>pandas</code>: <code>~30 s</code> of time and <code>23.5 MB</code></li> <li><code>pyarrow</code>: <code>~.1 s</code> of time (<code>153 ms</code>) and <code>23.9 MB</code></li> </ul> <p>In fairness, I tried <code>pandas</code> with the <code>pyarrow</code> engine. At the time of writing I can't find a fast datetime parse and <code>usecols</code> throws an error in <code>pyarrow</code> (see end of post). Reading the full CSV without datetime parsing is in line in terms of speed though.</p> <p>(also see why the best CSV is not a CSV at all for more on this path)</p> Python<pre><code>%%time\narrow_df = pd.read_csv(\n    \"uber-raw-data-sep14.csv.gz\",\n    engine='pyarrow',\n    names=[\n        \"date/time\",\n        \"lat\",\n        \"lon\",\n        \"CONST\"\n    ],  # specify names directly since they don't change\n    skiprows=1,  # don't read header since names specified directly\n    # usecols=[1, 2],  # doesn't load last column, constant value \"B02512\"\n    parse_dates=[\n        \"date/time\"\n    ],  # set as datetime instead of converting after the fact\n    # infer_datetime_format=True  # Unsupported for pyarrow\n    date_parser=lambda x: pd.to_datetime(x)\n)\n</code></pre> Text Only<pre><code>CPU times: user 30.2 s, sys: 193 ms, total: 30.4 s\nWall time: 30.2 s\n</code></pre> Python<pre><code>arrow_df.info()\n</code></pre> Text Only<pre><code>&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 1028136 entries, 0 to 1028135\nData columns (total 4 columns):\n #   Column     Non-Null Count    Dtype         \n---  ------     --------------    -----         \n 0   date/time  1028136 non-null  datetime64[ns]\n 1   lat        1028136 non-null  float64       \n 2   lon        1028136 non-null  float64       \n 3   CONST      1028136 non-null  object        \ndtypes: datetime64[ns](1), float64(2), object(1)\nmemory usage: 31.4+ MB\n</code></pre> Python<pre><code>%%timeit\narrow_df_no_datetime = pd.read_csv(\n    \"uber-raw-data-sep14.csv.gz\",\n    engine='pyarrow',\n    names=[\n        \"date/time\",\n        \"lat\",\n        \"lon\",\n        \"CONST\"\n    ],  # specify names directly since they don't change\n    skiprows=1,  # don't read header since names specified directly\n    # usecols=[1, 2],  # doesn't load last column, constant value \"B02512\"\n)\n</code></pre> Text Only<pre><code>139 ms \u00b1 568 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 10 loops each)\n</code></pre>","tags":["python","data","intermediate"]},{"location":"blog/holy-duck/#filtration","title":"Filtration","text":"<p>We have 3 main analysis functions to compare between <code>pandas</code> and <code>duckdb</code> for this app, laid out below:</p> Python<pre><code># FILTER DATA FOR A SPECIFIC HOUR, CACHE\n# @st.experimental_memo\ndef filterdata(df, hour_selected):\n    return df[df[\"date/time\"].dt.hour == hour_selected]\n\n\n# CALCULATE MIDPOINT FOR GIVEN SET OF DATA\n# @st.experimental_memo\ndef mpoint(lat, lon):\n    return (np.average(lat), np.average(lon))\n\n\n# FILTER DATA BY HOUR\n# @st.experimental_memo\ndef histdata(df, hr):\n    filtered = data[\n        (df[\"date/time\"].dt.hour &gt;= hr) &amp; (df[\"date/time\"].dt.hour &lt; (hr + 1))\n    ]\n\n    hist = np.histogram(filtered[\"date/time\"].dt.minute, bins=60, range=(0, 60))[0]\n\n    return pd.DataFrame({\"minute\": range(60), \"pickups\": hist})\n</code></pre> Python<pre><code>%%timeit\n# For fairness, we'll use the full dataframe\nfilterdata(full_data, 14)\n</code></pre> Text Only<pre><code>18 ms \u00b1 65.4 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 100 loops each)\n</code></pre> Python<pre><code>%%timeit\nmpoint(full_data[\"lat\"], full_data[\"lon\"])\n</code></pre> Text Only<pre><code>404 \u00b5s \u00b1 1.65 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 1,000 loops each)\n</code></pre> Python<pre><code>%%timeit\nhistdata(full_data, 14)\n</code></pre> Text Only<pre><code>/var/folders/cp/ktx4zddx7q3bqctqfjykn5700000gn/T/ipykernel_302/2026438809.py:16: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n  filtered = data[\n\n\n39.9 ms \u00b1 111 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 10 loops each)\n</code></pre> <p>How about Duckdb (with conversion back to <code>pandas</code> for fairness)</p> Python<pre><code>def duck_filterdata(con, hour_selected):\n    return con.query(\n        f'SELECT \"date/time\", lat, lon FROM arrow_data WHERE hour(\"date/time\") = {hour_selected}'\n    ).to_df()\n\n\ndef duck_mpoint(con):\n    return con.query(\"SELECT AVG(lat), AVG(lon) FROM arrow_data\").fetchone()\n\n\ndef duck_histdata(con, hr):\n    hist_query = f'SELECT histogram(minute(\"date/time\")) FROM arrow_data WHERE hour(\"date/time\") &gt;= {hr} and hour(\"date/time\") &lt; {hr + 1}'\n    results, *_ = con.query(hist_query).fetchone()\n    return pd.DataFrame(results)\n</code></pre> Python<pre><code>%%timeit\nduck_filterdata(con, 14)\n</code></pre> Text Only<pre><code>6.03 ms \u00b1 10.9 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 100 loops each)\n</code></pre> Python<pre><code>%%timeit\nduck_mpoint(con)\n</code></pre> Text Only<pre><code>1.62 ms \u00b1 16.6 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 1,000 loops each)\n</code></pre> Python<pre><code>%%timeit\nduck_histdata(con, 14)\n</code></pre> Text Only<pre><code>2.86 ms \u00b1 19.5 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 100 loops each)\n</code></pre> <p>We got a modest improvement in <code>filterdata</code> and more than 10x speedup in <code>histdata</code>, but actually lost out to <code>numpy</code> for finding the average of 2 arrays in <code>mpoint</code>!</p> <ul> <li><code>filterdata</code>:</li> <li><code>pandas</code>: 18 ms \u00b1 65.4 \u00b5s</li> <li><code>duckdb</code>: 6.03 ms \u00b1 10.9 \u00b5s</li> <li><code>mpoint</code>:</li> <li><code>numpy</code>: 404 \u00b5s \u00b1 1.65 \u00b5s</li> <li><code>duckdb</code>: 1.62 ms \u00b1 16.6 \u00b5s</li> <li><code>histdata</code>:</li> <li><code>pandas</code> + <code>numpy</code>: 39.9 ms \u00b1 111 \u00b5s</li> <li><code>duckdb</code>: 2.86 ms \u00b1 19.5 \u00b5s</li> </ul> Python<pre><code>18 / 6.03\n</code></pre> Text Only<pre><code>2.9850746268656714\n</code></pre> Python<pre><code>404 / 1620\n</code></pre> Text Only<pre><code>0.24938271604938272\n</code></pre> Python<pre><code>39.9 / 2.86\n</code></pre> Text Only<pre><code>13.951048951048952\n</code></pre>","tags":["python","data","intermediate"]},{"location":"blog/holy-duck/#larger-than-memory-data","title":"Larger Than Memory Data","text":"<p>Where this DuckDB + Arrow combo really shines is analyzing data that can't be handled by Pandas on your machine.</p> <p>In many cases if the data doesn't fit in your computer's memory (RAM) then using Pandas will consume disk (Swap) to try and fit it, which will slow things down.</p> <p>With the 10 Year dataset below the DuckDB authors found Pandas used <code>248 GBs</code> (!!!) of memory to read out <code>~300,000</code> rows from the <code>~1,500,000,000</code>. In this case it just crashes most laptops.</p> <p>So there evolved libraries such as Dask for handling these out-of-core situations through multiprocessing and distributed computing. Pandas has a whole list of related ecosystem projects.</p> <p>To cut through data on a single laptop, DuckDB + Arrow + the Parquet format provide some impressive optimizations to where we don't need those <code>248 GBs</code> on any number of machines.</p> Python<pre><code>def load_from_10_year():\n    nyc = ds.dataset(\"nyc-taxi/\", partitioning=[\"year\", \"month\"])\n    # Get database connection\n    con = duckdb.connect()\n\n    # Run query that selects part of the data\n    query = con.execute(\n        f\"SELECT total_amount, passenger_count,year FROM nyc where total_amount &gt; 100 and year &gt; 2014\"\n    )\n\n    # Create Record Batch Reader from Query Result.\n    # \"fetch_record_batch()\" also accepts an extra parameter related to the desired produced chunk size.\n    record_batch_reader = query.fetch_record_batch()\n\n    # Retrieve all batch chunks\n    all_chunks = []\n    while True:\n        try:\n            # Process a single chunk here\n            # pyarrow.lib.RecordBatch\n            chunk = record_batch_reader.read_next_batch()\n            all_chunks.append(chunk)\n        except StopIteration:\n            break\n    data = pa.Table.from_batches(all_chunks)\n    return data\n\nload_from_10_year()\n</code></pre> Text Only<pre><code>pyarrow.Table\ntotal_amount: float\npassenger_count: int8\nyear: int32\n----\ntotal_amount: [[149.3,132.18,205.05,137.55,106.57,107.3,187.85,145.55,230.3,118.3,...,104.42,110.8,102.04,112.56,209.9,103.41,175.8,144.95,241.92,102.8]]\npassenger_count: [[1,1,1,1,1,2,1,4,2,1,...,5,1,1,8,2,2,1,1,3,2]]\nyear: [[2015,2015,2015,2015,2015,2015,2015,2015,2015,2015,...,2019,2019,2019,2019,2019,2019,2019,2019,2019,2019]]\n</code></pre> Python<pre><code>%%timeit\nload_from_10_year()\n</code></pre> Text Only<pre><code>2.4 s \u00b1 127 ms per loop (mean \u00b1 std. dev. of 7 runs, 1 loop each)\n</code></pre> <p>Less than 3 seconds to sift through 1.5 Billion rows of data...</p> <p>Let's look at how long it takes to iterate over the whole set in chunks (the inefficient solution to most out-of-memory issues)</p> Python<pre><code>%%time\nnyc = ds.dataset(\"nyc-taxi/\", partitioning=[\"year\", \"month\"])\n# Get database connection\ncon = duckdb.connect()\nquery = con.execute(\n    \"SELECT total_amount FROM nyc\"\n)\n\nrecord_batch_reader = query.fetch_record_batch()\n\ntotal_rows = 0\nwhile True:\n    try:\n        chunk = record_batch_reader.read_next_batch()\n        total_rows += len(chunk)\n    except StopIteration:\n        break\ntotal_rows\n</code></pre> Text Only<pre><code>CPU times: user 1min 30s, sys: 1min 54s, total: 3min 25s\nWall time: 3min 53s\n\n\n\n\n\n1547741381\n</code></pre> <p>Iterating is definitely not going to work for user interactive apps. 2 - 3 seconds is bearable for most users (with a loading indicator), but 4 minutes is far too long to be engaging.</p> <p>Pandas currently would need to perform this whole iteration or load the whole dataset to process the query we asked before.</p> <p>Download instructions (from Ursa Labs S3 bucket) and Streamlit demo using the 10 year set is available in the same repo</p>","tags":["python","data","intermediate"]},{"location":"blog/holy-duck/#conclusions","title":"Conclusions","text":"<p>It's no secret that Python is not a fast language, but there are tricks to speed it up. Common advice is to utilize C optimizations via <code>numpy</code> and <code>pandas</code>.</p> <p>Another new contender is utilizing the C++ driven <code>duckdb</code> as an in-process OLAP database manager. It takes some re-writing of Python code into SQL (or utilize the Relational API or another library such as Ibis Project), but can play nicely with <code>pandas</code> and <code>pyarrow</code>.</p> <p>Speaking of Arrow \ud83c\udff9, it seems to be efficient and growing in popularity and adoption. <code>streamlit</code> \ud83c\udf88 utilizes it to simplify objects in protobufs between browser and server. <code>pandas</code> \ud83d\udc3c has further integrations on their roadmap. <code>polars</code> \ud83d\udc3b\u200d\u2744\ufe0f uses it to power their Rust-written DataFrame library.</p> <p>This post explores an example <code>streamlit</code> app that utilizes some <code>pandas</code> and <code>numpy</code> functions such as <code>read_csv</code>, <code>average</code>, and DataFrame slicing.</p> <p>Using <code>pyarrow</code> to load data gives a speedup over the default <code>pandas</code> engine. Using <code>duckdb</code> to generate new views of data also speeds up difficult computations.</p> <p>It also touches on the power of this combination for processing larger than memory datasets efficiently on a single machine.</p> Python<pre><code>pd.read_csv(\n    \"uber-raw-data-sep14.csv.gz\",\n    # nrows=100000,  # approx. 10% of data\n    engine='pyarrow',\n    names=[\n        \"date/time\",\n        \"lat\",\n        \"lon\",\n        # \"CONST\"\n    ],  # specify names directly since they don't change\n    skiprows=1,  # don't read header since names specified directly\n    # usecols=[1, 2],  # doesn't load last column, constant value \"B02512\"\n    parse_dates=[\n        \"date/time\"\n    ],  # set as datetime instead of converting after the fact\n    # # infer_datetime_format=True  # Unsupported for pyarrow\n    date_parser=lambda x: pd.to_datetime(x)\n)\n</code></pre>       0 date/time lat lon     0 9/1/2014 0:01:00 1970-01-01 00:00:00.000000040 -74.0021 B02512   1 9/1/2014 0:01:00 1970-01-01 00:00:00.000000040 -74.0027 B02512   2 9/1/2014 0:03:00 1970-01-01 00:00:00.000000040 -73.9864 B02512   3 9/1/2014 0:06:00 1970-01-01 00:00:00.000000040 -73.9889 B02512   4 9/1/2014 0:11:00 1970-01-01 00:00:00.000000040 -73.9444 B02512   ... ... ... ... ...   1028131 9/30/2014 22:57:00 1970-01-01 00:00:00.000000040 -73.9845 B02764   1028132 9/30/2014 22:57:00 1970-01-01 00:00:00.000000040 -74.1773 B02764   1028133 9/30/2014 22:58:00 1970-01-01 00:00:00.000000040 -73.9319 B02764   1028134 9/30/2014 22:58:00 1970-01-01 00:00:00.000000040 -74.0066 B02764   1028135 9/30/2014 22:58:00 1970-01-01 00:00:00.000000040 -73.9496 B02764    <p>1028136 rows \u00d7 4 columns</p>  Python<pre><code>pd.read_csv(\n    \"uber-raw-data-sep14.csv.gz\",\n    # nrows=100000,  # approx. 10% of data\n    engine='pyarrow',\n    # names=[\n    #     \"date/time\",\n    #     \"lat\",\n    #     \"lon\",\n    #     \"CONST\"\n    # ],  # specify names directly since they don't change\n    # skiprows=1,  # don't read header since names specified directly\n    usecols=[0,1],  # doesn't load last column, constant value \"B02512\"\n    # parse_dates=[\n    #     \"date/time\"\n    # ],  # set as datetime instead of converting after the fact\n    # # infer_datetime_format=True  # Unsupported for pyarrow\n    # date_parser=lambda x: pd.to_datetime(x)\n).info()\n</code></pre> Text Only<pre><code>---------------------------------------------------------------------------\n\nTypeError                                 Traceback (most recent call last)\n\n/Users/gar/projects/tech-blog/_notebooks/2022-04-26-holy-duck.ipynb Cell 44' in &lt;cell line: 1&gt;()\n----&gt; &lt;a href='vscode-notebook-cell:/Users/gar/projects/tech-blog/_notebooks/2022-04-26-holy-duck.ipynb#ch0000038?line=0'&gt;1&lt;/a&gt; pd.read_csv(\n      &lt;a href='vscode-notebook-cell:/Users/gar/projects/tech-blog/_notebooks/2022-04-26-holy-duck.ipynb#ch0000038?line=1'&gt;2&lt;/a&gt;     \"uber-raw-data-sep14.csv.gz\",\n      &lt;a href='vscode-notebook-cell:/Users/gar/projects/tech-blog/_notebooks/2022-04-26-holy-duck.ipynb#ch0000038?line=2'&gt;3&lt;/a&gt;     # nrows=100000,  # approx. 10% of data\n      &lt;a href='vscode-notebook-cell:/Users/gar/projects/tech-blog/_notebooks/2022-04-26-holy-duck.ipynb#ch0000038?line=3'&gt;4&lt;/a&gt;     engine='pyarrow',\n      &lt;a href='vscode-notebook-cell:/Users/gar/projects/tech-blog/_notebooks/2022-04-26-holy-duck.ipynb#ch0000038?line=4'&gt;5&lt;/a&gt;     # names=[\n      &lt;a href='vscode-notebook-cell:/Users/gar/projects/tech-blog/_notebooks/2022-04-26-holy-duck.ipynb#ch0000038?line=5'&gt;6&lt;/a&gt;     #     \"date/time\",\n      &lt;a href='vscode-notebook-cell:/Users/gar/projects/tech-blog/_notebooks/2022-04-26-holy-duck.ipynb#ch0000038?line=6'&gt;7&lt;/a&gt;     #     \"lat\",\n      &lt;a href='vscode-notebook-cell:/Users/gar/projects/tech-blog/_notebooks/2022-04-26-holy-duck.ipynb#ch0000038?line=7'&gt;8&lt;/a&gt;     #     \"lon\",\n      &lt;a href='vscode-notebook-cell:/Users/gar/projects/tech-blog/_notebooks/2022-04-26-holy-duck.ipynb#ch0000038?line=8'&gt;9&lt;/a&gt;     #     \"CONST\"\n     &lt;a href='vscode-notebook-cell:/Users/gar/projects/tech-blog/_notebooks/2022-04-26-holy-duck.ipynb#ch0000038?line=9'&gt;10&lt;/a&gt;     # ],  # specify names directly since they don't change\n     &lt;a href='vscode-notebook-cell:/Users/gar/projects/tech-blog/_notebooks/2022-04-26-holy-duck.ipynb#ch0000038?line=10'&gt;11&lt;/a&gt;     # skiprows=1,  # don't read header since names specified directly\n     &lt;a href='vscode-notebook-cell:/Users/gar/projects/tech-blog/_notebooks/2022-04-26-holy-duck.ipynb#ch0000038?line=11'&gt;12&lt;/a&gt;     usecols=[0,1],  # doesn't load last column, constant value \"B02512\"\n     &lt;a href='vscode-notebook-cell:/Users/gar/projects/tech-blog/_notebooks/2022-04-26-holy-duck.ipynb#ch0000038?line=12'&gt;13&lt;/a&gt;     # parse_dates=[\n     &lt;a href='vscode-notebook-cell:/Users/gar/projects/tech-blog/_notebooks/2022-04-26-holy-duck.ipynb#ch0000038?line=13'&gt;14&lt;/a&gt;     #     \"date/time\"\n     &lt;a href='vscode-notebook-cell:/Users/gar/projects/tech-blog/_notebooks/2022-04-26-holy-duck.ipynb#ch0000038?line=14'&gt;15&lt;/a&gt;     # ],  # set as datetime instead of converting after the fact\n     &lt;a href='vscode-notebook-cell:/Users/gar/projects/tech-blog/_notebooks/2022-04-26-holy-duck.ipynb#ch0000038?line=15'&gt;16&lt;/a&gt;     # # infer_datetime_format=True  # Unsupported for pyarrow\n     &lt;a href='vscode-notebook-cell:/Users/gar/projects/tech-blog/_notebooks/2022-04-26-holy-duck.ipynb#ch0000038?line=16'&gt;17&lt;/a&gt;     # date_parser=lambda x: pd.to_datetime(x)\n     &lt;a href='vscode-notebook-cell:/Users/gar/projects/tech-blog/_notebooks/2022-04-26-holy-duck.ipynb#ch0000038?line=17'&gt;18&lt;/a&gt; ).info()\n\n\nFile ~/miniconda3/envs/py39/lib/python3.9/site-packages/pandas/util/_decorators.py:311, in deprecate_nonkeyword_arguments.&lt;locals&gt;.decorate.&lt;locals&gt;.wrapper(*args, **kwargs)\n    &lt;a href='file:///Users/gar/miniconda3/envs/py39/lib/python3.9/site-packages/pandas/util/_decorators.py?line=304'&gt;305&lt;/a&gt; if len(args) &gt; num_allow_args:\n    &lt;a href='file:///Users/gar/miniconda3/envs/py39/lib/python3.9/site-packages/pandas/util/_decorators.py?line=305'&gt;306&lt;/a&gt;     warnings.warn(\n    &lt;a href='file:///Users/gar/miniconda3/envs/py39/lib/python3.9/site-packages/pandas/util/_decorators.py?line=306'&gt;307&lt;/a&gt;         msg.format(arguments=arguments),\n    &lt;a href='file:///Users/gar/miniconda3/envs/py39/lib/python3.9/site-packages/pandas/util/_decorators.py?line=307'&gt;308&lt;/a&gt;         FutureWarning,\n    &lt;a href='file:///Users/gar/miniconda3/envs/py39/lib/python3.9/site-packages/pandas/util/_decorators.py?line=308'&gt;309&lt;/a&gt;         stacklevel=stacklevel,\n    &lt;a href='file:///Users/gar/miniconda3/envs/py39/lib/python3.9/site-packages/pandas/util/_decorators.py?line=309'&gt;310&lt;/a&gt;     )\n--&gt; &lt;a href='file:///Users/gar/miniconda3/envs/py39/lib/python3.9/site-packages/pandas/util/_decorators.py?line=310'&gt;311&lt;/a&gt; return func(*args, **kwargs)\n\n\nFile ~/miniconda3/envs/py39/lib/python3.9/site-packages/pandas/io/parsers/readers.py:680, in read_csv(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\n    &lt;a href='file:///Users/gar/miniconda3/envs/py39/lib/python3.9/site-packages/pandas/io/parsers/readers.py?line=664'&gt;665&lt;/a&gt; kwds_defaults = _refine_defaults_read(\n    &lt;a href='file:///Users/gar/miniconda3/envs/py39/lib/python3.9/site-packages/pandas/io/parsers/readers.py?line=665'&gt;666&lt;/a&gt;     dialect,\n    &lt;a href='file:///Users/gar/miniconda3/envs/py39/lib/python3.9/site-packages/pandas/io/parsers/readers.py?line=666'&gt;667&lt;/a&gt;     delimiter,\n   (...)\n    &lt;a href='file:///Users/gar/miniconda3/envs/py39/lib/python3.9/site-packages/pandas/io/parsers/readers.py?line=675'&gt;676&lt;/a&gt;     defaults={\"delimiter\": \",\"},\n    &lt;a href='file:///Users/gar/miniconda3/envs/py39/lib/python3.9/site-packages/pandas/io/parsers/readers.py?line=676'&gt;677&lt;/a&gt; )\n    &lt;a href='file:///Users/gar/miniconda3/envs/py39/lib/python3.9/site-packages/pandas/io/parsers/readers.py?line=677'&gt;678&lt;/a&gt; kwds.update(kwds_defaults)\n--&gt; &lt;a href='file:///Users/gar/miniconda3/envs/py39/lib/python3.9/site-packages/pandas/io/parsers/readers.py?line=679'&gt;680&lt;/a&gt; return _read(filepath_or_buffer, kwds)\n\n\nFile ~/miniconda3/envs/py39/lib/python3.9/site-packages/pandas/io/parsers/readers.py:581, in _read(filepath_or_buffer, kwds)\n    &lt;a href='file:///Users/gar/miniconda3/envs/py39/lib/python3.9/site-packages/pandas/io/parsers/readers.py?line=577'&gt;578&lt;/a&gt;     return parser\n    &lt;a href='file:///Users/gar/miniconda3/envs/py39/lib/python3.9/site-packages/pandas/io/parsers/readers.py?line=579'&gt;580&lt;/a&gt; with parser:\n--&gt; &lt;a href='file:///Users/gar/miniconda3/envs/py39/lib/python3.9/site-packages/pandas/io/parsers/readers.py?line=580'&gt;581&lt;/a&gt;     return parser.read(nrows)\n\n\nFile ~/miniconda3/envs/py39/lib/python3.9/site-packages/pandas/io/parsers/readers.py:1243, in TextFileReader.read(self, nrows)\n   &lt;a href='file:///Users/gar/miniconda3/envs/py39/lib/python3.9/site-packages/pandas/io/parsers/readers.py?line=1240'&gt;1241&lt;/a&gt; if self.engine == \"pyarrow\":\n   &lt;a href='file:///Users/gar/miniconda3/envs/py39/lib/python3.9/site-packages/pandas/io/parsers/readers.py?line=1241'&gt;1242&lt;/a&gt;     try:\n-&gt; &lt;a href='file:///Users/gar/miniconda3/envs/py39/lib/python3.9/site-packages/pandas/io/parsers/readers.py?line=1242'&gt;1243&lt;/a&gt;         df = self._engine.read()\n   &lt;a href='file:///Users/gar/miniconda3/envs/py39/lib/python3.9/site-packages/pandas/io/parsers/readers.py?line=1243'&gt;1244&lt;/a&gt;     except Exception:\n   &lt;a href='file:///Users/gar/miniconda3/envs/py39/lib/python3.9/site-packages/pandas/io/parsers/readers.py?line=1244'&gt;1245&lt;/a&gt;         self.close()\n\n\nFile ~/miniconda3/envs/py39/lib/python3.9/site-packages/pandas/io/parsers/arrow_parser_wrapper.py:153, in ArrowParserWrapper.read(self)\n    &lt;a href='file:///Users/gar/miniconda3/envs/py39/lib/python3.9/site-packages/pandas/io/parsers/arrow_parser_wrapper.py?line=145'&gt;146&lt;/a&gt; pyarrow_csv = import_optional_dependency(\"pyarrow.csv\")\n    &lt;a href='file:///Users/gar/miniconda3/envs/py39/lib/python3.9/site-packages/pandas/io/parsers/arrow_parser_wrapper.py?line=146'&gt;147&lt;/a&gt; self._get_pyarrow_options()\n    &lt;a href='file:///Users/gar/miniconda3/envs/py39/lib/python3.9/site-packages/pandas/io/parsers/arrow_parser_wrapper.py?line=148'&gt;149&lt;/a&gt; table = pyarrow_csv.read_csv(\n    &lt;a href='file:///Users/gar/miniconda3/envs/py39/lib/python3.9/site-packages/pandas/io/parsers/arrow_parser_wrapper.py?line=149'&gt;150&lt;/a&gt;     self.src,\n    &lt;a href='file:///Users/gar/miniconda3/envs/py39/lib/python3.9/site-packages/pandas/io/parsers/arrow_parser_wrapper.py?line=150'&gt;151&lt;/a&gt;     read_options=pyarrow_csv.ReadOptions(**self.read_options),\n    &lt;a href='file:///Users/gar/miniconda3/envs/py39/lib/python3.9/site-packages/pandas/io/parsers/arrow_parser_wrapper.py?line=151'&gt;152&lt;/a&gt;     parse_options=pyarrow_csv.ParseOptions(**self.parse_options),\n--&gt; &lt;a href='file:///Users/gar/miniconda3/envs/py39/lib/python3.9/site-packages/pandas/io/parsers/arrow_parser_wrapper.py?line=152'&gt;153&lt;/a&gt;     convert_options=pyarrow_csv.ConvertOptions(**self.convert_options),\n    &lt;a href='file:///Users/gar/miniconda3/envs/py39/lib/python3.9/site-packages/pandas/io/parsers/arrow_parser_wrapper.py?line=153'&gt;154&lt;/a&gt; )\n    &lt;a href='file:///Users/gar/miniconda3/envs/py39/lib/python3.9/site-packages/pandas/io/parsers/arrow_parser_wrapper.py?line=155'&gt;156&lt;/a&gt; frame = table.to_pandas()\n    &lt;a href='file:///Users/gar/miniconda3/envs/py39/lib/python3.9/site-packages/pandas/io/parsers/arrow_parser_wrapper.py?line=156'&gt;157&lt;/a&gt; return self._finalize_output(frame)\n\n\nFile ~/miniconda3/envs/py39/lib/python3.9/site-packages/pyarrow/_csv.pyx:580, in pyarrow._csv.ConvertOptions.__init__()\n\n\nFile ~/miniconda3/envs/py39/lib/python3.9/site-packages/pyarrow/_csv.pyx:734, in pyarrow._csv.ConvertOptions.include_columns.__set__()\n\n\nFile stringsource:15, in string.from_py.__pyx_convert_string_from_py_std__in_string()\n\n\nTypeError: expected bytes, int found\n</code></pre>","tags":["python","data","intermediate"]},{"location":"blog/how-i-code/","title":"How I Edit Text Files (VS Code Settings)","text":"<p>Editing Text Files. It's the core of programming.</p> <p>I admit there are a lot of GUI and block coding tools in the 21st century.</p> <p>But unless you're going to retire from coding and go to woodworking, you'll probably need to at least open and view some code files or non code files.</p>","tags":["beginner","habits","general-coding"]},{"location":"blog/how-i-code/#why-use-a-code-editor","title":"Why use a Code Editor?","text":"<p>For the same reason some teachers allow cheatsheets: So you can focus on the relevant task. It's not efficient to constantly think about the syntax of your program. It's especially not efficient to constantly run into fixable bugs and errors.</p> <p>A few features in text editors that I now take for granted are:</p> <ul> <li>templating</li> <li>autocompletion &amp; suggestion</li> <li>syntax highlighting</li> <li>syntax correction</li> </ul> <p>In this post I'll focus on how I set up my VS Code to help decrease my cognitive load when coding in Python (And some bonus tips for blogging / READMEs with Markdown). I'll go over relevant extensions for the Python ecosystem, and try to discuss different settings options you may prefer better.</p> <p>NOTE: I personally find VS Code more friendly for new users than older text editors like Emacs and Vim, but those are also incredibly powerful and customizable. It also has significant community extension support. In fact, this post was partially inspired and is partially fueled by \"How I VSCode\" a 100% open-source VS Code extension by Scott W that generates a simple public profile to share the extensions you use. Here's mine: howivscode.com/gerardrbentley.</p>","tags":["beginner","habits","general-coding"]},{"location":"blog/how-i-code/#first-steps","title":"First Steps","text":"","tags":["beginner","habits","general-coding"]},{"location":"blog/how-i-code/#download","title":"Download","text":"<p>A VS Code download for your system should be available here. If you need additional directions on downloading and installing, Microsoft already has an extensive guide on using VS Code that I'll reference a bit.</p>","tags":["beginner","habits","general-coding"]},{"location":"blog/how-i-code/#open-code","title":"Open Code","text":"<p>I recommend using your OS's search (probably <code>cmd + spacebar</code>, <code>windows key</code>, or <code>super key</code>) and typing <code>code</code> to search and if it pops up hit enter to open VS Code with just the keyboard.</p> <p>If you don't want to use the keyboard, go to your Applications folder / Start menu / Launch menu to find VS Code</p>","tags":["beginner","habits","general-coding"]},{"location":"blog/how-i-code/#new-project-folder","title":"New Project Folder","text":"<p>For this demo we'll only be making a few files, but VS Code works best when we have a project folder, so create one now.</p> <p>Use <code>ctrl+k</code> then <code>ctrl+o</code> to Open a folder. Go ahead and create one named <code>/vscodetests/</code> and put it on your Desktop (or use a different name and location, choose your own adventure!)</p> <p>If you don't want to use the keyboard, the <code>File -&gt; Open Folder</code> option from the top bar will do the same action.</p>","tags":["beginner","habits","general-coding"]},{"location":"blog/how-i-code/#new-file","title":"New File","text":"<p>You can use <code>ctrl+n</code> to create a New file, but I usually Right-Click on the name of the folder to put the file in and select <code>New File</code>; name it something like <code>basic_python.py</code>.</p> <p>If you have only one folder you can Right-Click anywhere in the left \"File Explorer\" panel (<code>ctrl+shift+e</code> to get it back in focus if it's gone)</p> <p>Check out the default keyboard shortcuts for mac, windows, linux keyboard shortcuts</p>","tags":["beginner","habits","general-coding"]},{"location":"blog/how-i-code/#first-extensions","title":"First Extensions","text":"<p>After opening a new file (or saving it with <code>.py</code> ending if you used <code>ctrl+n</code> to create one), you'll probably get a popup in the bottom right corner suggesting you install the Recommended Python Extensions. This is from Microsoft and the de-facto Python settings extension in VS Code, so go ahead and install it from there.</p> <p>In general when you open a new filetype you will get a popup like this. The features vary by language / filetype, but there are many linters, formatters, and checkers that benefit from Language Server Protocol (LSP) these days.</p> <p>If you ever want to uninstall the Python extension or install new extensions, use <code>ctrl+shift+x</code> to open the \"EXtensions\" panel on the left.</p> <p>Some good candidates for Python project contributors: - Python Specific     - Python Docstring Generator (Nils Werner)     - Even Better TOML (tamasfe) - General VS Code     - RETIRED, Built In Now Bracket Pair Colorizer 2 (CoenraadS): Visualize nested brackets and parentheses more easily     - indent-rainbow (oderwat): Visualize deeply indented blocks more easily     - GitLens (Eric Amodio): Quickly check git history of files, branches, lines, etc. - Various File Types     - Markdown TOC (AlanWalk)     - markdownlint (David Anson)     - XML (Red Hat)     - SQL Formatter (adpyke)</p> <p>Bonus: For Emacs users, \"Awesome Emacs Keymap\" will get you most of the way to familiar text editing keybindings</p>","tags":["beginner","habits","general-coding"]},{"location":"blog/how-i-code/#vs-code-set-python-env-and-formatter","title":"VS Code Set Python Env and Formatter","text":"<p>Recommended settings in VS Code</p> <ul> <li>Type <code>ctrl+shift+p</code> then type \"settings json\" and select the entry, add these key-value pairs to file<ul> <li>Edit the <code>CHANGEME:YOURNAME</code> with your Windows user. (On Ubuntu based systems use <code>/home/username/miniconda3/bin/python</code>, similar for other Unix systems but with relevant user home directory)</li> </ul> </li> </ul> <p>note: you can instead use <code>ctrl+shift+p</code> and search \"settings ui\" (or use <code>ctrl+,</code>) and then search for each of the keynames below to explore what other settings you might like to try (or different choices for these options!).</p> JSON<pre><code>{\n    \"files.autoSave\": \"onFocusChange\",\n    \"editor.wordWrap\": \"on\",\n    \"editor.wordWrapColumn\": 88,\n    \"jupyter.alwaysTrustNotebooks\": true,\n    \"python.linting.flake8Enabled\": true,\n    \"python.linting.flake8Args\": [\n        \"--max-line-length=88\"\n    ],\n    \"python.analysis.typeCheckingMode\": \"basic\",\n    \"workbench.editorAssociations\": [\n        {\n            \"viewType\": \"jupyter.notebook.ipynb\",\n            \"filenamePattern\": \"*.ipynb\"\n        }\n    ],\n    \"python.linting.pylintEnabled\": false,\n    \"python.formatting.provider\": \"black\",\n    \"python.linting.ignorePatterns\": [\n        \".vscode/*.py\",\n        \"**/site-packages/**/*.py\",\n        \"venv/*.py\"\n    ],\n    \"python.testing.pytestEnabled\": true,\n    \"python.venvPath\": \"${workspaceFolder}\"\n}\n</code></pre>","tags":["beginner","habits","general-coding"]},{"location":"blog/how-i-code/#get-on-to-getting-things-done","title":"Get on to Getting Things Done","text":"<p>That's the bare-bones Python setup.</p> <p>Some of the suggestions below rely on popular Python packages:</p> Text Only<pre><code># formatter\nblack\n# linter + static checker\nflake8\n# interact with jupyter notebooks\nipykernel\n# sort imports in a consistent fashion\nisort\n# let flake8 nag you about object naming\npep8-naming\n</code></pre> <p>You can install them to your current Python environment with <code>pip install black</code> for example.</p>","tags":["beginner","habits","general-coding"]},{"location":"blog/how-i-code/#python-things-to-try","title":"Python Things to Try","text":"<p>Feel free to try out / research the following (Python related):</p> <ul> <li>Typing error-filled python code and see how <code>flake8</code> warns you</li> <li>Start your <code>.py</code> document with <code># %%</code> to make it a VS Code \"Python Interactive\" document (really awesome way to experiment with code blocks and debug small chunks)</li> <li>Use <code>alt+shift+f</code> or <code>ctrl+shift+p</code> and search 'Format Document' to format python according to <code>black</code> standard</li> <li>Text Editing and Searching Guide including stuff like using multiple cursors, search and replace over multiple files, auto save (which is included in my <code>json</code> settings above)</li> <li>Make a new virtual environment for your project with <code>python -m venv venv</code> or look into using <code>conda</code> to manage your environments</li> <li>Look into VS Code's live code sharing and git integrations to collaborate better</li> </ul>","tags":["beginner","habits","general-coding"]},{"location":"blog/how-i-code/#other-languages","title":"Other Languages","text":"<p>Or expand your environment by setting up a few more tools (tangential to Python):</p> <ul> <li>Look into Emmet the auto completer for html files built into VS Code</li> <li>Install Node if you're going to be doing some web development or otherwise need <code>npm</code></li> <li>Install Docker if you want to explore modern container based deployment</li> <li>Find some good Python Reading (Ok one more Python recommendation; some of these are available for low-to-no cost from the authors)</li> </ul>","tags":["beginner","habits","general-coding"]},{"location":"blog/how-i-code/#relevant-links","title":"Relevant Links","text":"<p>PyCharm is tooled specifically for Python and is very popular in the community.</p> <p>JupyterLab and plain Jupyter Notebooks are common development tools for Python users as well.</p> <p>Atom and Sublime are also still rather popular among coders.</p> <p>Emacs and Vim are also still great (well, Emacs lacks a decent editor...)</p>","tags":["beginner","habits","general-coding"]},{"location":"blog/lightgbm-water-pumps/","title":"LightGBM Predicting Water Pump Functionality","text":"<p>Water Pump functionality prediction. Based on the DrivenData challenge Pump it Up: Data Mining the Water Table</p> <p>See the exploration in Streamlit Cloud \ud83c\udf88</p> <p>For better performance, grab the code from github repo</p> <p>Trying out LightGBM for a @drivendataorg competition assessing water pump functionality. Still need to work on the pre-processing before I go posting any scores though \ud83d\ude06\ud83d\udd79 @streamlit Demo: https://t.co/P4XzP1t9vC\ud83d\udc19 Github Repo: https://t.co/nn1Ose9tL4#30DaysOfStreamlit</p>\u2014 @gar@gerardbentley.com (@GarsBar35Plus) April 20, 2022","tags":["streamlit","python","intermediate","drivendata"]},{"location":"blog/lightgbm-water-pumps/#project-walkthrough","title":"Project Walkthrough","text":"<p>Using data from Taarifa and the Tanzanian Ministry of Water, can we predict which pumps are functional, which need some repairs, and which don't work at all?</p> <p>Predict one of these three classes based on a number of variables about what kind of pump is operating, when it was installed, and how it is managed.</p> <p>A smart understanding of which waterpoints will fail can improve maintenance operations and ensure that clean, potable water is available to communities across Tanzania.</p>","tags":["streamlit","python","intermediate","drivendata"]},{"location":"blog/lightgbm-water-pumps/#data-sources","title":"Data Sources","text":"<ul> <li>Training and test data from drivendata, along with submission format</li> <li>Details in problem description</li> </ul>","tags":["streamlit","python","intermediate","drivendata"]},{"location":"blog/lightgbm-water-pumps/#feature-exploration-and-engineering","title":"Feature Exploration and Engineering","text":"<p>Much of this was guided by the DrivenData Competition forum, specifically this user's EDA + Catboost example (I haven't tried out all of his data processing steps... yet)</p>","tags":["streamlit","python","intermediate","drivendata"]},{"location":"blog/lightgbm-water-pumps/#credits","title":"Credits","text":"<p>This package was created with Cookiecutter and the <code>gerardrbentley/cookiecutter-streamlit</code> project template.</p> <ul> <li>Cookiecutter: https://github.com/audreyr/cookiecutter</li> <li><code>gerardrbentley/cookiecutter-streamlit</code>: https://github.com/gerardrbentley/cookiecutter-streamlit</li> </ul>","tags":["streamlit","python","intermediate","drivendata"]},{"location":"blog/lightgbm-water-pumps/#drivendata-platform","title":"DrivenData Platform","text":"Text Only<pre><code>@misc{&lt;https://doi.org/10.48550/arxiv.1606.07781&gt;,\n  doi = {10.48550/ARXIV.1606.07781},\n\n  url = {&lt;https://arxiv.org/abs/1606.07781&gt;},\n\n  author = {Bull, Peter and Slavitt, Isaac and Lipstein, Greg},\n\n  keywords = {Human-Computer Interaction (cs.HC), Computers and Society (cs.CY), Social and Information Networks (cs.SI), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences},\n\n  title = {Harnessing the Power of the Crowd to Increase Capacity for Data Science in the Social Sector},\n\n  publisher = {arXiv},\n\n  year = {2016},\n\n  copyright = {Creative Commons Attribution 4.0 International}\n}\n</code></pre>","tags":["streamlit","python","intermediate","drivendata"]},{"location":"blog/my-favorite-python-one-liner-sucks/","title":"My Favorite Python One Liner Sucks","text":"","tags":["beginner","python"]},{"location":"blog/my-favorite-python-one-liner-sucks/#writing-to-a-file-in-one-line","title":"Writing to a file in one line","text":"<p>tl;dr it's now:</p> Python<pre><code># Write data to file called my_file.txt\nPath('my_file.txt').write_text('\\n'.join(some_data))\n</code></pre> Text Only<pre><code>15\n</code></pre> Python<pre><code># Setup\nsome_data = ['alice', 'bob', 'chuck']  # Some sample data\nfrom pathlib import Path\n</code></pre>","tags":["beginner","python"]},{"location":"blog/my-favorite-python-one-liner-sucks/#favorite-one-liner","title":"Favorite One-Liner","text":"<p>Python is full of one-liners.</p> <p>One of my favorites has been:</p> Python<pre><code>open('my_file.txt', 'w').write('\\n'.join(some_data))\n</code></pre> Text Only<pre><code>15\n</code></pre> <p>It opens a file in write mode and writes some data or other text to that file, returning the amount of bytes written to the file. Generally useful for saving a list of column names or people names or what have you. No external libraries, and even no extra imports because <code>open()</code> is a built-in function</p> <p>But it leaves the file handle open...</p> <p>Which may not be a problem in some usecases, but it can prevent other users or processes from moving or deleting the file. Google's Python style guide provides some more reasons, including wasting resources and preventing logical errors in code.</p>","tags":["beginner","python"]},{"location":"blog/my-favorite-python-one-liner-sucks/#with-statement","title":"With Statement","text":"<p>Of course the <code>with ... as ...:</code> syntax from PEP 343 is great for this safe handling of a file object that has to be opened and closed.</p> <p>It can be done in one line, but most prefer to break it up into 2. This is more awkward to use in a REPL doing ad-hoc work or notebook trying to conserve cell space </p> Python<pre><code>with open('my_file.txt', 'w') as f: f.write('\\n'.join(some_data))\n</code></pre> Python<pre><code>with open('my_file.txt', 'w') as f: \n    f.write('\\n'.join(some_data))\n</code></pre>","tags":["beginner","python"]},{"location":"blog/my-favorite-python-one-liner-sucks/#comparing-with-and-without-with","title":"Comparing With and Without (With)","text":"<p>Checking out the bytecode on a simplified <code>write</code>, we can confirm that my favorite one-liner doesn't close the file, whereas the <code>with</code> one-liner does.</p> <p>(If you're not familiar with dis, it's the Python module for disassembling Python code into its C bytecode. Not always necessary, but will prove 100% whether 2 code snippets operate the same under the covers)</p> Python<pre><code>from dis import dis\ndis(\"open('my_file.txt', 'w').write('something')\")\n</code></pre> Text Only<pre><code>  1           0 LOAD_NAME                0 (open)\n              2 LOAD_CONST               0 ('my_file.txt')\n              4 LOAD_CONST               1 ('w')\n              6 CALL_FUNCTION            2\n              8 LOAD_METHOD              1 (write)\n             10 LOAD_CONST               2 ('something')\n             12 CALL_METHOD              1\n             14 RETURN_VALUE\n</code></pre> <p>Definitely no calls to <code>close()</code>. What about in the with statement?</p> Python<pre><code>dis(\"with open('my_file.txt', 'w') as f: f.write('something')\")\n</code></pre> Text Only<pre><code>  1           0 LOAD_NAME                0 (open)\n              2 LOAD_CONST               0 ('my_file.txt')\n              4 LOAD_CONST               1 ('w')\n              6 CALL_FUNCTION            2\n              8 SETUP_WITH              26 (to 36)\n             10 STORE_NAME               1 (f)\n             12 LOAD_NAME                1 (f)\n             14 LOAD_METHOD              2 (write)\n             16 LOAD_CONST               2 ('something')\n             18 CALL_METHOD              1\n             20 POP_TOP\n             22 POP_BLOCK\n             24 LOAD_CONST               3 (None)\n             26 DUP_TOP\n             28 DUP_TOP\n             30 CALL_FUNCTION            3\n             32 POP_TOP\n             34 JUMP_FORWARD            16 (to 52)\n        &gt;&gt;   36 WITH_EXCEPT_START\n             38 POP_JUMP_IF_TRUE        42\n             40 RERAISE\n        &gt;&gt;   42 POP_TOP\n             44 POP_TOP\n             46 POP_TOP\n             48 POP_EXCEPT\n             50 POP_TOP\n        &gt;&gt;   52 LOAD_CONST               3 (None)\n             54 RETURN_VALUE\n</code></pre> <p>The context manager takes a lot more steps to manage the call stack. But there's no clear call to the <code>close()</code> function, which is the other standard way of writing to a file and closing the handle.</p> Python<pre><code>dis(\"f = open('my_file.txt', 'w')\\nf.write('something')\\nf.close()\")\n</code></pre> Text Only<pre><code>  1           0 LOAD_NAME                0 (open)\n              2 LOAD_CONST               0 ('my_file.txt')\n              4 LOAD_CONST               1 ('w')\n              6 CALL_FUNCTION            2\n              8 STORE_NAME               1 (f)\n\n  2          10 LOAD_NAME                1 (f)\n             12 LOAD_METHOD              2 (write)\n             14 LOAD_CONST               2 ('something')\n             16 CALL_METHOD              1\n             18 POP_TOP\n\n  3          20 LOAD_NAME                1 (f)\n             22 LOAD_METHOD              3 (close)\n             24 CALL_METHOD              0\n             26 POP_TOP\n             28 LOAD_CONST               3 (None)\n             30 RETURN_VALUE\n</code></pre> <p>The <code>open()</code> function code lives here.</p> <p>It's a wrapper around a FileIO object, whose <code>close</code> method will use <code>os.close()</code>, the low-level file closing method.</p> <p>The FileIO inherits a context manager from IOBase that calls <code>close</code> when exited, so we can be sure it'll get called.</p>","tags":["beginner","python"]},{"location":"blog/my-favorite-python-one-liner-sucks/#pathlib-usage","title":"Pathlib usage","text":"<p>Making the extra import is worth it for <code>Path</code> object to get the following:</p> <ul> <li>Accurate path on any OS</li> <li>file open and close with <code>write_text()</code></li> <li>Still a similar one-liner!</li> </ul> Python<pre><code>from pathlib import Path\nPath('my_file.txt').write_text('\\n'.join(some_data))\n</code></pre> Text Only<pre><code>15\n</code></pre> <p>Does the disassembler tell us anything?</p> Python<pre><code>dis(\"Path('my_file.txt').write_text('something')\")\n</code></pre> Text Only<pre><code>  1           0 LOAD_NAME                0 (Path)\n              2 LOAD_CONST               0 ('my_file.txt')\n              4 CALL_FUNCTION            1\n              6 LOAD_METHOD              1 (write_text)\n              8 LOAD_CONST               1 ('something')\n             10 CALL_METHOD              1\n             12 RETURN_VALUE\n</code></pre> <p>Not really, what about on the write_text method specifically?</p> Python<pre><code>dis(Path('my_file.txt').write_text)\n</code></pre> Text Only<pre><code>1282           0 LOAD_GLOBAL              0 (isinstance)\n               2 LOAD_FAST                1 (data)\n               4 LOAD_GLOBAL              1 (str)\n               6 CALL_FUNCTION            2\n               8 POP_JUMP_IF_TRUE        26\n\n1283          10 LOAD_GLOBAL              2 (TypeError)\n              12 LOAD_CONST               1 ('data must be str, not %s')\n\n1284          14 LOAD_FAST                1 (data)\n              16 LOAD_ATTR                3 (__class__)\n              18 LOAD_ATTR                4 (__name__)\n\n1283          20 BINARY_MODULO\n              22 CALL_FUNCTION            1\n              24 RAISE_VARARGS            1\n\n1285     &gt;&gt;   26 LOAD_FAST                0 (self)\n              28 LOAD_ATTR                5 (open)\n              30 LOAD_CONST               2 ('w')\n              32 LOAD_FAST                2 (encoding)\n              34 LOAD_FAST                3 (errors)\n              36 LOAD_CONST               3 (('mode', 'encoding', 'errors'))\n              38 CALL_FUNCTION_KW         3\n              40 SETUP_WITH              26 (to 68)\n              42 STORE_FAST               4 (f)\n\n1286          44 LOAD_FAST                4 (f)\n              46 LOAD_METHOD              6 (write)\n              48 LOAD_FAST                1 (data)\n              50 CALL_METHOD              1\n              52 POP_BLOCK\n              54 ROT_TWO\n              56 LOAD_CONST               4 (None)\n              58 DUP_TOP\n              60 DUP_TOP\n              62 CALL_FUNCTION            3\n              64 POP_TOP\n              66 RETURN_VALUE\n         &gt;&gt;   68 WITH_EXCEPT_START\n              70 POP_JUMP_IF_TRUE        74\n              72 RERAISE\n         &gt;&gt;   74 POP_TOP\n              76 POP_TOP\n              78 POP_TOP\n              80 POP_EXCEPT\n              82 POP_TOP\n              84 LOAD_CONST               4 (None)\n              86 RETURN_VALUE\n</code></pre> <p>Well that's a whole lot to end up looking like the same bytecode as the <code>with</code> statement...</p> <p>In fact, pathlib would have gotten away with it too if it weren't for that meddling source code to betray it!</p> <p>We wind up calling the same with statement, but get a free assertion that our data is a valid string:</p> Python<pre><code># pathlib.Path.write_text\n# ...\n        with self.open(mode='w', encoding=encoding, errors=errors, newline=newline) as f:\n            return f.write(data)\n</code></pre>","tags":["beginner","python"]},{"location":"blog/my-favorite-python-one-liner-sucks/#conclusion","title":"Conclusion","text":"<p>Pathlib <code>write_text()</code> is just <code>with open()...</code> under the covers.</p> <p>Nevertheless, I prefer the <code>write_text()</code> one-liner to the <code>with open() as f: ...</code> one-liner out of respect for colons.</p>","tags":["beginner","python"]},{"location":"blog/personal-domain/","title":"Share a website to the world in less than a day (Free for Students!)","text":"","tags":["beginner","web-dev","deployment","guide"]},{"location":"blog/personal-domain/#why-make-your-own-website","title":"Why make your own website?","text":"<p>Your own digital footprint and learning coding building blocks! Nowadays you can access a website from anywhere on pretty much any device, it can be simple, it can be personal, you can show your friends, you can show your family, you can show the internet. It's a pretty cool feeling to see code you wrote on someone else's screen, and hopefully it'll encourage you to keep exploring.</p> <p>If you don't mind having a slightly less custom domain then there are plenty of free options that will host your site as a 'subdomain'. In fact this site is accessible at https://gerardrbentley.gitlab.io/, which is the way I set it up with Gitlab Pages where I host this project's code. If you happen to want to know a bit more about how domains actually work, this site has a decent introduction.</p> <p>A custom domain (website) name goes for around $12 USD per year, but students can get 3! free domains with The Github dev pack (highly recommended if you're eligible).  I use Google Domains for this domain (gerardbentley.com) mostly because I trust they won't have any hidden fees, won't leak my credit card info, and will be reliable. Feel free to use GoDaddy or Namecheap or any other domain name service you prefer. Custom domains are limited to one owner, so don't be disappointed if someone else got what you wanted first (capitalism!).</p> <p>NOTE Plenty of businesses these days probably pop up in your ads telling you how easy it is to build a site with them, but that's not what this guide is about. Wix, Squarespace, and Wordpress are valid tools on their own, but this method of building a site uses tools that will be useful in more general software development (HTML, JavaScript, Continuous Integration / Deployment tools).</p>","tags":["beginner","web-dev","deployment","guide"]},{"location":"blog/personal-domain/#project-repository","title":"Project Repository","text":"<p>Gitlab is where I prefer to keep the code for my projects (repositories). It's open source with a free tier that is plenty decent for my use and because it supports the full lifecycle of your app in one place. Github is also super popular, probably still more popular than Gitlab, and Github Pages is their hosting service, but I won't discuss that further.</p> <p>For more on gitlab and some pointers on managing a project that you update over time or work on with others, see my post on Gitlab Workflow. It also has a decent web editor if you haven't set one up on your machine, see my post on how I use VS Code if you want to improve the way you type.</p>","tags":["beginner","web-dev","deployment","guide"]},{"location":"blog/pipreqs-fastapi/","title":"Pipreqs FastAPI Server + Github Bot","text":"","tags":["python","intermediate","fastapi"]},{"location":"blog/pipreqs-fastapi/#python-pipreqs-api-github-bot","title":"Python Pipreqs API + Github Bot","text":"<p>tl;dr: repo link</p> <p>Turned my #Python requirements API into a @github #bot that reminds me when I forget to update dependencies! \ud83e\udd16I used httpx and gidgethub, deployed with the existing @FastAPI server on @heroku Followed a great guide by @mariatta -&gt; https://t.co/3GthR5urKe https://t.co/ARQw007LEQ pic.twitter.com/WRbuxvF3qA</p>\u2014 @gar@gerardbentley.com (@GarsBar35Plus) April 29, 2022  <p>Powered by FastAPI, pipreqs, and gidgethub. Specifically, FastAPI runs the API server for receiving requests, pipreqs does the code requirements assessing, and gidgethub handles routing requests from Github Webhooks.</p> <p>Basic API to power bots / helpers to rid the world of Python projects without correct requirements! Simple bot to open an issue if <code>requirements.txt</code> doesn't match the output of <code>pipreqs</code>.</p> <p>Attempts to shallow <code>git clone</code> a given repo then run <code>pipreqs</code> on the codebase. Returns the resulting <code>requirements.txt</code> contents!</p>","tags":["python","intermediate","fastapi"]},{"location":"blog/pipreqs-fastapi/#how-to-use","title":"How to Use","text":"<ul> <li>Use the pyscript frontend on github pages</li> <li>Use the Streamlit frontend \ud83c\udf88</li> <li>Use the fastapi generated swagger docs</li> <li>Use <code>curl</code> or other http client</li> </ul> Bash<pre><code>curl \"https://pipreqs-api.herokuapp.com/pipreqs?code_url=https://github.com/gerardrbentley/pipreqs-api\"\n</code></pre> <p>Response</p> Text Only<pre><code>cachetools==5.0.0\nfastapi==0.75.2\ngidgethub==5.1.0\nhttpx==0.22.0\npydantic==1.9.0\npytest==7.1.2\nstreamlit==1.8.1\nuvicorn==0.17.6\n</code></pre>","tags":["python","intermediate","fastapi"]},{"location":"blog/pipreqs-fastapi/#background","title":"Background","text":"<p>Python is a popular language, but it lacks a standard build &amp; dependency management tool. The official docs recommend the third party pipenv, and others such as poetry and pants are available for other preferences. Even conda can provide environment and dependency management!</p> <p>Slowing things down, the basic way of installing Python packages is with <code>pip</code> and a file called <code>requirements.txt</code> (which is a list of packages like the one above!)</p> <p>Common guides will recommend running something like:</p> Bash<pre><code>pip install -r requirements.txt\n</code></pre> <p>Or maybe this to make sure the active Python environment's pip gets used:</p> Bash<pre><code>python -m pip install -r requirements.txt\n</code></pre>","tags":["python","intermediate","fastapi"]},{"location":"blog/pipreqs-fastapi/#deployments","title":"Deployments","text":"<p>This project had the goal of reducing headaches from the following loop:</p> <ul> <li>Write code for a Python / Streamlit app</li> <li>Push the code to a github repo</li> <li>Connect the repo to Streamlit Cloud</li> <li>Wait for deploy on Streamlit Cloud</li> <li>Get <code>\"No module named\"</code> or other error from not having up-to-date requirements</li> </ul> <p>So why is this the way it is?</p> <p>I could say it's because \"Python lacks a standard build &amp; dependency management tool\", but I know that a simple build script / Dockerfile and something like pre-commit can prevent these headaches before pushing code.</p> <p>So then the answer is a combination of laziness and a desire to not bloat every repo with many tools and scripts. Part of the beauty of managed application hosts such as Heroku and Streamlit Cloud is the minimal amount of setup needed to launch your app.</p> <p>A truly minimal Streamlit Cloud repo just needs the <code>.py</code> file that holds the <code>streamlit</code> calls. Once your app needs more third party Python packages than <code>streamlit</code>, some kind of dependency file is needed for the Platform to know how to install and run your app.</p>","tags":["python","intermediate","fastapi"]},{"location":"blog/pipreqs-fastapi/#manual-method","title":"Manual Method","text":"<p>The essence of this API and bot can be boiled down to 2 CLI calls:</p> <ul> <li><code>git clone --depth 1</code></li> <li><code>pipreqs --print</code></li> </ul> <p>The clone command will download a copy of the repository that we want to check for requirements. The <code>--depth 1</code> flag is important to limit how many files get downloaded (we don't need the whole repo history, just the latest).</p> <p>pipreqs is a tool that analyzes a directory containing Python files and aims to produce a <code>requirements.txt</code> file with every third party package that is imported in the project directory. The <code>--print</code> flag will tell pipreqs not to produce a file, but just to print out the file contents to stdout.</p> <p>You can try this on your own!</p> Bash<pre><code>git clone --depth 1 https://github.com/gerardrbentley/pipreqs-api test_dir\npython -m pip install pipreqs\npipreqs --print test_dir\n\n# clean up\nrm -r test_dir\n</code></pre>","tags":["python","intermediate","fastapi"]},{"location":"blog/pipreqs-fastapi/#python-method","title":"Python Method","text":"<p>From Python we can either use a library such as gitpython, or make the system call to <code>git</code> with the subprocess standard library module.</p> <p>Here is a basic example (using <code>.split()</code> to avoid using shell execution)</p> Python<pre><code>import subprocess\nsubprocess.run('git clone --depth 1 https://github.com/gerardrbentley/pipreqs-api test_dir'.split())\n</code></pre> <p>Since <code>pipreqs</code> uses <code>docopt</code> to parse CLI arguments, I figured it would be easy enough to repeat the <code>subprocess</code> pattern and be sure to capture the stdout output (the requirements!)</p>","tags":["python","intermediate","fastapi"]},{"location":"blog/pipreqs-fastapi/#api-endpoint","title":"API Endpoint","text":"<p>For users to access our function over the internet (and for bots \ud83e\udd16 to access it!) we'll serve it up in a REST API. Python has plenty of web framework libraries, pretty much any of them would handle this fine.</p> <p>For version 1 of this system I've chosen to have a single endpoint that responds to HTTP GET requests and requires a query argument called <code>code_url</code>. All it will respond with is plain text containing the <code>requirements.txt</code> contents and a <code>200</code> status code.</p> <p>Here's what that looks like in my FastAPI app:</p> Python<pre><code>@router.get(\"/pipreqs\", response_class=PlainTextResponse)\nasync def pipreqs_endpoint(code_url: str):\n    ... # Clone the repo from code_url\n    ... # Run pipreqs on the cloned repo\n    return requirements\n</code></pre>","tags":["python","intermediate","fastapi"]},{"location":"blog/pipreqs-fastapi/#async-python","title":"Async Python","text":"<p>This method so far works fine for a single user, but imagine the system with multiple users / bots making requests at once. Each <code>git clone</code> and <code>pipreqs</code> call would have to complete for a single user before the next in the queue can be processed.</p> <p><code>git clone</code> relies on the I/O speed of our server to write files, the remote git server to read the files, and the network speed to get them to our server. <code>pipreqs</code> includes synchronous <code>requests</code> calls to pypi, so it relies on the response speed of the pypi server and the I/O speed of our server to loop over project files and run <code>ast.parse</code> (from standard library) on the <code>.py</code> files.</p> <p>WSGI frameworks such as Flask and Bottle rely on threading tricks such as gevent greenlets (read more from Bottle) to handle many connections in a single process. ASGI frameworks such as FastAPI (built on Starlette) instead rely on a single-threaded event loop and async coroutines to handle many connections.</p> <p>Above we defined the <code>/pipreqs</code> route handler with <code>async def</code> function. In FastAPI our function will already be running in the event loop so we can use <code>await</code> in our code to utilize other <code>async</code> functions!</p> <p>Speaking of other <code>async</code> functions, we can swap out the synchronous <code>subprocess.run()</code> with another standard library function <code>asyncio.create_subprocess_exec()</code> (see the asyncio subprocess docs). This will ensure that our program can let <code>git clone</code> and <code>pipreqs</code> run without hanging up our server process when there's no work to check on.</p> <p>A general asynchronous run function using exec needs a try block (using <code>create_subprocess_shell</code> will return the errors in <code>stderr</code> without raising).</p> Python<pre><code>import asyncio\nfrom typing import Tuple\n\nasync def _run(cmd: str) -&gt; Tuple[str, str, int]:\n    try:\n        proc = await asyncio.create_subprocess_exec(\n            *cmd.split(),\n            stdout=asyncio.subprocess.PIPE,\n            stderr=asyncio.subprocess.PIPE,\n        )\n    except Exception as e:\n        log.exception(e, stack_info=True)\n        return \"\", str(e), 1\n    stdout, stderr = await proc.communicate()\n\n    return stdout.decode(), stderr.decode(), proc.returncode\n</code></pre>","tags":["python","intermediate","fastapi"]},{"location":"blog/pipreqs-fastapi/#primary-function","title":"Primary Function","text":"<p>Above in the endpoint code we glossed over the most important details:</p> <ul> <li>Clone the repo from <code>code_url</code></li> <li>Run <code>pipreqs</code> on the cloned repo</li> </ul> <p>When we clone a repo we don't need to store its contents forever. That would take a lot of storage space and each repo will get new commits that invalidate the old downloads anyway. Making a temporary directory, cloning into it, running <code>pipreqs</code> on it, then deleting it will work just fine.</p> <p>PRE-NOTE: We'll continue to use <code>async</code> functions</p> Python<pre><code>import tempfile\n\nasync def pipreqs_from_url(code_url: str) -&gt; Tuple[str, str]:\n    with tempfile.TemporaryDirectory() as dir_path:\n        old_requirements = await fetch_code(code_url, dir_path)\n        pipreqs_output = await run_pipreqs(code_url, dir_path)\n    return pipreqs_output, old_requirements\n</code></pre> <p>NOTE: We're adding a bit of complexity to prepare for a bot use-case, which is comparing existing <code>requirements.txt</code> to <code>pipreqs</code> result.</p> <p>To clone the repo and check the contents of <code>requirements.txt</code>, we can do something like this:</p> Python<pre><code>async def fetch_code(\n    code_url: str, destination_dir: str, requirements_path: str = \"requirements.txt\"\n) -&gt; str:\n    clone_cmd = f\"git clone --depth 1 {code_url!r} {destination_dir!r}\"\n    stdout, stderr, returncode = await _run(clone_cmd)\n\n    if returncode != 0:\n        message = f\"Could not clone the code from {code_url!r}!\"\n        log.exception(message, stack_info=True)\n        raise HTTPException(status_code=400, detail=message)\n\n    old_requirements = Path(destination_dir) / requirements_path\n    if old_requirements.is_file():\n        return old_requirements.read_text()\n    else:\n        return \"\"\n</code></pre> <p>And to run pipreqs on the directory we just cloned:</p> Python<pre><code>async def run_pipreqs(code_url: str, dir_path: str) -&gt; str:\n    clone_cmd = f\"pipreqs --print {dir_path}\"\n    stdout, stderr, returncode = await _run(clone_cmd)\n    if returncode != 0:\n        message = f\"Could not run pipreqs on the code from {code_url!r}!\"\n        log.exception(message, stack_info=True)\n        raise HTTPException(status_code=500, detail=message)\n    return stdout\n</code></pre> <p>POST-NOTE: using HTTPException from FastAPI couples the logic of our program to the API component. I'm fine with this for now, but a more specific error to the function would allow it to be re-used more easily in another app.</p>","tags":["python","intermediate","fastapi"]},{"location":"blog/pipreqs-fastapi/#caching","title":"Caching","text":"<p>If you've written a bot that makes requests to an external API then you might see making unlimited <code>git clone</code> and <code>pipreqs</code> calls as a potential problem.</p> <p>One strategy to prevent (some) repeated requests to external APIs is by caching results for your server to reference when the same user request comes in. The Python standard library has <code>functools.lru_cache</code>, but in this case I want the results to expire if the cache becomes too large and also if more than 5 minutes has passed.</p> <p>I used <code>cachetools</code> library for a tested TTLCache implementation, but to get it to work with an asynchronous function we have to get a little creative.</p> Python<pre><code>from cachetools import TTLCache\n\nclass RequirementsCache(TTLCache):\n    def __missing__(self, code_url):\n        future = asyncio.create_task(pipreqs_from_url(code_url))\n        self[code_url] = future\n        return future\n</code></pre> <p>Since <code>cachetools</code> caches act much like a Dictionary, we can overwrite the dunder method that gets called when we try to access a key that doesn't exist! Here we create a future / task, which is to run the <code>pipreqs_from_url</code> function.</p> <p>Then we assign that future as the value for the <code>code_url</code> key (so that future checks will access that instead of going to <code>__missing__</code>)</p> <p>Finally we return the bare future so that we don't lock up the program while executing <code>pipreqs_from_url</code> immediately and synchronously.</p>","tags":["python","intermediate","fastapi"]},{"location":"blog/pipreqs-fastapi/#singular-cache","title":"Singular Cache","text":"<p><code>functools.lru_cache</code> does come in handy for creating an object that serves as a singleton in Python. We can treat the <code>RequirementsCache</code> for our program this way and make a small, cached wrapper function that returns it once and only once.</p> Python<pre><code>@functools.lru_cache(maxsize=1)\ndef get_requirements_cache() -&gt; RequirementsCache:\n    requirements_cache = RequirementsCache(1024, 300)\n    return requirements_cache\n</code></pre>","tags":["python","intermediate","fastapi"]},{"location":"blog/pipreqs-fastapi/#using-the-cache","title":"Using the cache","text":"<p>Now we can fill in the API endpoint code!</p> <p>The wrapper function to fetch the cache is synchronous so we don't need to await <code>get_requirements_cache()</code>. Then we use Dictionary bracket notation to ask the cache for the future, which we must <code>await</code> to get the actual returned value from <code>pipreqs_from_url</code></p> <p>The <code>__missing__</code> function we wrote above will get called when a given <code>code_url</code> isn't in the Cache. Otherwise the cache can return the same future that was stored earlier in <code>__missing__</code>!</p> Python<pre><code>@router.get(\"/pipreqs\", response_class=PlainTextResponse)\nasync def pipreqs_endpoint(code_url: str):\n    requirements_cache = get_requirements_cache()\n    requirements, old_requirements = await requirements_cache[code_url]\n    return requirements\n</code></pre> <p>For this endpoint the <code>old_requirements</code> isn't useful, so we just return the <code>pipreqs</code> generated requirements.</p>","tags":["python","intermediate","fastapi"]},{"location":"blog/pipreqs-fastapi/#github-bot","title":"Github Bot","text":"<p>I followed this guide by Python Core developer and Github Bot builder Mariatta, but adapted it to fit in FastAPI.</p> <p>The main concept of our bot interaction goes as follows:</p> <ul> <li>Whenever there is a push to my repository the bot should be notified with at least the url and whoever made the push</li> <li>The bot should then run our <code>pipreqs_from_url</code> function on the repository url and this time compare the generated <code>requirements.txt</code> to any existing <code>requirements.txt</code></li> <li>If there's a difference between the files then the bot should open an Issue on the repository to note the differences</li> </ul> <p>Maybe it seems simple in concept but there's a few technical and non-technical aspects to highlight.</p>","tags":["python","intermediate","fastapi"]},{"location":"blog/pipreqs-fastapi/#webhooks","title":"Webhooks","text":"<p>Github (among other git hosting services) allows you to set up webhooks on your repositories. This is the technology that will notify our bot whenever someone pushes a commit.</p> <p>Webhooks prevent our bot service from having to constantly ping the repository to check for changes. So long as we trust Github's servers are running correctly, we can be confident that our bot will be notified whenever a push happens.</p> <p>We can set one up manually in a repo with a secret key to ensure no attacker sends bogus data. To expand this idea to other users, a Github app or adapting to PubSub might be necessary to ease the creation and access of webhooks.</p> <p>Check out the webhook events and payloads documentation for more on what Github will send to our bot. Which brings us to the next step, what exactly is the bot?</p>","tags":["python","intermediate","fastapi"]},{"location":"blog/pipreqs-fastapi/#gidgethub-endpoint-the-bot","title":"Gidgethub Endpoint (the bot)","text":"<p>Our bot is actually another FastAPI endpoint! It's barely a robot at all!</p> <p>Since we already have a FastAPI server that will be running (to host the main endpoint), it makes sense to me to include our bot as an additional endpoint. Another strategy would be to utilze a new web server hosted somewhere else to listen for Github's webhook events.</p> <p>The guide linked above utilizes aiohttp as both the server and the http client for interacting with Github after receiving a webhook event (since we'll open an Issue in some cases). Another option is to use a \"serverless\" technology such as AWS Lambda as your bot host, which might reduce your server costs if it is idling a lot.</p> <p>Here's what the endpoint looks like FastAPI / Starlette. Gidgethub is doing the heavy lifting, we simply pass along request headers and body to the library in order to validate and parse them.</p> Python<pre><code>from fastapi import APIRouter, Request, Response\nfrom gidgethub import routing, sansio\n\nrouter = APIRouter()\ngh_router = routing.Router()\n\n\n@router.post(\"/\", response_class=Response)\nasync def main(request: Request):\n    body = await request.body()\n    secret = os.environ.get(\"GITHUB_SECRET\")\n    oauth_token = os.environ.get(\"GITHUB_TOKEN\")\n    event = sansio.Event.from_http(request.headers, body, secret=secret)\n    async with httpx.AsyncClient() as client:\n        gh = gidgethub.httpx.GitHubAPI(\n            client, \"gerardrbentley\", oauth_token=oauth_token\n        )\n        await gh_router.dispatch(event, gh)\n    return Response(status_code=200)\n</code></pre>","tags":["python","intermediate","fastapi"]},{"location":"blog/pipreqs-fastapi/#automatically-opening-an-issue","title":"Automatically Opening an Issue","text":"<p>I considered 3 ways to alert the repo owner (myself in this case) of a potential dependency mismatch:</p> <ul> <li>Send an email</li> <li>Open a Pull Request with new <code>requirements.txt</code></li> <li>Open an Issue with new and old <code>requirements.txt</code></li> </ul> <p>Reasons in favor of opening an Issue:</p> <ul> <li>There may be Python modules that aren't used in deployment that trigger extra dependencies in pipreqs</li> <li>A specific older vesion of a package might be required (pipreqs grabs latest version)</li> <li>Allows the user to set email preferences for alerts</li> </ul> <p>The main reasons against opening a Pull Request:</p> <ul> <li>Thinking about scaling this bot into a scraper of sorts for other people's projects, it's not always courteous to open a Pull Request without alerting the maintainer in an Issue</li> <li>Some projects prefer <code>environment.yml</code> or <code>pyproject.toml</code> for dependencies</li> </ul> <p>Utilizing <code>gidgethub</code>, we can write a function that responds to any <code>push</code> events to the repository:</p> Python<pre><code>@gh_router.register(\"push\")\nasync def push_to_repo_event(event, gh, *args, **kwargs):\n    \"\"\"\n    Whenever a push is made, check the requirements matches pipreqs output; else open an issue.\n    \"\"\"\n    url = event.data[\"repository\"][\"url\"]\n    pusher = event.data[\"pusher\"][\"name\"]\n\n    log.info(\n        f\"Recent push by @{pusher} to {url}! Checking if requirements.txt is in line.\"\n    )\n    ...\n</code></pre> <p>Next, we can utilize our <code>requirements_cache</code> the same way as in the API endpoint to fetch the requirements and old_requirements (if present) for the repo. If our pipreqs requirements match what is in the repo, then there's nothing to do for the bot. Otherwise, we'll grab the provided url for interacting with Issues in the repo:</p> Python<pre><code>    requirements_cache = get_requirements_cache()\n    requirements, old_requirements = await requirements_cache[url]\n    if requirements == old_requirements:\n        log.debug(f\"Requirements satisfied in repo {url}\")\n    else:\n        issues_url = event.data[\"repository\"][\"issues_url\"]\n</code></pre> <p>Finally, we'll write a short message and use the <code>gh</code> http client to make the POST request to alert the repository owner and whoever submitted the push!</p> Python<pre><code>        payload = {\n            \"title\": \"Check dependencies\",\n            \"body\": f\"\"\"\\\n\u26a0\ufe0f Warning! Please confirm dependencies are satisfied.\nRequirements generated by pipreqs:\n\n```txt\n{requirements}\n```\"\"\",\n            \"assignees\": [pusher],\n        }\n        await gh.post(issues_url, data=payload)\n</code></pre>","tags":["python","intermediate","fastapi"]},{"location":"blog/pipreqs-fastapi/#testing","title":"Testing","text":"<p>We can use <code>pytest</code> and a handful of its helpers to make testing the API and subprocess calls a bit easier:</p> Text Only<pre><code>pytest==7.1.1\npytest-asyncio==0.18.3\npytest-cov==3.0.0\npytest-mock==3.7.0\n</code></pre> <p>By using a <code>conftest.py</code> we can prepare a Starlette test client for testing the API endpoints:</p> Python<pre><code># conftest.py\nimport pytest\nfrom fastapi.testclient import TestClient\n\nfrom app.config import Settings, get_settings\nfrom app.main import create_application\n\n\ndef get_settings_override():\n    return Settings(testing=1)\n\n\n@pytest.fixture(scope=\"module\")\ndef test_app():\n    app = create_application()\n    app.dependency_overrides[get_settings] = get_settings_override\n    with TestClient(app) as test_client:\n        yield test_client\n</code></pre> <p>Then in our <code>test_pipreqsapi.py</code> we need to make sure to cue <code>pytest-asyncio</code> and clear the cache after each test in order to validate call counts:</p> Python<pre><code>@pytest.mark.asyncio\nclass TestPipreqsApi:\n    \"\"\"Test backend logic that handles cloning repos and running pipreqs\"\"\"\n\n    @pytest.fixture(autouse=True)\n    def clear_cache(self):\n        yield\n        cache = get_requirements_cache()\n        cache.clear()\n</code></pre> <p>From there we can test each function, building up to the main endpoint. We'll include general tests on whether things succeed when given good inputs or mocks and whether they fail with the intended errors when bad things happen.</p> <p>Testing our async subprocess run is a good example. Here we use known command <code>ls</code> and broken command <code>lz</code> to verify our function responds as we expect.</p> Python<pre><code>    async def test_async_run_succeeds(self):\n        stdout, stderr, resultcode = await _run(\"ls\")\n        assert resultcode == 0\n\n    async def test_async_run_fails_bad_command(self):\n        stdout, stderr, resultcode = await _run(\"lz\")\n        assert resultcode == 1\n        assert stderr == \"[Errno 2] No such file or directory: 'lz'\"\n</code></pre> <p>(I don't include git clone here since it needs to go out to an external server without other setup / guarantees. It's definitely worth an end to end test though at some point.)</p> <p>We can use monkeypatching to fake an expected result now that we've verified our <code>_run</code> function works. This next test assumes we cloned the repo and that it didn't have an existing <code>requirements.txt</code>:</p> Python<pre><code>    async def test_fetch_code_succeeds(self, monkeypatch):\n        async def fake_run_success(*args):\n            return None, None, 0\n\n        monkeypatch.setattr(pipreqsapi, \"_run\", fake_run_success)\n        res = await fetch_code(\"good_clone_url\", \"none\")\n        assert res == \"\"\n</code></pre> <p>To simulate the case where there is a <code>requirements.txt</code> we could set up a dummy repo with one or use mocking to pretend that any Path from pathlib can find it and read it:</p> Python<pre><code>MOCK_REQS = \"\"\"fastapi==0.75.1\ngunicorn==20.1.0\npipreqs==0.4.11\nuvicorn==0.17.6\"\"\"\n...\n\n    async def test_fetch_code_returns_requirements(self, monkeypatch, mocker):\n        async def fake_run_success(*args):\n            return None, None, 0\n\n        monkeypatch.setattr(pipreqsapi, \"_run\", fake_run_success)\n        mocker.patch.object(Path, \"is_file\", return_value=True)\n        mocker.patch.object(Path, \"read_text\", return_value=MOCK_REQS)\n\n        res = await fetch_code(\"good_clone_url\", \"none\")\n        assert res == MOCK_REQS\n</code></pre> <p>We can also use mocking to keep track of how many times a function gets called. This is useful for validating that our cache will not call the same function twice when it has the value cached already.</p> Python<pre><code>    async def test_requirements_cache_caches(self, mocker):\n        mock_pipreqs_from_url = mocker.patch(\n            \"app.pipreqsapi.pipreqs_from_url\", return_value=MOCK_REQS\n        )\n        cache = get_requirements_cache()\n        result = await cache[\"good_clone_url\"]\n        second_result = await cache[\"good_clone_url\"]\n        assert result == MOCK_REQS\n        mock_pipreqs_from_url.assert_called_once_with(\"good_clone_url\")\n        assert second_result == MOCK_REQS\n        mock_pipreqs_from_url.assert_called_once_with(\"good_clone_url\")\n</code></pre> <p>Finally for the endpoints we'll need to rely on the <code>test_app</code> established in <code>conftest.py</code>. We can assert things about the status and any expected errors:</p> Python<pre><code>    async def test_pipreqs_endpoint_succeeds(self, test_app, monkeypatch):\n        async def fake_pipreqs_from_url_success(*args):\n            return (MOCK_REQS, \"\")\n\n        monkeypatch.setattr(\n            pipreqsapi, \"pipreqs_from_url\", fake_pipreqs_from_url_success\n        )\n        response = test_app.get(\"/pipreqs\", params={\"code_url\": \"good_clone_url\"})\n        assert response.status_code == 200\n        assert response.text == MOCK_REQS\n\n    async def test_pipreqs_endpoint_fails_bad_repo(self, test_app):\n        response = test_app.get(\"/pipreqs\", params={\"code_url\": \"bad_clone_url\"})\n        assert response.status_code == 400\n        assert (\n            response.json()[\"detail\"]\n            == \"Could not clone the code from 'bad_clone_url'!\"\n        )\n</code></pre>","tags":["python","intermediate","fastapi"]},{"location":"blog/pipreqs-fastapi/#next-steps","title":"Next Steps","text":"<p>Overall this was a fun project for creating a FastAPI server with caching and async distributed transactions.</p> <p>It also got me to think about github bots a bit and how we might clean up Python repos going forward.</p>","tags":["python","intermediate","fastapi"]},{"location":"blog/pipreqs-fastapi/#bot","title":"Bot","text":"<p>This server works for exposing the pipreqs functionality via API, but the bot leaves some steps to be desired. As of now the <code>gidgethub</code> handling relies on my own github account's access token and the webhooks are validated based on the secret established for my particular repo.</p> <p>The bot might be better applied as a Github App to allow users to more easily click a button and get the functionality.</p>","tags":["python","intermediate","fastapi"]},{"location":"blog/pipreqs-fastapi/#api","title":"API","text":"<p>Adding features already available in pipreqs would be straightforward with further optional query params:</p> <ul> <li>min / compatible version options</li> <li>using non-pypi package host</li> </ul> <p>Adding features not already in pipreqs would be more involved or hacky:</p> <ul> <li>Convert from <code>requirements.txt</code> to <code>pyproject.toml</code> / <code>environment.yml</code></li> <li>Async parsing and fetching package info</li> </ul> <p>Another idea is on-demand Issue / Pull requests on a given repo (for helping out others without putting too much effor in yourself)</p>","tags":["python","intermediate","fastapi"]},{"location":"blog/python-form-generator/","title":"Python Form Generator (JSON to Streamlit Form!)","text":"","tags":["intermediate","streamlit","python"]},{"location":"blog/python-form-generator/#python-form-generator","title":"Python Form Generator","text":"<p>From raw JSON to Python Pydantic Model to Streamlit Input Form </p> <p>Bootstrapping more Streamlit apps from within a Streamlit app </p> <p>Generate well-typed and validated Python forms from basic JSON example.</p> <p>DEPENDENCIES:</p> <ul> <li>Datamodel Code Generator for mapping to Pydantic model</li> <li>JSON to Pydantic for proof this can work</li> <li>Streamlit Pydantic for bootstrapped app code</li> </ul>","tags":["intermediate","streamlit","python"]},{"location":"blog/python-form-generator/#full-walkthrough","title":"Full Walkthrough","text":"","tags":["intermediate","streamlit","python"]},{"location":"blog/python-form-generator/#skeleton","title":"Skeleton","text":"<p>GOAL: Initialize git repo with necessary file scaffold:</p> <ul> <li><code>app.py</code>: The main streamlit entry app</li> <li><code>requirements.txt</code>: Pin versions of Python packages so deployment and development is repeatable</li> <li><code>README.md</code>: This documentation</li> <li><code>.gitignore</code>: Prevent secrets or large files or unecessary things from going into github<ul> <li>Recommended github python gitignore template works fine for this, slightly overkill</li> </ul> </li> <li><code>LICENSE</code>: Apache 2.0 same as Streamlit Terms</li> </ul> Bash<pre><code>mkdir python-form-generator\ncd python-form-generator/\ntouch README.md LICENSE .gitignore app.py requirements.txt\ngit init\ngit checkout -b main\ngit remote add origin git@github.com:gerardrbentley/python-form-generator.git\ngit add .\ngit commit -m \"skeleton\"\n</code></pre>","tags":["intermediate","streamlit","python"]},{"location":"blog/python-form-generator/#install","title":"Install","text":"<p>We'll plan on 4 main packages for this:</p> <ul> <li><code>streamlit</code></li> <li><code>pydantic</code></li> <li><code>streamlit-pydantic</code></li> <li><code>datamodel-code-generator</code></li> </ul> Bash<pre><code>python -m venv venv\n. ./venv/bin/activate\npip install streamlit-pydantic datamodel-code-generator\npip list | grep \"streamlit\\|pydantic\\|datamodel-code\"\n</code></pre> <p>Note: This lets <code>streamlit-pydantic</code> to choose which versions of <code>streamlit</code> and <code>pydantic</code> work for the latest release.</p> <p>Saving the output to <code>requirements.txt</code> in the format:</p> Text Only<pre><code>datamodel-code-generator==0.11.19\npydantic==1.9.0\nstreamlit==1.5.1\nstreamlit-pydantic==0.5.0\n</code></pre> <p>Saving the output to requirements.txt allows new users / deploys to install pinned verions with something like the following:</p> <p><code>pip install -r requirements.txt</code></p> Bash<pre><code>git add requirements.txt README.md\ngit commit -m \"install\"\n</code></pre>","tags":["intermediate","streamlit","python"]},{"location":"blog/python-form-generator/#hello-world","title":"Hello World","text":"<p>Adding the following to <code>app.py</code>:</p> Python<pre><code>import streamlit as st\n\n\ndef main() -&gt; None:\n    st.header(\"Python Form Generator\")\n    st.subheader(\"Enter your JSON and get a free Pydantic model + Streamlit Input Form using it!\")\n    json_input = st.text_area('JSON example', height=200)\n    st.write(json_input)\n\n\nif __name__ == \"__main__\":\n    main()\n</code></pre> <p>Then run with <code>streamlit</code> to check if basic input / processing frontend works.</p> Bash<pre><code>streamlit run app.py\n# ctrl + c to stop\n</code></pre> Bash<pre><code>git add app.py README.md\ngit commit -m \"input hello world\"\n</code></pre>","tags":["intermediate","streamlit","python"]},{"location":"blog/python-form-generator/#model-generation","title":"Model Generation","text":"<p>We'll draw from 3 sources for the meat and potatoes of this app:</p> <ul> <li>Streamlit Pydantic Usage Guide</li> <li>Datamodel Code Generator conversion</li> <li>JSON to Pydantic conversion</li> </ul>","tags":["intermediate","streamlit","python"]},{"location":"blog/python-form-generator/#imports","title":"Imports","text":"<p>Expand import section of <code>app.py</code> to accomodate Streamlit Pydantic, Datamodel code generator, and GenSON</p> <p>(Datamodel code generator is intended as CLI tool, so we have to dig a bit into it)</p> Python<pre><code>import json\n\nfrom pydantic import Json, BaseModel\nfrom datamodel_code_generator.parser.jsonschema import JsonSchemaParser\nfrom genson import SchemaBuilder\nimport streamlit as st\nimport streamlit_pydantic as sp\n</code></pre>","tags":["intermediate","streamlit","python"]},{"location":"blog/python-form-generator/#upgraded-input","title":"Upgraded Input","text":"<p>We'll use Streamlit Pydantic to get out of the box validation of the JSON that the user provides.  Plus we get extensibility via the FormModel if we want to add more configuration options!</p> <p>Instead of a plain <code>text_input</code> from streamlit, we utilize <code>sp.pydantic_form</code> and provide our model (which only has 1 input field for now!)</p> <p>After getting some input JSON from the User it will pass off to converting to a model.</p> Python<pre><code>class FormGeneratorModel(BaseModel):\n    model_schema: Json\n\n\ndef main() -&gt; None:\n    st.header(\"Python Form Generator\")\n    st.subheader(\n        \"Enter your JSON and get a free Pydantic model + Streamlit Input Form using it!\"\n    )\n    data = sp.pydantic_form(key=\"json_input\", model=FormGeneratorModel)\n    if data:\n        show_generated_code(data.model_schema)\n</code></pre>","tags":["intermediate","streamlit","python"]},{"location":"blog/python-form-generator/#conversion","title":"Conversion","text":"<p>Following in the footsteps of JSON to Pydantic and Datamodel Code Generator, we use GenSON to build a JSONSchema representation from raw JSON data, then dump that into the Datamodel Code Generator parser.</p> <p>We'll handle the same error case Datamodel code generator does, otherwise assume the happy path and display the results!</p> Python<pre><code>def show_generated_code(schema: Json) -&gt; None:\n    model_code = json_to_pydantic(schema)\n\n    if not model_code:\n        st.error(\"Models not found in the input data\")\n    else:\n        st.code(model_code)\n\n\ndef json_to_pydantic(input_text: str) -&gt; str:\n    builder = SchemaBuilder()\n    builder.add_object(input_text)\n    schema = builder.to_schema()\n    parser = JsonSchemaParser(\n        source=json.dumps(schema),\n        base_class=\"pydantic.BaseModel\",\n    )\n\n    return parser.parse()\n</code></pre>","tags":["intermediate","streamlit","python"]},{"location":"blog/python-form-generator/#lgtm","title":"LGTM","text":"<p>Trying out a simple entry (even simpler than the littlest fullstack app) such as the following:</p> Text Only<pre><code>{\"body\": \":tada:\", \"username\": \":cat:\"}\n</code></pre> <p>Produces expected result</p> Python<pre><code>from __future__ import annotations\n\nfrom pydantic import BaseModel\n\n\nclass Model(BaseModel):\n    body: str\n    username: str\n</code></pre> <p>Time to ship it and generate a Form!</p> Bash<pre><code>git add app.py README.md\ngit commit -m \"model generation\"\n</code></pre>","tags":["intermediate","streamlit","python"]},{"location":"blog/python-form-generator/#bootstrapping","title":"Bootstrapping","text":"<p>Alright, we've got a nice Pydantic model, time to generate a Streamlit Pydantic scaffold from it and provide a download link.</p>","tags":["intermediate","streamlit","python"]},{"location":"blog/python-form-generator/#update-show-model","title":"Update Show Model","text":"<p>We'll allow the user to download just the model into a <code>.py</code> file and hide the generated model unless they want to see it.</p> Python<pre><code>def show_generated_code(schema: Json) -&gt; None:\n    model_code = json_to_pydantic(schema)\n\n    if not model_code:\n        st.error(\"Models not found in the input data\")\n    else:\n        with st.expander(\"Original Converted Model\"):\n            st.code(model_code, language=\"python\")\n        st.download_button(\"Download Generated Model Only\", data=model_code, file_name=\"model.py\", mime=\"text/plain\")\n        show_generated_form(model_code)\n</code></pre>","tags":["intermediate","streamlit","python"]},{"location":"blog/python-form-generator/#inject-generated-code","title":"Inject Generated Code","text":"<p>We only need 2 features to really bootstrap these forms:</p> <ul> <li>Added imports</li> <li>Retrieve form data in Streamlit app</li> </ul> <p>I'll add a <code>main()</code> method to keep scope contained, but the Streamlit execution model embraces all Python scripting</p> Python<pre><code>MAIN_TEMPLATE = \"\"\"\\\n\n\ndef main() -&gt; None:\n    st.header(\"Model Form Submission\")\n    data = sp.pydantic_form(key=\"my_model\", model=Model)\n    if data:\n        st.json(data.json())\n\n\nif __name__ == \"__main__\":\n    main()\n\"\"\"\n\ndef show_generated_form(model_code: str) -&gt; None:\n    code_lines = model_code.split('\\n')\n    code_lines.insert(2, \"import streamlit_pydantic as sp\")\n    code_lines.insert(2, \"import streamlit as st\")\n\n    code_lines.insert(-1, MAIN_TEMPLATE)\n\n    full_code = '\\n'.join(code_lines)\n\n    st.subheader(\"Generated Streamlit Pydantic App\")\n    st.caption(\"Download it and run with `streamlit run model_form.py`\")\n    st.download_button(\"Download Generated Form!\", data=full_code, file_name=\"model_form.py\", mime=\"text/plain\")\n    st.code(full_code, language=\"python\")\n</code></pre>","tags":["intermediate","streamlit","python"]},{"location":"blog/python-form-generator/#lgtm_1","title":"LGTM","text":"<p>Testing out the download and run on our simple model yields great results!</p> Bash<pre><code>git add app.py README.md\ngit commit -m \"app generation\"\n</code></pre>","tags":["intermediate","streamlit","python"]},{"location":"blog/python-form-generator/#wrap-up","title":"Wrap Up","text":"<p>That's it for the basic idea!</p> <p>Next steps would be allowing configuration options such as \"all Optional\" and snake casing akin to JSON to Pydantic. Also more input forms, as Datamodel code generator handles OpenAPI, CSV data, and more.</p> Bash<pre><code>git add README.md\ngit commit -m \"lgtm!\"\ngit push origin main\n</code></pre>","tags":["intermediate","streamlit","python"]},{"location":"blog/run-python/","title":"3 Ways to Run Python Code","text":"<p>Cutting to the chase:</p> <ul> <li><code>python -i</code> in a terminal. This begins an \"interactive\" or \"REPL\" session where you can enter code line by line or copy and paste into it. (you might be able to just enter <code>python</code>)</li> <li><code>python script_name.py</code> in a terminal with a valid python file named \"script_name.py\" in the current directory. This runs the code in the file then exits<ul> <li>Bonus: <code>python -i script_name.py</code> combines both. It runs the code in the file, then allows you to continue entering python code in the REPL</li> </ul> </li> <li><code>python -m jupyter notebook</code> lets you access a code notebook in your browser for a mix of live coding and code editing. Requires <code>pip install notebook</code> or a code editor with ability to handle <code>.ipynb</code> files (such as VS Code)</li> </ul>","tags":["python","beginner"]},{"location":"blog/run-python/#the-repl","title":"The REPL","text":"<p><code>python -i</code> in a terminal.</p> <p>The Python \"interactive\" session. Known in other languages as a Read-Eval-Print-Loop (REPL).</p> <p>This method of running Python is useful for small bits of code and experiments. Need to test out what happens when you multiply an int and a string? Try it out in the REPL:</p> Python<pre><code>&gt;&gt;&gt; 3 * \"hello world \"\n</code></pre> <p>note: The common notation in books and online is to use the <code>&gt;&gt;&gt;</code> that appears in the terminal to indicate code can be entered as input to the Python REPL. Don't include the <code>&gt;&gt;&gt;</code> characters when copying though!</p> <p>It's hard to save your work and sometimes awkward to paste code into the REPL as each line is executed without a chance to edit.</p>","tags":["python","beginner"]},{"location":"blog/run-python/#upgrading-the-repl","title":"Upgrading the REPL","text":"<p>Try out iPython. It provides a much richer experience with features such as autocomplete, command history, and saving your session.</p> <p>It uses the same Jupyter kernel which powers the notebooks we'll see later. </p>","tags":["python","beginner"]},{"location":"blog/run-python/#python-script","title":"Python Script","text":"<p><code>python script_name.py</code> in a terminal.</p> <p>Most common way to \"run a Python script.\" Many programmers will have their first script be something like <code>hello.py</code> with a line of code <code>print(\"hello world\")</code>. To execute that script, the <code>python</code> command has to be used (or use a play button in your code editor which calls <code>python</code> in the background for you)</p> <p>Anything included after the script name is passed into the Python execution as entries in the <code>sys.argv</code> list, which has the script name itself as the first element. <code>argv</code> is short for \"argument values\", which means they're usually used as the parameters for your script.</p> <p>Save the following as <code>my_script.py</code> Python<pre><code>import sys\nprint(f\"Command line args: {sys.argv}\")\n</code></pre></p> <p>Then run the following in a terminal window with <code>my_script.py</code> in the current directory: Bash<pre><code>python my_script.py test args to be printed in sys.argv\n</code></pre></p> <p>Python will print any <code>print()</code> statements to the terminal window unless otherwise specified (see the \"file\" param in the print docs).</p> <p>If your program gets stuck in an infinite loop or you need to interrupt it, hitting <code>ctrl+c</code> one or more times should give you back control of the command line.</p> <p>Unlike the REPL, this is a valid way to run Python code in Production. It doesn't require any user interaction so it can be included in a bash / batch / ci/cd script.</p> <p><code>python -i script_name.py</code> combines runs the code in the file, then allows you to continue entering python code in the REPL. This can be useful for debugging, which can be done similarly with <code>breakpoint()</code> at the end of the script (which will require some understanding of pdb)</p>","tags":["python","beginner"]},{"location":"blog/run-python/#upgrading-your-script-runs","title":"Upgrading your script runs","text":"<p>In most cases you should prefer the built-in <code>argparse</code> and <code>logging</code> modules in order to handle command line arguments and output messages from your script.</p> <p>If you're willing to use packages from outside of the standard library you can check out Click, the incredibly popular CLI library. For a project specifically focused on the CLI, Typer builds from Click with even more ~magic~ (sensible defaults and type-hinting)</p> <p>As for logging, popular packages such as structlog (with enhancements via rich) and loguru will help you format your logs in a way that's easy to read for you when you're developing and easy to parse by machines if you run your code in production.</p>","tags":["python","beginner"]},{"location":"blog/run-python/#jupyter-notebook","title":"Jupyter Notebook","text":"<p><code>python -m jupyter notebook</code> runs a code notebook server locally on your computer.</p> <p>This lets you code in a Jupyter notebook in your browser. Code notebooks are a non-traditional way of coding, but are quite popular in education, data science, and research. Each notebook is made up of \"cells\" of either code or some documentation text (usually in Markdown).</p> <p>Jupyter notebooks (and this post) are focused on Python code cells, but it's good to know some other languages allow this kind of interaction in various code editors and online tools.</p> <p>The main benefit of notebooks is having a \"live\" coding environment. You are able to run chunks of your code, observe the results, then tweak something and re-run just the last chunk.</p> <p>Notebooks are also great for incorporating human readable documentation with your code and have some useful integrations with Image handling libraries and some charting and other data libraries. Some of the most notable: <code>matplotlib</code>, <code>Pillow</code>, and <code>opencv</code></p> <p>If you didn't notice at the top, this post itself was generated from a notebook. You can check out the notebook on github or make a copy of it and run the following Python code on your own. Then make some changes with your own data and run it again.</p> Python<pre><code>from datetime import datetime, date\nBIRTH_YEAR = 1996\n\ndef get_min_days_old(birth_year):\n    difference = date.today() - date(birth_year, 12, 31)\n    return difference.days\n\nmin_days_old = get_min_days_old(BIRTH_YEAR)\nmin_days_old\n</code></pre> Text Only<pre><code>9163\n</code></pre>","tags":["python","beginner"]},{"location":"blog/run-python/#upgrading-your-notebooks","title":"Upgrading your Notebooks","text":"<p>Many complaints about notebooks involve not being able to edit them like a normal text file, not being able to do normal Python library development, and difficulty testing.</p> <p>Libraries such as nbdev, fastpages (What this site is built with), and fastdoc from fast.ai address these gripes and more.</p> <p>Code editors such as VS Code and Atom have ways of breaking your <code>.py</code> file up into runnable Jupyter cells with some magic comments. VS Code calls this the Python Interactive Window. This method of using Jupyter allows more natural text-editing.</p>","tags":["python","beginner"]},{"location":"blog/streamlit-fullstack/","title":"Streamlit Full Stack Part 1: Python Web App in One File","text":"<p>A Frontend, Backend, and Data Store in one Python file!</p> <p>Create, Read, Update, and Delete from a feed of 140 character markdown notes!</p> <p>ex: </p>","tags":["streamlit","python","beginner"]},{"location":"blog/streamlit-fullstack/#the-littlest-full-stack-app","title":"The Littlest Full Stack App","text":"<p>The idea for this was starting with built-in sqlite module and streamlit to build a full stack application in a single Python file:</p> <ul> <li>Streamlit</li> <li>Frontend</li> <li>Python w/ SQLite3</li> <li>Backend</li> <li>SQLite</li> <li>Data Store</li> </ul> <p>Obviously this takes some liberties with the definition of \"Full Stack App\", for my purposes I take it to mean \"a web application with a frontend that receives data from some a backend and that data is persisted in some data store\"</p> <p>For the first swing at this I also went with classic CRUD Operations that drive requirements for so many Full-Stack applications:</p> <ul> <li>Create</li> <li>Read</li> <li>Update</li> <li>Delete</li> </ul>","tags":["streamlit","python","beginner"]},{"location":"blog/streamlit-fullstack/#data-store","title":"Data Store","text":"<p>Using SQLite is straightforward if you understand how to set up and query other SQL flavors. (For a more in depth guide on SQL databases, I'd recommend the SQLModel docs and Intro to Databases for a more beginner friednly intro than reading SQLite documentation...)</p> <p>I say it's straightforward because we don't need to download or run any external database server. SQLite is a C library that works within an application to store data to disk or memory without an external server. It will let us connect with a database using just two lines of python!</p> Python<pre><code>import sqlite3\nconnection = sqlite3.connect(':memory:')\n</code></pre> <p>This gets us a <code>Connection</code> object for interacting with an in-memory SQL database!</p> <p>For the purposes of using it as a more persistant store, it can be configured to write to a local file (conventionally ending with <code>.db</code>).</p> <p>It also defaults to only being accessible by a single thread, so we'll need to turn this off for multiple users hitting the application in the same thread.</p> Python<pre><code>connection = sqlite3.connect('notes.db', check_same_thread=False)\n</code></pre>","tags":["streamlit","python","beginner"]},{"location":"blog/streamlit-fullstack/#backend","title":"Backend","text":"<p>This is all in one file, but the idea of a \"Service\" that provides reading and writing to the data store is useful to keep track of our goals. This can be captured in a class as a namespace, using staticmethods to indicate they don't need an instance:</p> Python<pre><code>class NoteService:\n    \"\"\"Namespace for Database Related Note Operations\"\"\"\n\n    @staticmethod        \n    def list_all_notes(\n        connection: sqlite3.Connection,\n    ) -&gt; List[sqlite3.Row]:\n        \"\"\"Returns rows from all notes. Ordered in reverse creation order\"\"\"\n        read_notes_query = f\"\"\"SELECT rowid, created_timestamp, updated_timestamp, username, body\n        FROM notes ORDER BY rowid DESC;\"\"\"\n        note_rows = execute_query(connection, read_notes_query)\n        return note_rows\n    @staticmethod    \n    def create_note(connection: sqlite3.Connection, note: BaseNote) -&gt; None:\n        \"\"\"Create a Note in the database\"\"\"\n        create_note_query = f\"\"\"INSERT into notes(created_timestamp, updated_timestamp, username, body)\n    VALUES(:created_timestamp, :updated_timestamp, :username, :body);\"\"\"\n        execute_query(connection, create_note_query, asdict(note))\n    @staticmethod\n    def update_note(connection: sqlite3.Connection, note: Note) -&gt; None:\n        \"\"\"Replace a Note in the database\"\"\"\n        update_note_query = f\"\"\"UPDATE notes SET updated_timestamp=:updated_timestamp, username=:username, body=:body WHERE rowid=:rowid;\"\"\"\n        execute_query(connection, update_note_query, asdict(note))\n    @staticmethod\n    def delete_note(connection: sqlite3.Connection, note: Note) -&gt; None:\n        \"\"\"Delete a Note in the database\"\"\"\n        delete_note_query = f\"\"\"DELETE from notes WHERE rowid = :rowid;\"\"\"\n        execute_query(connection, delete_note_query, {\"rowid\": note.rowid})\n</code></pre> <p>NOTE: You could definitely initialize the class with a connection and not pass it to functions. Or in another module. Or both!</p>","tags":["streamlit","python","beginner"]},{"location":"blog/streamlit-fullstack/#frontend","title":"Frontend","text":"<p>Streamlit provides all the frontend components we need, supported underneath by React components such as those from Base Web / UI.</p> <p>I chose to use a Selectbox in the Sidebar to act as page navigation. This organizes things similarly to other Streamlit multi page examples (Such as this awesome US Data Explorer from arup).</p> <p>The main entrypoint looks like this:</p> Python<pre><code>def main() -&gt; None:\n    \"\"\"Main Streamlit App Entry\"\"\"\n    connection = get_connection(DATABASE_URI)\n    init_db(connection)\n\n    st.header(f\"The Littlest Fullstack App!\")\n    render_sidebar(connection)\n\n\ndef render_sidebar(connection: sqlite3.Connection) -&gt; None:\n    \"\"\"Provides Selectbox Drop Down for which view to render\"\"\"\n    views = {\n        \"Read Note Feed\": render_read,  # Read first for display default\n        \"Create a Note\": render_create,\n        \"Update a Note\": render_update,\n        \"Delete a Note\": render_delete,\n        \"About\": render_about,\n    }\n    choice = st.sidebar.selectbox(\"Menu\", views.keys())\n    render_func = views.get(choice)\n    render_func(connection)\n</code></pre> <p>Each of those <code>render_xyz</code> functions will use <code>st.</code> functions to display in the main body of the page when it is chosen in the SelectBox / drop down.</p> <p>This is the <code>render_read</code> for example:</p> Python<pre><code>def render_note(note: Note) -&gt; None:\n    \"\"\"Show a note with streamlit display functions\"\"\"\n    st.subheader(f\"By {note.username} at {display_timestamp(note.created_timestamp)}\")\n    st.caption(\n        f\"Note #{note.rowid} -- Updated at {display_timestamp(note.updated_timestamp)}\"\n    )\n    st.write(note.body)\n\n\ndef render_read(connection: sqlite3.Connection) -&gt; None:\n    \"\"\"Show all of the notes in the database in a feed\"\"\"\n    st.success(\"Reading Note Feed\")\n    note_rows = NoteService.list_all_notes(connection)\n    with st.expander(\"Raw Note Table Data\"):\n        st.table(note_rows)\n\n    notes = [Note(**row) for row in note_rows]\n    for note in notes:\n        render_note(note)\n</code></pre> <p>For more on the forms for Creating, Updating and Deleting, check out the source code on github.</p>","tags":["streamlit","python","beginner"]},{"location":"blog/streamlit-fullstack/#gluing-it-all-together","title":"Gluing It All Together","text":"<ul> <li>SQLite can run with the Python process, so we're good to deploy it wherever the Streamlit app runs</li> <li>Frontend and Backend are in one server, so there's no HTTP or Websocket communication going between App services</li> </ul> <p>Python <code>dataclasses.dataclass</code> provides a nice way of modeling simple entities like this Note example. It lacks all of the features of <code>pydantic</code> and <code>attrs</code>, but it does give us a free <code>__init__</code> with typed kwargs and the <code>dataclasses.asdict</code> method.</p> <p>After the rows are read from the database, the data is passed into this <code>dataclass</code> Note model. The model provides some level of validation on the data types and a Python object with known attributes for type-hinting and checking.</p> Python<pre><code>@dataclass\nclass BaseNote:\n    \"\"\"Note Entity for Creation / Handling without database ID\"\"\"\n\n    created_timestamp: int\n    updated_timestamp: int\n    username: str\n    body: str\n\n\n@dataclass\nclass Note(BaseNote):\n    \"\"\"Note Entity to model database entry\"\"\"\n\n    rowid: int\n</code></pre>","tags":["streamlit","python","beginner"]},{"location":"blog/streamlit-fullstack/#conclusion","title":"Conclusion","text":"<p>That's the basic app!</p> <p>It's not perfect, in fact there's no error handling or testing or linting of any sort.</p> <p>Also building things in one file is generally unsustainable, but I wanted to push the limits on this. One of the great python web frameworks, bottle, is a single (~4000 line) file!</p> <p>But those sorts of things can be addressed after getting something working. In the next post we'll replace SQLite with Postgres as the Data Store layer. When doing that, it's my preference to start using Docker, specifically the Docker-Compose tool. It makes sure things are easy to setup, run, then destroy without worrying about installing things to my user / filesystem directly.</p>","tags":["streamlit","python","beginner"]},{"location":"blog/streamlit-url-scan/","title":"Scanning URLs from Images","text":"<p>QR codes for humans: URLs</p> <p>The idea for this popped up when talking to my mom about how QR codes work. The simple usecase for QR codes is the same as a URL: get the user to a certain website.</p> <p>So why can't we just scan a real URL with our cameras...? (spoiler: many modern android and ios devices can, but that's a short end to the story)</p> <p>I'm all for QR codes being used for more things, but URLs aren't going away any time soon for a few reasons: - Not everybody knows how to make a QR code (hint: write click on a page in Chrome or use the python-qrcode library) - URLs often contain the company name so people can remember them (qr codes can contain logos in some cases) - URLs are a single line (most of the time) rather than taking up a whole square of printing / advertising space</p> <p>I also wanted to give a live Streamlit app a shot after using it for some AI demos with students. So was born the Streamlit URL Scanner!</p> <p>In this notebook I'll breakdown the basic mechanisms behind the app, see the full source code on github</p>"},{"location":"blog/streamlit-url-scan/#python-dependencies","title":"Python Dependencies","text":"<p>There are 3 goals for this app: - Allow user to upload images containing URLs to the web app - Run high-accuracy Optical Character Recognition (OCR) on the uploaded image - Provide any URLs from the extracted text to the user for easy clicking</p> <p>I went with the libraries that I thought would give the fastest and most successful development experience.</p>"},{"location":"blog/streamlit-url-scan/#python-web-app","title":"Python Web App","text":"<ul> <li><code>streamlit</code>: Python rapid development web app framework <ul> <li>Provides a file upload component out of the box with <code>st.file_uploader</code></li> <li>Simple Cloud deployment with secure secrets for OCR component</li> </ul> </li> </ul> <p>Contenders - <code>fastapi</code>: Providing a route to OCR component as a service      - Asynchronous by default is nice for handling distributed transactions to OCR task - <code>django</code>: Overbaked for usecase     - This is a proof of concept tool, not a full-stack user-oriented website - <code>flask</code> / <code>bottle</code>:      - Have ways of being asynchronous, but synchronous by default</p> <p>All of these other options would require a frontend app or integrating some JS library or other <code>index.html</code> + JS + CSS combination</p>"},{"location":"blog/streamlit-url-scan/#ocr","title":"OCR","text":"<ul> <li>AWS Rekognition: Trained for text detection on real world images<ul> <li>Limited to 100 words detected</li> <li>Accessed with the <code>boto3</code> library</li> </ul> </li> </ul> <p>Contenders - AWS Textract: More tuned for documents than real world - Tesseract: Still a good OCR, but also focused on documents     - Can be self-hosted, not paid per transaction</p>"},{"location":"blog/streamlit-url-scan/#url-extraction","title":"URL Extraction","text":"<ul> <li><code>urlextract</code>: I didn't want to write a regex for URLs when there's a small library without any known issues</li> </ul>"},{"location":"blog/streamlit-url-scan/#glue-code","title":"Glue Code","text":"<ul> <li><code>PIL</code> / Pillow: Python Imaging Library for handling user uploaded images and resizing if needed</li> <li><code>pydantic</code>: Typed Settings management with loading from CLI, Environment, Secrets</li> </ul> Python<pre><code>import io\nimport json\n\nimport boto3\nimport streamlit as st\nfrom PIL import Image, ImageDraw, ImageOps\nfrom pydantic import BaseSettings\nfrom urlextract import URLExtract\n</code></pre>"},{"location":"blog/streamlit-url-scan/#ocr-extractor-setup","title":"OCR + Extractor Setup","text":"<p>Pydantic's <code>BaseSettings</code> Class allows us to read in settings for connecting to AWS account. This can be used with Docker secrets, but this app is deployed to Streamlit cloud.</p> <p><code>boto3</code> lets us establish a client to the Rekognition service.</p> <p>URL Extract requires some initialization to recognize all domain names. </p> Python<pre><code>class Settings(BaseSettings):\n    \"\"\"Handles fetching configuration from environment variables and secrets.\n    Type-hinting for config as a bonus\"\"\"\n\n    aws_access_key_id: str\n    aws_secret_access_key: str\n    aws_region: str\n\nsettings = Settings()\n\nrekog_client = boto3.client(\n    \"rekognition\",\n    region_name=settings.aws_region,\n    aws_access_key_id=settings.aws_access_key_id,\n    aws_secret_access_key=settings.aws_secret_access_key,\n)\n\nextractor = URLExtract()\n</code></pre>"},{"location":"blog/streamlit-url-scan/#detecting-text-in-an-image","title":"Detecting Text in an Image","text":"<p>AWS Rekognition can receive either a path to an S3 object or raw image bytes. For this app I went with passing just the image bytes, so a helper function to compress larger images was needed. We'll ignore the streamlit specific alert message that this is happening for this demo. (The S3 version isn't much more complicated, and is beneficial for more general OCR apps)</p> <p>Another small helper for passing the correct parameters to boto3 will wrap up this section.</p> <p><code>Pillow</code> will do our image handling in the app, so we'll use it for showing a demo detection in the following code cells </p> Python<pre><code>def compress_pil_image(image: Image, limit=(5 * (2 ** 20))) -&gt; bytes:\n    \"\"\"Takes a Pillow image and returns byte values of the image saved as png.\n    Reduces dimensions of image if it is larger than provided limit.\n\n    Args:\n        image (Image): Image to get the bytes for\n        limit (int, optional): Maximum number of bytes. Defaults to 5mb (5 * (2 ** 20)).\n\n    Returns:\n        bytes: image saved as PNG bytes object\n    \"\"\"\n    image_bytes = io.BytesIO()\n    image.save(image_bytes, \"PNG\")\n    output = image_bytes.getvalue()\n\n    limit_to_bytes_ratio = limit / len(output)\n\n    if limit_to_bytes_ratio &gt;= 1.0:\n        return output\n    else:\n        # st.warning(f\"Resizing by ratio: {limit_to_bytes_ratio}\")\n        width, height = image.size\n        new_width = int(width * limit_to_bytes_ratio)\n        new_height = int(height * limit_to_bytes_ratio)\n        new_image = image.resize((new_width, new_height), Image.ANTIALIAS)\n        return compress_pil_image(new_image, limit)\n\n\ndef rekog_detect_by_bytes(image_bytes: bytes) -&gt; dict:\n    \"\"\"Takes an array of bytes representing jpg / png image.\n    Tries to return response from AWS Rekognition detect_text API on the image bytes\n    See docs for more: https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/rekognition.html#Rekognition.Client.detect_text  # noqa: E501\n\n    Args:\n        image_bytes (bytes): Image to run detection on (less than 5 mb)\n\n    Returns:\n        dict: List of text detections, geometry of the detections, and metadata\n    \"\"\"\n    response = rekog_client.detect_text(Image={\"Bytes\": image_bytes})\n    return response\n</code></pre> Python<pre><code>demo_image = Image.open('test_images/sample_urls.jpg')\ndemo_image\n</code></pre>  Python<pre><code>image_bytes = compress_pil_image(demo_image)\nrekognotion_response = rekog_detect_by_bytes(image_bytes)\nrekognotion_response\n</code></pre> Text Only<pre><code>{'TextDetections': [{'DetectedText': 'Lorem ipsum...',\n   'Type': 'LINE',\n   'Id': 0,\n   'Confidence': 83.6632080078125,\n   'Geometry': {'BoundingBox': {'Width': 0.198333278298378,\n     'Height': 0.05164804682135582,\n     'Left': 0.4017779529094696,\n     'Top': 0.3004961609840393},\n    'Polygon': [{'X': 0.4021288752555847, 'Y': 0.3004961609840393},\n     {'X': 0.6001112461090088, 'Y': 0.3018783628940582},\n     {'X': 0.5997602939605713, 'Y': 0.3521442115306854},\n     {'X': 0.4017779529094696, 'Y': 0.3507620096206665}]}},\n  {'DetectedText': 'google.com',\n   'Type': 'LINE',\n   'Id': 1,\n   'Confidence': 99.61248779296875,\n   'Geometry': {'BoundingBox': {'Width': 0.142822265625,\n     'Height': 0.0458984375,\n     'Left': 0.428466796875,\n     'Top': 0.3544921875},\n    'Polygon': [{'X': 0.428466796875, 'Y': 0.3544921875},\n     {'X': 0.5712890625, 'Y': 0.3544921875},\n     {'X': 0.5712890625, 'Y': 0.400390625},\n     {'X': 0.428466796875, 'Y': 0.400390625}]}},\n  {'DetectedText': 'https://streamlit.io/',\n   'Type': 'LINE',\n   'Id': 2,\n   'Confidence': 97.68013763427734,\n   'Geometry': {'BoundingBox': {'Width': 0.2527962923049927,\n     'Height': 0.051752522587776184,\n     'Left': 0.37402498722076416,\n     'Top': 0.45518985390663147},\n    'Polygon': [{'X': 0.37402498722076416, 'Y': 0.455630898475647},\n     {'X': 0.6267316937446594, 'Y': 0.45518985390663147},\n     {'X': 0.6268212795257568, 'Y': 0.506501317024231},\n     {'X': 0.3741145431995392, 'Y': 0.5069423913955688}]}},\n  {'DetectedText': 'Lorem',\n   'Type': 'WORD',\n   'Id': 3,\n   'ParentId': 0,\n   'Confidence': 99.94923400878906,\n   'Geometry': {'BoundingBox': {'Width': 0.07525634765625,\n     'Height': 0.0341796875,\n     'Left': 0.402099609375,\n     'Top': 0.3046875},\n    'Polygon': [{'X': 0.402099609375, 'Y': 0.3046875},\n     {'X': 0.47735595703125, 'Y': 0.3046875},\n     {'X': 0.47735595703125, 'Y': 0.3388671875},\n     {'X': 0.402099609375, 'Y': 0.3388671875}]}},\n  {'DetectedText': 'ipsum...',\n   'Type': 'WORD',\n   'Id': 4,\n   'ParentId': 0,\n   'Confidence': 67.37718200683594,\n   'Geometry': {'BoundingBox': {'Width': 0.11253990232944489,\n     'Height': 0.05104871839284897,\n     'Left': 0.48755744099617004,\n     'Top': 0.30109521746635437},\n    'Polygon': [{'X': 0.4879346489906311, 'Y': 0.30109521746635437},\n     {'X': 0.6000973582267761, 'Y': 0.30386465787887573},\n     {'X': 0.5997201800346375, 'Y': 0.35214391350746155},\n     {'X': 0.48755744099617004, 'Y': 0.3493745028972626}]}},\n  {'DetectedText': 'google.com',\n   'Type': 'WORD',\n   'Id': 5,\n   'ParentId': 1,\n   'Confidence': 99.61248779296875,\n   'Geometry': {'BoundingBox': {'Width': 0.142822265625,\n     'Height': 0.0458984375,\n     'Left': 0.428466796875,\n     'Top': 0.3544921875},\n    'Polygon': [{'X': 0.428466796875, 'Y': 0.3544921875},\n     {'X': 0.5712890625, 'Y': 0.3544921875},\n     {'X': 0.5712890625, 'Y': 0.400390625},\n     {'X': 0.428466796875, 'Y': 0.400390625}]}},\n  {'DetectedText': 'https://streamlit.io/',\n   'Type': 'WORD',\n   'Id': 6,\n   'ParentId': 2,\n   'Confidence': 97.68013763427734,\n   'Geometry': {'BoundingBox': {'Width': 0.2527649700641632,\n     'Height': 0.051752522587776184,\n     'Left': 0.3740406334400177,\n     'Top': 0.45518985390663147},\n    'Polygon': [{'X': 0.3740406334400177, 'Y': 0.4563566744327545},\n     {'X': 0.6267316937446594, 'Y': 0.45518985390663147},\n     {'X': 0.6268056035041809, 'Y': 0.5057755708694458},\n     {'X': 0.3741145431995392, 'Y': 0.5069423913955688}]}}],\n 'TextModelVersion': '3.0',\n 'ResponseMetadata': {'RequestId': '6ee34f23-945f-45ea-9fe4-439580da7ff2',\n  'HTTPStatusCode': 200,\n  'HTTPHeaders': {'x-amzn-requestid': '6ee34f23-945f-45ea-9fe4-439580da7ff2',\n   'content-type': 'application/x-amz-json-1.1',\n   'content-length': '2883',\n   'date': 'Fri, 28 Jan 2022 02:56:30 GMT'},\n  'RetryAttempts': 0}}\n</code></pre>"},{"location":"blog/streamlit-url-scan/#extracting-urls-from-text","title":"Extracting URLs from Text","text":"<p>If you're not familiar with APIs or bounding boxes the above output might be a bit of a mess. That's alright, we're here to work through it.</p> <p>Rekognition's text detection returns a List of \"Text Detection\" records. Each of these \"Text Detections\" has a few features, but the most important to our purpose is \"Detected Text.\"</p> <p>If we're really just interested in the text, we can use a list comprehension to get the detections and pass them to the URL extractor</p> Python<pre><code>detected_text = [detection['DetectedText'] for detection in rekognotion_response['TextDetections'] if detection[\"Type\"] == \"LINE\"]\nextracted_urls = extractor.find_urls(\" \".join(detected_text))\nextracted_urls\n</code></pre> Text Only<pre><code>['google.com', 'https://streamlit.io/']\n</code></pre>"},{"location":"blog/streamlit-url-scan/#streamlit-aspect","title":"Streamlit aspect","text":"<p>Streamlit provides the frontend components for uploading and viewing images and links. (And giving a semblance of user experience)</p> <p>It's hard to demo these aspects in a notebook, but here are the streamlit snippets and use cases in the app.</p> Python<pre><code># Header and Description \nst.title(\"URL Scan :computer:\")\nst.header(\n    \"Never type a URL from real life again! \"\n    \"Take a picture with a URL in it and we'll scan any links so you can click them!\"\n)\nst.subheader(\"(Or upload an image you already have on your device)\")\n\n# Retrieve image from camera or upload\ncamera_bytes = st.camera_input(\"Take a picture\")\nuploaded_bytes = st.file_uploader(\n    \"Upload an image\",\n    type=[\"png\", \"jpg\", \"jpeg\"],\n)\n\n# Context manager to give better loading experience\nwith st.spinner(\"Loading Image Bytes\"):\n    # Compress pil image\n    pass\n\n# Provide visual alerts to the user\nst.success(\n    f\"Found {len(extracted_urls)} URLs!\"\n)\n\n# Allow downloading the detected text / urls\nst.download_button(\n    label=\"Download extracted url list\",\n    data='\\n'.join(extracted_urls),\n    file_name=\"extracted_urls.txt\",\n    mime=\"text\",\n)\n\n# Display the raw and detected images\nst.image(\n    demo_image,\n    use_column_width=True,\n)\n</code></pre>"},{"location":"blog/streamlit-url-scan/#testing-and-deployment","title":"Testing and deployment","text":"<p>Docker is used to help smooth environments between windows / linux / mac. Docker-compose is used to open up to future extensions with other backend apps.</p> <p>Linting, Static Checking, and Testing are handled locally before deployment.</p> <p>E2E testing consists of Selenium visual baseline testing against locally deployed app in docker-compose</p> <p>Deployment would be straightforward with docker-compose, but Streamlit cloud provides plenty of resources for this use case. Not having to write a CI/CD pipeline is fine by me.</p> <p>Downsides are the need to deploy secrets manually to streamlit and it requires setting up a seperate app deployment if a staging / UAT environment is desired.</p>"},{"location":"blog/streamlit-url-scan/#bonus-painting-detections","title":"BONUS: Painting detections","text":"<p>The url extraction code used above isn't the same process as used in the app. I think the bounding box aspect of text detection is engaging for users to understand the OCR component, so I include a copy of their image with the bounding boxes painted on.</p> <p>We get all of the location data in the \"Text Detections\" from Rekognition, but we have to do a bit of conversion from their format to draw them with Pillow's ImageDraw. In this case we're converting from a format that provides the Width, Height, Left-side X coordinate, and Top-side Y coordinate in percentage of the image size.</p> <p>Our goal is to use some arithmetic to get the (X,Y) coordinates of the top-left corner of our bounding box and the bottom-right corner in pixels. (If you haven't worked with bounding boxes, there's even more possible formats...)</p> Python<pre><code>image_w, image_h = demo_image.size\npainted_image = demo_image.copy()\ncanvas = ImageDraw.Draw(painted_image)\nfor detection in rekognotion_response[\"TextDetections\"]:\n    if detection[\"Type\"] == \"LINE\":\n        text = detection[\"DetectedText\"]\n        aws_bbox = detection[\"Geometry\"][\"BoundingBox\"]\n        top_left_x = aws_bbox[\"Left\"] * image_w\n        top_left_y = aws_bbox[\"Top\"] * image_h\n        box_width = aws_bbox[\"Width\"] * image_w\n        box_height = aws_bbox[\"Height\"] * image_h\n        bot_right_x = top_left_x + box_width\n        bot_right_y = top_left_y + box_height\n        canvas.rectangle(\n            (top_left_x, top_left_y, bot_right_x, bot_right_y),\n            outline=\"Red\",\n            width=3,\n        )\npainted_image\n</code></pre>"},{"location":"blog/terminal/","title":"The Terminal (/ Shell / Command Line / Console)","text":"","tags":["beginner"]},{"location":"blog/terminal/#whats-a-terminal","title":"What's a Terminal","text":"<p>The Terminal, which you might also hear called shell, console, or command line (with a fair bit of nuance) is a program that lets you enter commands for the computer to run. In different computers the program will vary (Terminal, Powershell, bash, etc.), but the idea of interacting with the computer using text commands one line at a time is present in all of them.</p>","tags":["beginner"]},{"location":"blog/terminal/#why-keyboard-over-mouse","title":"Why Keyboard Over Mouse?","text":"<p>So the Terminal lets you access apps and files and execute programs without using a mouse or other windows, but why would you want to abandon your mouse? All of the work we do in a Terminal is both plain text and can be automated much more easily than mouse movements and clicks.</p> <p>Using a shell also usually gives you access to the Kernel of your OS, the software in control of basically all your applications, so you can take back your technological freedom.</p> <p>Finally, Graphical (GUI) interfaces aren't available for all programs, especially a lot of the older software. It's generally easier to develop a program that takes a few Command Line arguments than a full blown GUI Application.</p>","tags":["beginner"]},{"location":"blog/terminal/#3-navigation-commands","title":"3 Navigation Commands","text":"<p>When you open Terminal.app (on many linux distros use <code>super+spacebar</code>, on mac you use <code>command+spacebar</code> then search <code>Terminal</code>) all you get is a blank command line prompt, probably ending with <code>$</code></p> <p>The terminal works like a filesystem/Finder window in that you need to navigate 'up' and 'down' into different folders to find particular files. When you open a new Terminal it is most likely located at your account's Home folder (also referred to as <code>~</code>).</p> <p>NOTE: On Mac the <code>~</code> can be replaced with <code>/Users/YOUR-USERNAME/</code> for an \"absolute\" path (absolute vs relative explanation). Ubuntu based distros <code>~</code> == <code>/home/YOUR-USERNAME/</code>. And Windows is different... <code>~</code> == <code>C:\\Users\\YOUR-USERNAME</code>. For reference, your Desktop folder is located at ~/Desktop, your Documents at ~/Documents</p> <p>Now for the 3 Basic commands that will help you navigate around (type these in the command line and hit <code>enter</code>):</p>","tags":["beginner"]},{"location":"blog/terminal/#pwd","title":"PWD","text":"Bash<pre><code>pwd\n</code></pre> <p><code>Print Working Directory</code>: Tells you where in the filesystem the command line is currently pointed. On a fresh Terminal window it should show <code>/Users/YOUR-USERNAME/</code></p>","tags":["beginner"]},{"location":"blog/terminal/#ls","title":"LS","text":"Bash<pre><code>ls\n</code></pre> <p><code>List</code>: Lists out all the files and directories in the current directory you're pointed at. Also helps you know where in the filesystem you are and what files you have easy access to.</p>","tags":["beginner"]},{"location":"blog/terminal/#cd","title":"CD","text":"Bash<pre><code>cd\n</code></pre> <p><code>Change Directory</code>: Actually moves where the command line is pointed to a different directory / folder.</p> <p>The common uses of <code>cd</code> and <code>ls</code>:</p>","tags":["beginner"]},{"location":"blog/terminal/#go-to-your-project-folder","title":"Go to your project folder","text":"<p>On my personal computer I try to keep all my coding projects under a folder called <code>research</code> (in their own individual folders) which is in my Home folder (<code>~/research</code> or <code>/Users/Gerard/research</code>)</p> <p>So to get to my project I open terminal and enter</p> Bash<pre><code>cd research/my-project-folder\n</code></pre> <p>NOTE: This works because the Terminal is already pointed at my <code>~/</code> directory and <code>research</code> is in that directory. You can use <code>ls</code> to see if <code>research</code> is present in your Home directory</p> <p>If you don't remember the project-folder name you can do the following</p> Bash<pre><code>cd research\nls\n</code></pre> <p>This will show you all the files and folders in <code>research</code>, then you can <code>cd</code> directly into it without the <code>research/</code> part</p> Bash<pre><code>cd project-folder-i-remember-now\n</code></pre>","tags":["beginner"]},{"location":"blog/terminal/#go-back-one-or-more-directories","title":"Go back one or more directories","text":"<p>Just like <code>~/</code> is a shorthand symbol for \"Home Directory\", <code>./</code> is a symbol for the current working directory (<code>pwd</code>)</p> <p>By this I mean that <code>.</code> represents the current working directory, where the terminal command line is pointed.</p> <p>So the same command from before works the same like this (from a fresh Terminal located at <code>~/</code>)</p> Bash<pre><code>cd ./research/my-project-folder\n</code></pre> <p>After executing that command, <code>pwd</code> will tell you the Terminal is at <code>/Users/Gerard/research/my-project-folder</code>, which we want because we just <code>cd</code>d into that directory</p> <p>If we wanted to switch projects (to a different folder in <code>research</code>), we need to go 'up' a folder. To do this we use <code>..</code> to represent the folder 'above' the current folder</p> Bash<pre><code>cd ..\n</code></pre> <p>This brings us back to <code>research</code>, so <code>pwd</code> will say <code>/Users/Gerard/research/</code></p> <p>Now we can cd into a different folder</p> Bash<pre><code>cd my-other-project-folder\n</code></pre> <p>If we wanted to switch to a different project directly in one command we can use</p> Bash<pre><code>cd ../third-folder\n</code></pre> <p>I think of this as 'going to third-folder, which is in the folder above the current one'</p>","tags":["beginner"]},{"location":"blog/terminal/#going-further","title":"Going Further","text":"<p><code>ls</code> and <code>cd</code> should get you far enough to run Python scripts (also using <code>python</code> as a command!).</p> <p>Making your own or finding a terminal commands cheatsheet online can be extremely helpful when first starting out. Repetition is key to becoming comfortable with and memorizing these things. Nobody memorizes them after the first use!</p>","tags":["beginner"]},{"location":"blog/timeseries-part-1/","title":"Time Series Data Part 1: What is a Time Series?","text":"","tags":["python","timeseries","beginner"]},{"location":"blog/timeseries-part-1/#time-series-data-overview","title":"Time Series Data Overview","text":"<p>We live in an age of data that we don't know how to sort through. It comes in many forms; there are pictures and purchases and google searches and blog posts floating around hard drives all over the world.</p> <p>The focus of this will be on \"Time Series\" data; any data that is ordered by the passage of fixed periods of time. In other words: \"Data you keep track of every X number of minutes (or weeks, or months!)\"</p> <p>Some real-world Time Series data examples:</p> <ul> <li>Weather history</li> <li>The average temperature of each day</li> <li>Energy Usage</li> <li>The average amount of electricity used by a city each day</li> <li>Stock price history</li> <li>Opening / Closing price of each day</li> <li>Purchase history</li> <li>Average number of purchases in each month</li> <li>Field sensor signals</li> <li>Average seismic readings in an area per hour</li> </ul> <p>Mathematically speaking, we have example \"observations\" at each \"period\" of time. This can be represented like a list of values going back in time:</p> <p><code>y_t</code>, <code>y_t-1</code>, <code>y_t-2</code>, ... <code>y_0</code></p> <p>But before getting too deep into Time Series, what isn't a Time Series?</p>","tags":["python","timeseries","beginner"]},{"location":"blog/timeseries-part-1/#what-isnt-a-time-series","title":"What isn't a Time Series?","text":"<p>When developers think of \"data\", many will rightfully jump to \"databases.\" Often used are relational SQL such as Postgres and MySQL. There are also many flavors of unstructured and columnar Non/NO-SQL databases.</p>","tags":["python","timeseries","beginner"]},{"location":"blog/timeseries-part-1/#relational-data","title":"Relational Data","text":"<p>Relational data fits many normal business use cases, like keeping track of your employee / user / student data. Here's some relational data with 3 columns that holds some information about our users:</p>    id name favorite_food     1 Alice Spam   2 Bob Eggs","tags":["python","timeseries","beginner"]},{"location":"blog/timeseries-part-1/#non-relational-data","title":"Non-Relational Data","text":"<p>Unstructured data fits many user-created use cases, like representing a blog post or user's settings.</p> <p>VS Code handles non-default user preferences by adding to a JSON document. Here are some of mine:</p> JSON<pre><code>{\n    \"python.testing.pytestEnabled\": true,\n    \"python.sortImports.path\": \"isort\",\n    \"python.sortImports.args\": [\n        \"--profile black\"\n    ],\n    \"window.zoomLevel\": 1,\n}\n</code></pre>","tags":["python","timeseries","beginner"]},{"location":"blog/timeseries-part-1/#whats-the-use-of-time-series","title":"What's The Use of Time Series","text":"","tags":["python","timeseries","beginner"]},{"location":"blog/timeseries-part-1/#time-series-forecasting","title":"Time Series Forecasting","text":"<p>One of the powers of Time Series data is the ability to predict the future! It's not possible in all cases of course, but that is the goal of Time Series \"forecasting.\"</p> <p>By analyzing the history and patterns of the data, a prediction can be made for one or more periods in the future.</p> <p>This is incredibly powerful and used every day in real business scenarios. For example: using a businesses' historical sales to predict if they'll make enough to payback a bigger loan.</p> <p>Mathematically this is represented as the result of a function on the present and previous observations. \"Y Hat\" is the predicted value of the next period.</p> <p><code>y_hat_t+1</code> = <code>g(t</code>, <code>y_t</code>, <code>y_t-1</code>, <code>y_t-2</code>, ... <code>y_0)</code></p> <p>In python we might write a function like the following:</p> <p><code>prediction = get_prediction(period, observations)</code></p> <p>Many Time Series forecasts can be massively improved by data that is external to the raw observations. These external data are also passed to the prediction function and are often called \"Covariates\".</p> <p>Quoting from the Python [Darts] library:</p>  <ul> <li> <p>past covariates are (by definition) covariates known only into the past (e.g. measurements)</p> </li> <li> <p>future covariates are (by definition) covariates known into the future (e.g., weather forecasts)</p> </li> <li> <p>static covariates are (by definition) covariates constant over time. They are not yet supported in Darts, but we are working on it!</p> </li> </ul>","tags":["python","timeseries","beginner"]},{"location":"blog/timeseries-part-1/#unique-features-of-time-series","title":"Unique Features of Time Series","text":"<ul> <li>Trend:</li> <li>Long term general direction</li> <li>Affects how we think about the data over longer periods</li> <li>Cyclicality</li> <li>Long horizon patterns of high-low-high movement</li> <li>Longer than years, which can usually fit in seasonality </li> <li>Seasonality</li> <li>Shorter periods of rising and falling movement</li> <li>Usually in sync with times of year</li> <li>Irregularity</li> <li>Random changes that don't fit the trend and cycle</li> </ul>","tags":["python","timeseries","beginner"]},{"location":"blog/timeseries-part-1/#unforecastable-data","title":"Unforecastable Data","text":"<p>These concepts just don't exist in Relational and Unstructured data. You couldn't predict anything about the future of our users' favorite foods just by looking at the table above.</p> <p>(Note: If we also kept track of when each person's preference changed, then we would be able to create some Time Series data from that!)</p>","tags":["python","timeseries","beginner"]},{"location":"blog/timeseries-part-2/","title":"Time Series Data Part 2: Darts and Streamlit","text":"<p>Streamlit + Darts Demo live</p> <p>I wanted to explore the claim of \"Time Series Made Easy in Python\" by the Darts library.</p> <p>I knew from their pydata talk that making something interactive around the training API would be straightforward.</p> <p>Adding interactive web elements with Streamlit to the Darts documentation example led to this quick demo project that lets you explore any univariate Timeseries CSV and make forecasts with Exponential Smoothing. This version will resample and sum values to get to monthly samples (or change to weekly / quarterly / etc); there are other Pandas resampling aggregation options though!</p> <p>See the app script source</p> <p>Free CSV Entry Direct Link</p>  <p>Next steps on this would be:</p> <ul> <li>Series / Data info and timeseries attributes</li> <li>Exposing configuration options for the model</li> <li>Adding other model options from Darts</li> <li>Backtest / Historical Forecast view</li> <li>Grid search result view</li> </ul>","tags":["time-series","darts","streamlit","python"]},{"location":"blog/timeseries-part-3/","title":"Time Series Data Part 3: Custom Model Tuning","text":"<p>tl;dr:</p> <p>Time Series Model Tuning App</p> <p>Github Repo</p>","tags":["streamlit","darts","python","timeseries","intermediate"]},{"location":"blog/timeseries-part-3/#how-to-train-your-model","title":"How to Train Your Model","text":"<p>In Machine Learning our goal is to predict things based on some input data and usually a whole lot of training data. The accuracy of the model's predictions are determined by a variety of factors:</p> <ul> <li>Architecture</li> <li>Training Data</li> <li>Hyperparameters</li> </ul> <p>Each of these affects your training results and model outputs in different ways. Testing  different Architectures, Datasets, and Hyperparameters is paramount to getting the best results possible.</p> <p>People call each phase different things but you'll often hear terms such as:</p> <ul> <li>Model Selection</li> <li>Data Cleaning / Augmentation</li> <li>Hyperparameter Tuning / Grid Search</li> </ul> <p>In this post we'll go over each of these with a primary focus on Hyperparameter Tuning.</p>","tags":["streamlit","darts","python","timeseries","intermediate"]},{"location":"blog/timeseries-part-3/#model-selection","title":"Model Selection","text":"<p>First up, there are many Machine Learning model \"architectures\" in the world. Some are basically as simple as an <code>if</code> statement (perceptrons). Some require more computing power than any normal user could afford.</p> <p>The architecture will determine the math the model computes in making a prediction out of an input. The linear combinations from input to output are defined in this way by the model architecture.</p> <p>Time Series Forecast models (those that predict future observations of a Time Series) have several restrictions for different types of training. The labels below are not mutually exclusive, but they do determine whether a given model can be trained on a given set of data.</p> <ul> <li>Univariate Capable Models</li> <li>Can train and predict on a Univariate (single value column) time series</li> <li>Ex. Temperature</li> <li>Multivariate</li> <li>Can train and predict on a Multivariate (multiple value columns) time series</li> <li>Ex. Temperature and Humidity</li> <li>Probabilistic</li> <li>Produces an output based on some random element. Non-deterministic predictions.</li> <li>All neural network based methods have Probabilistic output</li> <li>Multiple-series training</li> <li>Can train and predict on multiple (potentially unaligned) time series'</li> <li>Ex. Train on many different stock IPOs</li> <li>Past-observed covariates support</li> <li>Can train and predict utilizing an additional time series that provides known past data</li> <li>Ex. Humidity when you're only trying to predict Temperature</li> <li>Future-known covariates support</li> <li>Can train and predict utilizing an additional time series that provides known future data</li> <li>Ex. Day of the Week, Month of the Year, other forecasts</li> </ul>","tags":["streamlit","darts","python","timeseries","intermediate"]},{"location":"blog/timeseries-part-3/#tabular-models-in-time-series","title":"Tabular Models in Time Series","text":"<p>Research and industry have found time and again that tabular regression based models such as LightGBM, Random Forests, and XGBoost perform just as well if not better than models specifically architected for Time Series data.</p>","tags":["streamlit","darts","python","timeseries","intermediate"]},{"location":"blog/timeseries-part-3/#dataset-selection-and-cleaning","title":"Dataset Selection and Cleaning","text":"<p>When diving into a Machine Learning project the training dataset needs to mimic the real world data that will be used as input. Because of this, you'll likely choose your starting dataset before settling on a model.</p> <p>That's ok though, because there is more to a Dataset than meets the eye.</p> <p>Some common tasks that go along with handling Time Series datasets:</p> <ul> <li>Removing known outliers and other bad data points</li> <li>Merging Multiple Time Series into a Multivariate Time Series</li> <li>Adding Future or Past Covariates</li> <li>Splitting into Train, Validation, Test Data</li> <li>In Time Series this is often split at a certain date into Train and Validation</li> </ul>","tags":["streamlit","darts","python","timeseries","intermediate"]},{"location":"blog/timeseries-part-3/#hyperparameter-tuning","title":"Hyperparameter Tuning","text":"<p>Model / Training Hyperparameters are specific to each model and are decided before the training happens. In Time Series models they include things such as:</p> <ul> <li>Seasonality Period</li> <li>Additive vs Mutliplicative Seasonality</li> <li>Number of differentiations</li> <li>Size of moving windows (avg, max, min, etc.)</li> </ul> <p>Each unique combination of Hyperparameters that a particular model can have will yield different results.</p> <p>For this reason it's important to try different values for each. It also implies we can use certain default values as a \"baseline\" for each model.</p> <p>Comparing results from different every possible combination of Hyperparameters to the baseline results lets you pick the best performance that model could have had on the dataset. This implies improving performance means changing to a different model architecture or changing the dataset, which were the topics above!</p> <p>Usually choosing all of the combinations is not feasible. It takes a lot of computing power in some circumstances!</p> <p>Nevertheless, the process of picking a range of values to try for different Hyperparameters is commonly called \"Grid Searching.\" The range of values depends on the model, how much time / computing power you have, and what you know about the model behaviour on similar data.</p> <p>For example, for data that appears to have a yearly seasonal trend, it would be a waste of resources to include period values very far from one year in the grid search. The results of a model trained with those Hyperparameters far away from the real trend are very likely to yield bad results.</p> <p>(note: using those model results as a baseline or to understand the model better is totally fine if you have the time and computing resources!)</p>","tags":["streamlit","darts","python","timeseries","intermediate"]},{"location":"blog/timeseries-part-3/#model-data-and-hyperparameter-selection-with-darts-and-streamlit","title":"Model, Data, and Hyperparameter selection with Darts and Streamlit","text":"<p>Live on Streamlit cloud</p> <p>Github Repo</p> <p>A playground web app for the Darts API with Streamlit!</p> <p>Featuring:</p> <ul> <li>Example datasets</li> <li>Upload your own dataset</li> <li>Model training tuning</li> <li>Model forecasting and plotting controls</li> <li>Downloadable forecasts</li> </ul>","tags":["streamlit","darts","python","timeseries","intermediate"]},{"location":"blog/timeseries-part-3/#explore-a-time-series","title":"Explore A Time Series!","text":"<p>Use your own csv data that has a well formed time series and plot some forecasts!</p> <p>Or use one of the example Darts datasets</p>","tags":["streamlit","darts","python","timeseries","intermediate"]},{"location":"blog/timeseries-part-3/#explorable-models","title":"Explorable Models","text":"<ul> <li> NaiveDrift</li> <li> NaiveMean</li> <li> NaiveSeasonal</li> <li> ARIMA</li> <li> VARIMA (Requires Multivariate dataset)</li> <li> ExponentialSmoothing</li> <li> LinearRegressionModel (Hand set Lag)</li> <li> FFT</li> <li> Theta</li> <li> FourTheta</li> <li> KalmanForecaster</li> <li> LightGBMModel</li> <li> RandomForest (Hand set Lag)</li> <li> RegressionModel</li> </ul>","tags":["streamlit","darts","python","timeseries","intermediate"]},{"location":"blog/timeseries-part-4/","title":"Time Series Data Part 4: A Full Stack Use Case","text":"","tags":["time-series","darts","streamlit","python","intermediate"]},{"location":"blog/timeseries-part-4/#roommate-spending-ledger-visualization","title":"Roommate Spending Ledger Visualization","text":"<p>Using Pandas + Plotly + SQLite to show a full stack use case of Time Series Data.</p> <p>Analyze spending over time per person (could be adapted to categories / tags / etc).</p> <p>See: Github Repo</p>","tags":["time-series","darts","streamlit","python","intermediate"]},{"location":"blog/timeseries-part-4/#idea","title":"Idea","text":"<p>One time series that any financially independent person should pay attention to is their spending over time.</p> <p>Tracking your spending gets complicated with roommates and budgets for different categories. It also complicates your understanding of your data with a glance, which is where charts and graphs can help.</p> <p>There are many personal budgeting and even group budgeting apps, but I wanted to make the simplest possible and stick to the Data and Visualizations as an MVP.</p> <p>One way to get this data is from a CSV export from a bank account or credit card company. In Part 3 is an app that uses this method on general time series data. Upload a CSV with some column for time stamps and some column to forecast and tune your own prediction model!</p> <p>The main drawbacks to this paradigm:</p> <ul> <li>Can't share data between people / sessions</li> <li>Can't persist data</li> <li>Can't incrementally add data</li> <li>Can't update data</li> </ul> <p>Another way is the CRUD paradigm explored in my Streamlit Full Stack Post. With this method we'll be able to operate on individual data points and let our friends / roommates add to it!</p> <p>(Of course the CSV paradigm could be blended with this)</p> <p>Now for each aspect of the app, from Backend to Front</p>","tags":["time-series","darts","streamlit","python","intermediate"]},{"location":"blog/timeseries-part-4/#devops","title":"DevOps","text":"<p>There's not much real DataOps in this project since the data is self-contained.</p> <p>That said, there are some DevOps aspects that are important in the time series world:</p> <ul> <li>Deployment: having a webserver accessible to multiple users</li> <li>Integration: how to get updated code into the deployment(s)</li> </ul> <p>Leaning on Streamlit Cloud sharing checks both of these boxes with ease.</p> <p>By including a <code>requirements.txt</code> and specifying a python version for the image, we get a free CI/CD pipeline from any push to a github branch (more providers to come). It'll provide us with an Ubuntu-like container that installs all requirements and tries to perform <code>streamlit run streamlit_app.py</code>, yielding a live webserver accessible to the public for public repos!</p>","tags":["time-series","darts","streamlit","python","intermediate"]},{"location":"blog/timeseries-part-4/#backend","title":"Backend","text":"<p>The Data Engineering aspect involves a bit of data design and a bit of service writing.</p> <p>I decided the minimum data to track expenses are:</p> <ul> <li><code>purchased_date</code></li> <li>The day on which the purchase was made</li> <li><code>purchased_by</code></li> <li>The name of the person who made the purchase</li> <li><code>price_in_cents</code></li> <li>Price of the purchase. Tracked in cents to avoid floating point perils</li> </ul> <p>Relying on SQLite, we'll have to represent the date as a string, but <code>pandas</code> will help us transform it to a date / datetime object. A table creation routine with SQLite for this might look like:</p> Python<pre><code>import sqlite3\nfrom typing import Optional\n\ndef get_connection(connection_string: str = \":memory:\") -&gt; sqlite3.Connection:\n    \"\"\"Make a connection object to sqlite3 with key-value Rows as outputs\n    - https://stackoverflow.com/questions/48218065/programmingerror-sqlite-objects-created-in-a-thread-can-only-be-used-in-that-sa\n    \"\"\"\n    connection = sqlite3.connect(connection_string)\n    connection.row_factory = sqlite3.Row\n    return connection\n\ndef execute_query(\n    connection: sqlite3.Connection, query: str, args: Optional[dict] = None\n) -&gt; list:\n    \"\"\"Given sqlite3.Connection and a string query (and optionally necessary query args as a dict),\n    Attempt to execute query with cursor, commit transaction, and return fetched rows\"\"\"\n    cur = connection.cursor()\n    if args is not None:\n        cur.execute(query, args)\n    else:\n        cur.execute(query)\n    connection.commit()\n    results = cur.fetchall()\n    cur.close()\n    return results\n\ndef create_expenses_table(connection: sqlite3.Connection) -&gt; None:\n    \"\"\"Create Expenses Table in the database if it doesn't already exist\"\"\"\n    init_expenses_query = f\"\"\"CREATE TABLE IF NOT EXISTS expenses(\n   purchased_date VARCHAR(10) NOT NULL,\n   purchased_by VARCHAR(120) NOT NULL,\n   price_in_cents INT NOT NULL);\"\"\"\n    execute_query(connection, init_expenses_query)\n\nconnection = get_connection()\ncreate_expenses_table(connection)\ninfo_results = execute_query(connection, \"SELECT name, type, sql FROM sqlite_schema WHERE name = 'expenses'\")\ninfo = info_results[0]\ninfo['name'], info['type'], info['sql']\n</code></pre> Text Only<pre><code>('expenses',\n 'table',\n 'CREATE TABLE expenses(\\n   purchased_date VARCHAR(10) NOT NULL,\\n   purchased_by VARCHAR(120) NOT NULL,\\n   price_in_cents INT NOT NULL)')\n</code></pre> <p>We also get a free autoincrementing <code>rowid</code> from SQLite, which will differentiate any purchases by the same person on the same day for the same amount!</p>","tags":["time-series","darts","streamlit","python","intermediate"]},{"location":"blog/timeseries-part-4/#python-object-model","title":"Python Object Model","text":"<p>That's all well and good for a DBA, but what about the Python glue?</p> <p>Using <code>pydantic</code> / <code>dataclasses</code> is my preferred way to make Python classes that represent database objects or API responses. Splitting the Model into a child class for the internal application usage and parent class for database syncing is one way to handle auto-created id's and optional vs. required arguments.</p> Python<pre><code>from datetime import date\nfrom pydantic import BaseModel\n\nclass BaseExpense(BaseModel):\n    price_in_cents: int\n    purchased_date: date\n    purchased_by: str\n\nclass Expense(BaseExpense):\n    rowid: int\n\nExpense(rowid=1, price_in_cents=100, purchased_date=date(2022, 3, 15), purchased_by='gar')\n</code></pre> Text Only<pre><code>Expense(price_in_cents=100, purchased_date=datetime.date(2022, 3, 15), purchased_by='gar', rowid=1)\n</code></pre>","tags":["time-series","darts","streamlit","python","intermediate"]},{"location":"blog/timeseries-part-4/#seeding","title":"Seeding","text":"<p>To get some values for playing around with and demonstrating Create / Update, here's a snippet of seeding the database</p> Python<pre><code>import random\ndef seed_expenses_table(connection: sqlite3.Connection) -&gt; None:\n    \"\"\"Insert sample Expense rows into the database\"\"\"\n    for i in range(200):\n        seed_expense = Expense(\n            rowid=i,\n            purchased_date=date(\n                random.randint(2020, 2022), random.randint(1, 12), random.randint(1, 28)\n            ).strftime(\"%Y-%m-%d\"),\n            purchased_by=random.choice([\"Alice\", \"Bob\", \"Chuck\"]),\n            price_in_cents=random.randint(50, 100_00),\n        )\n        seed_expense_query = f\"\"\"REPLACE into expenses(rowid, purchased_date, purchased_by, price_in_cents)\n        VALUES(:rowid, :purchased_date, :purchased_by, :price_in_cents);\"\"\"\n        execute_query(connection, seed_expense_query, seed_expense.dict())\n\nseed_expenses_table(connection)\nprint('Seeded 200 rows')\n</code></pre> Text Only<pre><code>Seeded 200 rows\n</code></pre> <p>200 times we'll create and save an Expense object with a hardcoded id and some random values for date, purchaser, and price. (Note the randomized days max out at 28 to avoid headaches with february being short. There's probably a builtin to help with random days, maybe just timedelta with random amount is easier)</p> <p>Using <code>kwarg</code> placeholders of the form <code>:keyname</code> lets us pass the dictionary / JSON representation of our Python object instead of specifying each invidual field in the correct order.</p> <p>The rest of the CRUD operations follow a similar pattern. Reading is the only hacky function to allow filtering / querying at database level before pulling ALL records into memory.</p>","tags":["time-series","darts","streamlit","python","intermediate"]},{"location":"blog/timeseries-part-4/#reshaping-the-data","title":"Reshaping the Data","text":"<p>The Data Science aspect of this involves massaging the data into something useful to display.</p> <p>The data as it stands is not actually the well formed time series you might have thought.</p> <p>Sure the date stamps are all real, but what value do we read from them? The goal is to track spending (<code>price_in_cents</code> in the database).</p> <p>But what if we have multiple purchases on the same day? Then we might start treating all the purchases on a given day as stochastic samples and that is not our use case. (But that might fit your use case if you are trying to model behaviour based off of many people's purchases)</p>","tags":["time-series","darts","streamlit","python","intermediate"]},{"location":"blog/timeseries-part-4/#enter-the-pandas","title":"Enter the Pandas","text":"<p>Utilizing Pydantic to parse / validate our database data then dumping as a list of dictionaries for Pandas to handle gets us a dataframe with all the expenses we want to see. Passing a <code>start_date</code>, <code>end_date</code>, and <code>selections</code> will limit the data to certain time range and <code>purchased_by</code> users.</p> Python<pre><code>#collapse-hide\nfrom typing import List\n\nclass ExpenseService:\n    \"\"\"Namespace for Database Related Expense Operations\"\"\"\n\n    def list_all_purchasers(connection: sqlite3.Connection) -&gt; List[str]:\n        select_purchasers = \"SELECT DISTINCT purchased_by FROM expenses\"\n        expense_rows = execute_query(connection, select_purchasers)\n        return [x[\"purchased_by\"] for x in expense_rows]\n\n    def list_all_expenses(\n        connection: sqlite3.Connection,\n        start_date: Optional[date] = None,\n        end_date: Optional[date] = None,\n        selections: Optional[list[str]] = None,\n    ) -&gt; List[sqlite3.Row]:\n        \"\"\"Returns rows from all expenses. Ordered in reverse creation order\"\"\"\n        select = (\n            \"SELECT rowid, purchased_date, purchased_by, price_in_cents FROM expenses\"\n        )\n        where = \"\"\n        do_and = False\n        kwargs = {}\n        if any(x is not None for x in (start_date, end_date, selections)):\n            where = \"WHERE\"\n        if start_date is not None:\n            where += \" purchased_date &gt;= :start_date\"\n            kwargs[\"start_date\"] = start_date\n            do_and = True\n        if end_date is not None:\n            if do_and:\n                where += \" and\"\n            where += \" purchased_date &lt;= :end_date\"\n            kwargs[\"end_date\"] = end_date\n            do_and = True\n        if selections is not None:\n            if do_and:\n                where += \" and\"\n            selection_map = {str(i): x for i, x in enumerate(selections)}\n            where += (\n                f\" purchased_by IN ({','.join(':' + x for x in selection_map.keys())})\"\n            )\n            kwargs.update(selection_map)\n\n        order_by = \"ORDER BY purchased_date DESC;\"\n        query = \" \".join((select, where, order_by))\n        expense_rows = execute_query(connection, query, kwargs)\n        return expense_rows\n</code></pre> Python<pre><code>import pandas as pd\nexpense_rows = ExpenseService.list_all_expenses(\n    connection, date(1900, 1, 1), date(2023, 1, 1), ['Alice', 'Bob', 'Chuck']\n)\nexpenses = [Expense(**row) for row in expense_rows]\nraw_df = pd.DataFrame([x.dict() for x in expenses])\nraw_df.iloc[:3]\n</code></pre>       price_in_cents purchased_date purchased_by rowid     0 9662 2022-12-13 Alice 2   1 9925 2022-12-12 Alice 138   2 6287 2022-12-10 Bob 92     <p>For this analysis I mainly care about total purchase amount per day per person. This means the rowid doesn't really matter to me as a unique identifier, so let's drop it.</p> <p>(This indexing selection will also re-order your columns if you do or do not want that)</p> Python<pre><code>df = raw_df[[\"purchased_date\", \"purchased_by\", \"price_in_cents\"]]\ndf.iloc[:3]\n</code></pre>       .dataframe tbody tr th:only-of-type {         vertical-align: middle;     }      .dataframe tbody tr th {         vertical-align: top;     }      .dataframe thead th {         text-align: right;     }      purchased_date purchased_by price_in_cents     0 2022-12-13 Alice 9662   1 2022-12-12 Alice 9925   2 2022-12-10 Bob 6287     <p>To handle the summation of each person's purchase per day, pandas <code>pivot_table</code> provides us the grouping and sum in one function call.</p> <p>This will get us roughly columnar shaped data for each person</p> Python<pre><code>pivot_df = df.pivot_table(\n    index=\"purchased_date\", columns=\"purchased_by\", aggfunc=\"sum\", fill_value=0\n)\npivot_df.iloc[:3]\n</code></pre>       .dataframe tbody tr th:only-of-type {         vertical-align: middle;     }      .dataframe tbody tr th {         vertical-align: top;     }      .dataframe thead tr th {         text-align: left;     }      .dataframe thead tr:last-of-type th {         text-align: right;     }      price_in_cents   purchased_by Alice Bob Chuck   purchased_date        2020-01-01 0 0 1566   2020-01-20 0 7072 0   2020-01-26 0 0 9982     <p>Looking more like a time series!</p> <p><code>pivot_table</code> had the minor side effect of adding a multi index, which can be popped off if not relevant</p> Python<pre><code>pivot_df.columns = pivot_df.columns.droplevel(0)\npivot_df.iloc[:3]\n</code></pre>       .dataframe tbody tr th:only-of-type {         vertical-align: middle;     }      .dataframe tbody tr th {         vertical-align: top;     }      .dataframe thead th {         text-align: right;     }     purchased_by Alice Bob Chuck   purchased_date        2020-01-01 0 0 1566   2020-01-20 0 7072 0   2020-01-26 0 0 9982     <p>Side note: I also added a feature where \"All\" is a valid selection in addition to all <code>purchased_by</code> users.</p> <p>The \"All\" spending per day is the sum of each row!</p> <p>(We can sanity check this by checking for rows with 2 non-zero values and sum those up to check the All column)</p> Python<pre><code>pivot_df['All'] = pivot_df.sum(axis=1)\npivot_df[(pivot_df.Alice &gt; 0) &amp; (pivot_df.Bob &gt; 0)]\n</code></pre>       .dataframe tbody tr th:only-of-type {         vertical-align: middle;     }      .dataframe tbody tr th {         vertical-align: top;     }      .dataframe thead th {         text-align: right;     }     purchased_by Alice Bob Chuck All   purchased_date         2020-08-12 3716 5896 0 9612   2020-10-09 4881 2595 0 7476   2021-04-11 3965 3623 0 7588   2021-10-19 3332 627 611 4570   2022-01-08 5021 11061 0 16082   2022-03-01 6895 4857 0 11752   2022-08-03 8642 258 0 8900   2022-09-11 4751 1617 0 6368     <p>To fill in date gaps (make the time series have a well defined period of one day), one way is to build your own range of dates and then reindex the time series dataframe with the full range of dates.</p> <p>Grabbing the min and max of the current index gets the start and end points for the range. Filling with 0 is fine by me since there were no purchases on those days</p> Python<pre><code>min_date = pivot_df.index.min()\nmax_date = pivot_df.index.max()\nall_dates = pd.date_range(min_date, max_date, freq=\"D\", name=\"purchased_date\")\npivot_df = pivot_df.reindex(all_dates, fill_value=0)\npivot_df.iloc[:3]\n</code></pre>       .dataframe tbody tr th:only-of-type {         vertical-align: middle;     }      .dataframe tbody tr th {         vertical-align: top;     }      .dataframe thead th {         text-align: right;     }     purchased_by Alice Bob Chuck All   purchased_date         2020-01-01 0 0 1566 1566   2020-01-02 0 0 0 0   2020-01-03 0 0 0 0     <p>To get the cumulative spend up to each point in time, pandas provides <code>cumsum()</code></p> Python<pre><code>cumulative = pivot_df.cumsum()\ncumulative.iloc[:5]\n</code></pre>       .dataframe tbody tr th:only-of-type {         vertical-align: middle;     }      .dataframe tbody tr th {         vertical-align: top;     }      .dataframe thead th {         text-align: right;     }     purchased_by Alice Bob Chuck All   purchased_date         2020-01-01 0 0 1566 1566   2020-01-02 0 0 1566 1566   2020-01-03 0 0 1566 1566   2020-01-04 0 0 1566 1566   2020-01-05 0 0 1566 1566     <p>And to analyze percentage contributed to the whole group's cumulative spending we can divide by the sum of each cumulative row.</p> <p>(We included the \"All\" summation already, so this case is actually slightly over-complicated)</p> Python<pre><code>percentages = (\n    cumulative[cumulative.columns.drop(\"All\", errors=\"ignore\")]\n    .divide(cumulative.sum(axis=1), axis=0)\n    .multiply(100)\n)\npercentages.iloc[:5]\n</code></pre>       .dataframe tbody tr th:only-of-type {         vertical-align: middle;     }      .dataframe tbody tr th {         vertical-align: top;     }      .dataframe thead th {         text-align: right;     }     purchased_by Alice Bob Chuck   purchased_date        2020-01-01 0.0 0.0 50.0   2020-01-02 0.0 0.0 50.0   2020-01-03 0.0 0.0 50.0   2020-01-04 0.0 0.0 50.0   2020-01-05 0.0 0.0 50.0     <p>Grabbing the totals of each spender might be a nice metric to display.</p> <p>This could also be grabbed from the end of the cumulative data</p> Python<pre><code>totals = pivot_df.sum()\ntotals.index.name = \"purchased_by\"\ntotals.name = \"value\"\ntotals = totals.div(100).reset_index()\ntotals\n</code></pre>       .dataframe tbody tr th:only-of-type {         vertical-align: middle;     }      .dataframe tbody tr th {         vertical-align: top;     }      .dataframe thead th {         text-align: right;     }      purchased_by value     0 Alice 3565.16   1 Bob 3019.81   2 Chuck 3826.65   3 All 10411.62     <p>Pandas also provides a convenient <code>rolling()</code> function for applying tranformations on moving windows.</p> <p>In this case let's get the cumulative spending per 7 days per person.</p> <p>Notice that the value will stay the same on days when the person made <code>$0.00</code> of purchases, since <code>x + 0 = x</code>!</p> Python<pre><code>rolling_df = pivot_df.rolling(7, min_periods=1).sum()\nrolling_df.iloc[:8]\n</code></pre>       .dataframe tbody tr th:only-of-type {         vertical-align: middle;     }      .dataframe tbody tr th {         vertical-align: top;     }      .dataframe thead th {         text-align: right;     }     purchased_by Alice Bob Chuck All   purchased_date         2020-01-01 0.0 0.0 1566.0 1566.0   2020-01-02 0.0 0.0 1566.0 1566.0   2020-01-03 0.0 0.0 1566.0 1566.0   2020-01-04 0.0 0.0 1566.0 1566.0   2020-01-05 0.0 0.0 1566.0 1566.0   2020-01-06 0.0 0.0 1566.0 1566.0   2020-01-07 0.0 0.0 1566.0 1566.0   2020-01-08 0.0 0.0 0.0 0.0     <p>We don't have to sum the rolling values though. Here we grab the biggest purchase each person made over each 30 day window.</p> <p>Notice that a given value will stick around for up to 30 days, but will get replaced if a bigger purchase occurs!</p> Python<pre><code>maxes_df = pivot_df.rolling(30, min_periods=1).max()\nmaxes_df.iloc[:31]\n</code></pre>       .dataframe tbody tr th:only-of-type {         vertical-align: middle;     }      .dataframe tbody tr th {         vertical-align: top;     }      .dataframe thead th {         text-align: right;     }     purchased_by Alice Bob Chuck All   purchased_date         2020-01-01 0.0 0.0 1566.0 1566.0   2020-01-02 0.0 0.0 1566.0 1566.0   2020-01-03 0.0 0.0 1566.0 1566.0   2020-01-04 0.0 0.0 1566.0 1566.0   2020-01-05 0.0 0.0 1566.0 1566.0   2020-01-06 0.0 0.0 1566.0 1566.0   2020-01-07 0.0 0.0 1566.0 1566.0   2020-01-08 0.0 0.0 1566.0 1566.0   2020-01-09 0.0 0.0 1566.0 1566.0   2020-01-10 0.0 0.0 1566.0 1566.0   2020-01-11 0.0 0.0 1566.0 1566.0   2020-01-12 0.0 0.0 1566.0 1566.0   2020-01-13 0.0 0.0 1566.0 1566.0   2020-01-14 0.0 0.0 1566.0 1566.0   2020-01-15 0.0 0.0 1566.0 1566.0   2020-01-16 0.0 0.0 1566.0 1566.0   2020-01-17 0.0 0.0 1566.0 1566.0   2020-01-18 0.0 0.0 1566.0 1566.0   2020-01-19 0.0 0.0 1566.0 1566.0   2020-01-20 0.0 7072.0 1566.0 7072.0   2020-01-21 0.0 7072.0 1566.0 7072.0   2020-01-22 0.0 7072.0 1566.0 7072.0   2020-01-23 0.0 7072.0 1566.0 7072.0   2020-01-24 0.0 7072.0 1566.0 7072.0   2020-01-25 0.0 7072.0 1566.0 7072.0   2020-01-26 0.0 7072.0 9982.0 9982.0   2020-01-27 0.0 7072.0 9982.0 9982.0   2020-01-28 0.0 7072.0 9982.0 9982.0   2020-01-29 0.0 7072.0 9982.0 9982.0   2020-01-30 0.0 7072.0 9982.0 9982.0   2020-01-31 0.0 7072.0 9982.0 9982.0","tags":["time-series","darts","streamlit","python","intermediate"]},{"location":"blog/timeseries-part-4/#now-make-it-pretty","title":"Now Make it Pretty","text":"<p>Since we did most of the work in pandas already to shape the data, the Data Analysis of it should be more straightforward</p> <p>We'll use a helper function to do one final transformation that applies to almost all our datasets</p> Python<pre><code>def prep_df_for_display(df: pd.DataFrame) -&gt; pd.DataFrame:\n    return df.divide(100).reset_index().melt(\"purchased_date\")\n\nprepped_cumulative = prep_df_for_display(cumulative)\nprepped_cumulative\n</code></pre>       .dataframe tbody tr th:only-of-type {         vertical-align: middle;     }      .dataframe tbody tr th {         vertical-align: top;     }      .dataframe thead th {         text-align: right;     }      purchased_date purchased_by value     0 2020-01-01 Alice 0.00   1 2020-01-02 Alice 0.00   2 2020-01-03 Alice 0.00   3 2020-01-04 Alice 0.00   4 2020-01-05 Alice 0.00   ... ... ... ...   4307 2022-12-09 All 10152.88   4308 2022-12-10 All 10215.75   4309 2022-12-11 All 10215.75   4310 2022-12-12 All 10315.00   4311 2022-12-13 All 10411.62    <p>4312 rows \u00d7 3 columns</p>  <p>Seems like it's undoing a lot of work we've already done, but this Long Format is generally easier for plotting software to work with.</p> <p>In this case we keep <code>purchased_date</code> as a column (not index), get a value column called <code>value</code>, and a column we can use for trend highlighting which is <code>purchased_by</code></p> <p>After that, plotly express provides the easiest (but not most performant) visualizations in my experience</p> Python<pre><code>import plotly.express as px\nfrom IPython.display import HTML\nline_chart = px.line(\n    prepped_cumulative,\n    x=\"purchased_date\",\n    y=\"value\",\n    color=\"purchased_by\",\n    labels={\"value\": \"Cumulative Dollars Spent\"},\n)\nline_chart.show()\n</code></pre> <p>                                           window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"e376a278-d7dd-4793-9402-b260d670f2dd\")) {                    Plotly.newPlot(                        \"e376a278-d7dd-4793-9402-b260d670f2dd\",                        [{\"hovertemplate\":\"purchased_by=Alice&lt;br&gt;purchased_date=%{x}&lt;br&gt;Cumulative Dollars Spent=%{y}&lt;extra&gt;&lt;/extra&gt;\",\"legendgroup\":\"Alice\",\"line\":{\"color\":\"#636efa\",\"dash\":\"solid\"},\"marker\":{\"symbol\":\"circle\"},\"mode\":\"lines\",\"name\":\"Alice\",\"showlegend\":true,\"x\":[\"2020-01-01T00:00:00\",\"2020-01-02T00:00:00\",\"2020-01-03T00:00:00\",\"2020-01-04T00:00:00\",\"2020-01-05T00:00:00\",\"2020-01-06T00:00:00\",\"2020-01-07T00:00:00\",\"2020-01-08T00:00:00\",\"2020-01-09T00:00:00\",\"2020-01-10T00:00:00\",\"2020-01-11T00:00:00\",\"2020-01-12T00:00:00\",\"2020-01-13T00:00:00\",\"2020-01-14T00:00:00\",\"2020-01-15T00:00:00\",\"2020-01-16T00:00:00\",\"2020-01-17T00:00:00\",\"2020-01-18T00:00:00\",\"2020-01-19T00:00:00\",\"2020-01-20T00:00:00\",\"2020-01-21T00:00:00\",\"2020-01-22T00:00:00\",\"2020-01-23T00:00:00\",\"2020-01-24T00:00:00\",\"2020-01-25T00:00:00\",\"2020-01-26T00:00:00\",\"2020-01-27T00:00:00\",\"2020-01-28T00:00:00\",\"2020-01-29T00:00:00\",\"2020-01-30T00:00:00\",\"2020-01-31T00:00:00\",\"2020-02-01T00:00:00\",\"2020-02-02T00:00:00\",\"2020-02-03T00:00:00\",\"2020-02-04T00:00:00\",\"2020-02-05T00:00:00\",\"2020-02-06T00:00:00\",\"2020-02-07T00:00:00\",\"2020-02-08T00:00:00\",\"2020-02-09T00:00:00\",\"2020-02-10T00:00:00\",\"2020-02-11T00:00:00\",\"2020-02-12T00:00:00\",\"2020-02-13T00:00:00\",\"2020-02-14T00:00:00\",\"2020-02-15T00:00:00\",\"2020-02-16T00:00:00\",\"2020-02-17T00:00:00\",\"2020-02-18T00:00:00\",\"2020-02-19T00:00:00\",\"2020-02-20T00:00:00\",\"2020-02-21T00:00:00\",\"2020-02-22T00:00:00\",\"2020-02-23T00:00:00\",\"2020-02-24T00:00:00\",\"2020-02-25T00:00:00\",\"2020-02-26T00:00:00\",\"2020-02-27T00:00:00\",\"2020-02-28T00:00:00\",\"2020-02-29T00:00:00\",\"2020-03-01T00:00:00\",\"2020-03-02T00:00:00\",\"2020-03-03T00:00:00\",\"2020-03-04T00:00:00\",\"2020-03-05T00:00:00\",\"2020-03-06T00:00:00\",\"2020-03-07T00:00:00\",\"2020-03-08T00:00:00\",\"2020-03-09T00:00:00\",\"2020-03-10T00:00:00\",\"2020-03-11T00:00:00\",\"2020-03-12T00:00:00\",\"2020-03-13T00:00:00\",\"2020-03-14T00:00:00\",\"2020-03-15T00:00:00\",\"2020-03-16T00:00:00\",\"2020-03-17T00:00:00\",\"2020-03-18T00:00:00\",\"2020-03-19T00:00:00\",\"2020-03-20T00:00:00\",\"2020-03-21T00:00:00\",\"2020-03-22T00:00:00\",\"2020-03-23T00:00:00\",\"2020-03-24T00:00:00\",\"2020-03-25T00:00:00\",\"2020-03-26T00:00:00\",\"2020-03-27T00:00:00\",\"2020-03-28T00:00:00\",\"2020-03-29T00:00:00\",\"2020-03-30T00:00:00\",\"2020-03-31T00:00:00\",\"2020-04-01T00:00:00\",\"2020-04-02T00:00:00\",\"2020-04-03T00:00:00\",\"2020-04-04T00:00:00\",\"2020-04-05T00:00:00\",\"2020-04-06T00:00:00\",\"2020-04-07T00:00:00\",\"2020-04-08T00:00:00\",\"2020-04-09T00:00:00\",\"2020-04-10T00:00:00\",\"2020-04-11T00:00:00\",\"2020-04-12T00:00:00\",\"2020-04-13T00:00:00\",\"2020-04-14T00:00:00\",\"2020-04-15T00:00:00\",\"2020-04-16T00:00:00\",\"2020-04-17T00:00:00\",\"2020-04-18T00:00:00\",\"2020-04-19T00:00:00\",\"2020-04-20T00:00:00\",\"2020-04-21T00:00:00\",\"2020-04-22T00:00:00\",\"2020-04-23T00:00:00\",\"2020-04-24T00:00:00\",\"2020-04-25T00:00:00\",\"2020-04-26T00:00:00\",\"2020-04-27T00:00:00\",\"2020-04-28T00:00:00\",\"2020-04-29T00:00:00\",\"2020-04-30T00:00:00\",\"2020-05-01T00:00:00\",\"2020-05-02T00:00:00\",\"2020-05-03T00:00:00\",\"2020-05-04T00:00:00\",\"2020-05-05T00:00:00\",\"2020-05-06T00:00:00\",\"2020-05-07T00:00:00\",\"2020-05-08T00:00:00\",\"2020-05-09T00:00:00\",\"2020-05-10T00:00:00\",\"2020-05-11T00:00:00\",\"2020-05-12T00:00:00\",\"2020-05-13T00:00:00\",\"2020-05-14T00:00:00\",\"2020-05-15T00:00:00\",\"2020-05-16T00:00:00\",\"2020-05-17T00:00:00\",\"2020-05-18T00:00:00\",\"2020-05-19T00:00:00\",\"2020-05-20T00:00:00\",\"2020-05-21T00:00:00\",\"2020-05-22T00:00:00\",\"2020-05-23T00:00:00\",\"2020-05-24T00:00:00\",\"2020-05-25T00:00:00\",\"2020-05-26T00:00:00\",\"2020-05-27T00:00:00\",\"2020-05-28T00:00:00\",\"2020-05-29T00:00:00\",\"2020-05-30T00:00:00\",\"2020-05-31T00:00:00\",\"2020-06-01T00:00:00\",\"2020-06-02T00:00:00\",\"2020-06-03T00:00:00\",\"2020-06-04T00:00:00\",\"2020-06-05T00:00:00\",\"2020-06-06T00:00:00\",\"2020-06-07T00:00:00\",\"2020-06-08T00:00:00\",\"2020-06-09T00:00:00\",\"2020-06-10T00:00:00\",\"2020-06-11T00:00:00\",\"2020-06-12T00:00:00\",\"2020-06-13T00:00:00\",\"2020-06-14T00:00:00\",\"2020-06-15T00:00:00\",\"2020-06-16T00:00:00\",\"2020-06-17T00:00:00\",\"2020-06-18T00:00:00\",\"2020-06-19T00:00:00\",\"2020-06-20T00:00:00\",\"2020-06-21T00:00:00\",\"2020-06-22T00:00:00\",\"2020-06-23T00:00:00\",\"2020-06-24T00:00:00\",\"2020-06-25T00:00:00\",\"2020-06-26T00:00:00\",\"2020-06-27T00:00:00\",\"2020-06-28T00:00:00\",\"2020-06-29T00:00:00\",\"2020-06-30T00:00:00\",\"2020-07-01T00:00:00\",\"2020-07-02T00:00:00\",\"2020-07-03T00:00:00\",\"2020-07-04T00:00:00\",\"2020-07-05T00:00:00\",\"2020-07-06T00:00:00\",\"2020-07-07T00:00:00\",\"2020-07-08T00:00:00\",\"2020-07-09T00:00:00\",\"2020-07-10T00:00:00\",\"2020-07-11T00:00:00\",\"2020-07-12T00:00:00\",\"2020-07-13T00:00:00\",\"2020-07-14T00:00:00\",\"2020-07-15T00:00:00\",\"2020-07-16T00:00:00\",\"2020-07-17T00:00:00\",\"2020-07-18T00:00:00\",\"2020-07-19T00:00:00\",\"2020-07-20T00:00:00\",\"2020-07-21T00:00:00\",\"2020-07-22T00:00:00\",\"2020-07-23T00:00:00\",\"2020-07-24T00:00:00\",\"2020-07-25T00:00:00\",\"2020-07-26T00:00:00\",\"2020-07-27T00:00:00\",\"2020-07-28T00:00:00\",\"2020-07-29T00:00:00\",\"2020-07-30T00:00:00\",\"2020-07-31T00:00:00\",\"2020-08-01T00:00:00\",\"2020-08-02T00:00:00\",\"2020-08-03T00:00:00\",\"2020-08-04T00:00:00\",\"2020-08-05T00:00:00\",\"2020-08-06T00:00:00\",\"2020-08-07T00:00:00\",\"2020-08-08T00:00:00\",\"2020-08-09T00:00:00\",\"2020-08-10T00:00:00\",\"2020-08-11T00:00:00\",\"2020-08-12T00:00:00\",\"2020-08-13T00:00:00\",\"2020-08-14T00:00:00\",\"2020-08-15T00:00:00\",\"2020-08-16T00:00:00\",\"2020-08-17T00:00:00\",\"2020-08-18T00:00:00\",\"2020-08-19T00:00:00\",\"2020-08-20T00:00:00\",\"2020-08-21T00:00:00\",\"2020-08-22T00:00:00\",\"2020-08-23T00:00:00\",\"2020-08-24T00:00:00\",\"2020-08-25T00:00:00\",\"2020-08-26T00:00:00\",\"2020-08-27T00:00:00\",\"2020-08-28T00:00:00\",\"2020-08-29T00:00:00\",\"2020-08-30T00:00:00\",\"2020-08-31T00:00:00\",\"2020-09-01T00:00:00\",\"2020-09-02T00:00:00\",\"2020-09-03T00:00:00\",\"2020-09-04T00:00:00\",\"2020-09-05T00:00:00\",\"2020-09-06T00:00:00\",\"2020-09-07T00:00:00\",\"2020-09-08T00:00:00\",\"2020-09-09T00:00:00\",\"2020-09-10T00:00:00\",\"2020-09-11T00:00:00\",\"2020-09-12T00:00:00\",\"2020-09-13T00:00:00\",\"2020-09-14T00:00:00\",\"2020-09-15T00:00:00\",\"2020-09-16T00:00:00\",\"2020-09-17T00:00:00\",\"2020-09-18T00:00:00\",\"2020-09-19T00:00:00\",\"2020-09-20T00:00:00\",\"2020-09-21T00:00:00\",\"2020-09-22T00:00:00\",\"2020-09-23T00:00:00\",\"2020-09-24T00:00:00\",\"2020-09-25T00:00:00\",\"2020-09-26T00:00:00\",\"2020-09-27T00:00:00\",\"2020-09-28T00:00:00\",\"2020-09-29T00:00:00\",\"2020-09-30T00:00:00\",\"2020-10-01T00:00:00\",\"2020-10-02T00:00:00\",\"2020-10-03T00:00:00\",\"2020-10-04T00:00:00\",\"2020-10-05T00:00:00\",\"2020-10-06T00:00:00\",\"2020-10-07T00:00:00\",\"2020-10-08T00:00:00\",\"2020-10-09T00:00:00\",\"2020-10-10T00:00:00\",\"2020-10-11T00:00:00\",\"2020-10-12T00:00:00\",\"2020-10-13T00:00:00\",\"2020-10-14T00:00:00\",\"2020-10-15T00:00:00\",\"2020-10-16T00:00:00\",\"2020-10-17T00:00:00\",\"2020-10-18T00:00:00\",\"2020-10-19T00:00:00\",\"2020-10-20T00:00:00\",\"2020-10-21T00:00:00\",\"2020-10-22T00:00:00\",\"2020-10-23T00:00:00\",\"2020-10-24T00:00:00\",\"2020-10-25T00:00:00\",\"2020-10-26T00:00:00\",\"2020-10-27T00:00:00\",\"2020-10-28T00:00:00\",\"2020-10-29T00:00:00\",\"2020-10-30T00:00:00\",\"2020-10-31T00:00:00\",\"2020-11-01T00:00:00\",\"2020-11-02T00:00:00\",\"2020-11-03T00:00:00\",\"2020-11-04T00:00:00\",\"2020-11-05T00:00:00\",\"2020-11-06T00:00:00\",\"2020-11-07T00:00:00\",\"2020-11-08T00:00:00\",\"2020-11-09T00:00:00\",\"2020-11-10T00:00:00\",\"2020-11-11T00:00:00\",\"2020-11-12T00:00:00\",\"2020-11-13T00:00:00\",\"2020-11-14T00:00:00\",\"2020-11-15T00:00:00\",\"2020-11-16T00:00:00\",\"2020-11-17T00:00:00\",\"2020-11-18T00:00:00\",\"2020-11-19T00:00:00\",\"2020-11-20T00:00:00\",\"2020-11-21T00:00:00\",\"2020-11-22T00:00:00\",\"2020-11-23T00:00:00\",\"2020-11-24T00:00:00\",\"2020-11-25T00:00:00\",\"2020-11-26T00:00:00\",\"2020-11-27T00:00:00\",\"2020-11-28T00:00:00\",\"2020-11-29T00:00:00\",\"2020-11-30T00:00:00\",\"2020-12-01T00:00:00\",\"2020-12-02T00:00:00\",\"2020-12-03T00:00:00\",\"2020-12-04T00:00:00\",\"2020-12-05T00:00:00\",\"2020-12-06T00:00:00\",\"2020-12-07T00:00:00\",\"2020-12-08T00:00:00\",\"2020-12-09T00:00:00\",\"2020-12-10T00:00:00\",\"2020-12-11T00:00:00\",\"2020-12-12T00:00:00\",\"2020-12-13T00:00:00\",\"2020-12-14T00:00:00\",\"2020-12-15T00:00:00\",\"2020-12-16T00:00:00\",\"2020-12-17T00:00:00\",\"2020-12-18T00:00:00\",\"2020-12-19T00:00:00\",\"2020-12-20T00:00:00\",\"2020-12-21T00:00:00\",\"2020-12-22T00:00:00\",\"2020-12-23T00:00:00\",\"2020-12-24T00:00:00\",\"2020-12-25T00:00:00\",\"2020-12-26T00:00:00\",\"2020-12-27T00:00:00\",\"2020-12-28T00:00:00\",\"2020-12-29T00:00:00\",\"2020-12-30T00:00:00\",\"2020-12-31T00:00:00\",\"2021-01-01T00:00:00\",\"2021-01-02T00:00:00\",\"2021-01-03T00:00:00\",\"2021-01-04T00:00:00\",\"2021-01-05T00:00:00\",\"2021-01-06T00:00:00\",\"2021-01-07T00:00:00\",\"2021-01-08T00:00:00\",\"2021-01-09T00:00:00\",\"2021-01-10T00:00:00\",\"2021-01-11T00:00:00\",\"2021-01-12T00:00:00\",\"2021-01-13T00:00:00\",\"2021-01-14T00:00:00\",\"2021-01-15T00:00:00\",\"2021-01-16T00:00:00\",\"2021-01-17T00:00:00\",\"2021-01-18T00:00:00\",\"2021-01-19T00:00:00\",\"2021-01-20T00:00:00\",\"2021-01-21T00:00:00\",\"2021-01-22T00:00:00\",\"2021-01-23T00:00:00\",\"2021-01-24T00:00:00\",\"2021-01-25T00:00:00\",\"2021-01-26T00:00:00\",\"2021-01-27T00:00:00\",\"2021-01-28T00:00:00\",\"2021-01-29T00:00:00\",\"2021-01-30T00:00:00\",\"2021-01-31T00:00:00\",\"2021-02-01T00:00:00\",\"2021-02-02T00:00:00\",\"2021-02-03T00:00:00\",\"2021-02-04T00:00:00\",\"2021-02-05T00:00:00\",\"2021-02-06T00:00:00\",\"2021-02-07T00:00:00\",\"2021-02-08T00:00:00\",\"2021-02-09T00:00:00\",\"2021-02-10T00:00:00\",\"2021-02-11T00:00:00\",\"2021-02-12T00:00:00\",\"2021-02-13T00:00:00\",\"2021-02-14T00:00:00\",\"2021-02-15T00:00:00\",\"2021-02-16T00:00:00\",\"2021-02-17T00:00:00\",\"2021-02-18T00:00:00\",\"2021-02-19T00:00:00\",\"2021-02-20T00:00:00\",\"2021-02-21T00:00:00\",\"2021-02-22T00:00:00\",\"2021-02-23T00:00:00\",\"2021-02-24T00:00:00\",\"2021-02-25T00:00:00\",\"2021-02-26T00:00:00\",\"2021-02-27T00:00:00\",\"2021-02-28T00:00:00\",\"2021-03-01T00:00:00\",\"2021-03-02T00:00:00\",\"2021-03-03T00:00:00\",\"2021-03-04T00:00:00\",\"2021-03-05T00:00:00\",\"2021-03-06T00:00:00\",\"2021-03-07T00:00:00\",\"2021-03-08T00:00:00\",\"2021-03-09T00:00:00\",\"2021-03-10T00:00:00\",\"2021-03-11T00:00:00\",\"2021-03-12T00:00:00\",\"2021-03-13T00:00:00\",\"2021-03-14T00:00:00\",\"2021-03-15T00:00:00\",\"2021-03-16T00:00:00\",\"2021-03-17T00:00:00\",\"2021-03-18T00:00:00\",\"2021-03-19T00:00:00\",\"2021-03-20T00:00:00\",\"2021-03-21T00:00:00\",\"2021-03-22T00:00:00\",\"2021-03-23T00:00:00\",\"2021-03-24T00:00:00\",\"2021-03-25T00:00:00\",\"2021-03-26T00:00:00\",\"2021-03-27T00:00:00\",\"2021-03-28T00:00:00\",\"2021-03-29T00:00:00\",\"2021-03-30T00:00:00\",\"2021-03-31T00:00:00\",\"2021-04-01T00:00:00\",\"2021-04-02T00:00:00\",\"2021-04-03T00:00:00\",\"2021-04-04T00:00:00\",\"2021-04-05T00:00:00\",\"2021-04-06T00:00:00\",\"2021-04-07T00:00:00\",\"2021-04-08T00:00:00\",\"2021-04-09T00:00:00\",\"2021-04-10T00:00:00\",\"2021-04-11T00:00:00\",\"2021-04-12T00:00:00\",\"2021-04-13T00:00:00\",\"2021-04-14T00:00:00\",\"2021-04-15T00:00:00\",\"2021-04-16T00:00:00\",\"2021-04-17T00:00:00\",\"2021-04-18T00:00:00\",\"2021-04-19T00:00:00\",\"2021-04-20T00:00:00\",\"2021-04-21T00:00:00\",\"2021-04-22T00:00:00\",\"2021-04-23T00:00:00\",\"2021-04-24T00:00:00\",\"2021-04-25T00:00:00\",\"2021-04-26T00:00:00\",\"2021-04-27T00:00:00\",\"2021-04-28T00:00:00\",\"2021-04-29T00:00:00\",\"2021-04-30T00:00:00\",\"2021-05-01T00:00:00\",\"2021-05-02T00:00:00\",\"2021-05-03T00:00:00\",\"2021-05-04T00:00:00\",\"2021-05-05T00:00:00\",\"2021-05-06T00:00:00\",\"2021-05-07T00:00:00\",\"2021-05-08T00:00:00\",\"2021-05-09T00:00:00\",\"2021-05-10T00:00:00\",\"2021-05-11T00:00:00\",\"2021-05-12T00:00:00\",\"2021-05-13T00:00:00\",\"2021-05-14T00:00:00\",\"2021-05-15T00:00:00\",\"2021-05-16T00:00:00\",\"2021-05-17T00:00:00\",\"2021-05-18T00:00:00\",\"2021-05-19T00:00:00\",\"2021-05-20T00:00:00\",\"2021-05-21T00:00:00\",\"2021-05-22T00:00:00\",\"2021-05-23T00:00:00\",\"2021-05-24T00:00:00\",\"2021-05-25T00:00:00\",\"2021-05-26T00:00:00\",\"2021-05-27T00:00:00\",\"2021-05-28T00:00:00\",\"2021-05-29T00:00:00\",\"2021-05-30T00:00:00\",\"2021-05-31T00:00:00\",\"2021-06-01T00:00:00\",\"2021-06-02T00:00:00\",\"2021-06-03T00:00:00\",\"2021-06-04T00:00:00\",\"2021-06-05T00:00:00\",\"2021-06-06T00:00:00\",\"2021-06-07T00:00:00\",\"2021-06-08T00:00:00\",\"2021-06-09T00:00:00\",\"2021-06-10T00:00:00\",\"2021-06-11T00:00:00\",\"2021-06-12T00:00:00\",\"2021-06-13T00:00:00\",\"2021-06-14T00:00:00\",\"2021-06-15T00:00:00\",\"2021-06-16T00:00:00\",\"2021-06-17T00:00:00\",\"2021-06-18T00:00:00\",\"2021-06-19T00:00:00\",\"2021-06-20T00:00:00\",\"2021-06-21T00:00:00\",\"2021-06-22T00:00:00\",\"2021-06-23T00:00:00\",\"2021-06-24T00:00:00\",\"2021-06-25T00:00:00\",\"2021-06-26T00:00:00\",\"2021-06-27T00:00:00\",\"2021-06-28T00:00:00\",\"2021-06-29T00:00:00\",\"2021-06-30T00:00:00\",\"2021-07-01T00:00:00\",\"2021-07-02T00:00:00\",\"2021-07-03T00:00:00\",\"2021-07-04T00:00:00\",\"2021-07-05T00:00:00\",\"2021-07-06T00:00:00\",\"2021-07-07T00:00:00\",\"2021-07-08T00:00:00\",\"2021-07-09T00:00:00\",\"2021-07-10T00:00:00\",\"2021-07-11T00:00:00\",\"2021-07-12T00:00:00\",\"2021-07-13T00:00:00\",\"2021-07-14T00:00:00\",\"2021-07-15T00:00:00\",\"2021-07-16T00:00:00\",\"2021-07-17T00:00:00\",\"2021-07-18T00:00:00\",\"2021-07-19T00:00:00\",\"2021-07-20T00:00:00\",\"2021-07-21T00:00:00\",\"2021-07-22T00:00:00\",\"2021-07-23T00:00:00\",\"2021-07-24T00:00:00\",\"2021-07-25T00:00:00\",\"2021-07-26T00:00:00\",\"2021-07-27T00:00:00\",\"2021-07-28T00:00:00\",\"2021-07-29T00:00:00\",\"2021-07-30T00:00:00\",\"2021-07-31T00:00:00\",\"2021-08-01T00:00:00\",\"2021-08-02T00:00:00\",\"2021-08-03T00:00:00\",\"2021-08-04T00:00:00\",\"2021-08-05T00:00:00\",\"2021-08-06T00:00:00\",\"2021-08-07T00:00:00\",\"2021-08-08T00:00:00\",\"2021-08-09T00:00:00\",\"2021-08-10T00:00:00\",\"2021-08-11T00:00:00\",\"2021-08-12T00:00:00\",\"2021-08-13T00:00:00\",\"2021-08-14T00:00:00\",\"2021-08-15T00:00:00\",\"2021-08-16T00:00:00\",\"2021-08-17T00:00:00\",\"2021-08-18T00:00:00\",\"2021-08-19T00:00:00\",\"2021-08-20T00:00:00\",\"2021-08-21T00:00:00\",\"2021-08-22T00:00:00\",\"2021-08-23T00:00:00\",\"2021-08-24T00:00:00\",\"2021-08-25T00:00:00\",\"2021-08-26T00:00:00\",\"2021-08-27T00:00:00\",\"2021-08-28T00:00:00\",\"2021-08-29T00:00:00\",\"2021-08-30T00:00:00\",\"2021-08-31T00:00:00\",\"2021-09-01T00:00:00\",\"2021-09-02T00:00:00\",\"2021-09-03T00:00:00\",\"2021-09-04T00:00:00\",\"2021-09-05T00:00:00\",\"2021-09-06T00:00:00\",\"2021-09-07T00:00:00\",\"2021-09-08T00:00:00\",\"2021-09-09T00:00:00\",\"2021-09-10T00:00:00\",\"2021-09-11T00:00:00\",\"2021-09-12T00:00:00\",\"2021-09-13T00:00:00\",\"2021-09-14T00:00:00\",\"2021-09-15T00:00:00\",\"2021-09-16T00:00:00\",\"2021-09-17T00:00:00\",\"2021-09-18T00:00:00\",\"2021-09-19T00:00:00\",\"2021-09-20T00:00:00\",\"2021-09-21T00:00:00\",\"2021-09-22T00:00:00\",\"2021-09-23T00:00:00\",\"2021-09-24T00:00:00\",\"2021-09-25T00:00:00\",\"2021-09-26T00:00:00\",\"2021-09-27T00:00:00\",\"2021-09-28T00:00:00\",\"2021-09-29T00:00:00\",\"2021-09-30T00:00:00\",\"2021-10-01T00:00:00\",\"2021-10-02T00:00:00\",\"2021-10-03T00:00:00\",\"2021-10-04T00:00:00\",\"2021-10-05T00:00:00\",\"2021-10-06T00:00:00\",\"2021-10-07T00:00:00\",\"2021-10-08T00:00:00\",\"2021-10-09T00:00:00\",\"2021-10-10T00:00:00\",\"2021-10-11T00:00:00\",\"2021-10-12T00:00:00\",\"2021-10-13T00:00:00\",\"2021-10-14T00:00:00\",\"2021-10-15T00:00:00\",\"2021-10-16T00:00:00\",\"2021-10-17T00:00:00\",\"2021-10-18T00:00:00\",\"2021-10-19T00:00:00\",\"2021-10-20T00:00:00\",\"2021-10-21T00:00:00\",\"2021-10-22T00:00:00\",\"2021-10-23T00:00:00\",\"2021-10-24T00:00:00\",\"2021-10-25T00:00:00\",\"2021-10-26T00:00:00\",\"2021-10-27T00:00:00\",\"2021-10-28T00:00:00\",\"2021-10-29T00:00:00\",\"2021-10-30T00:00:00\",\"2021-10-31T00:00:00\",\"2021-11-01T00:00:00\",\"2021-11-02T00:00:00\",\"2021-11-03T00:00:00\",\"2021-11-04T00:00:00\",\"2021-11-05T00:00:00\",\"2021-11-06T00:00:00\",\"2021-11-07T00:00:00\",\"2021-11-08T00:00:00\",\"2021-11-09T00:00:00\",\"2021-11-10T00:00:00\",\"2021-11-11T00:00:00\",\"2021-11-12T00:00:00\",\"2021-11-13T00:00:00\",\"2021-11-14T00:00:00\",\"2021-11-15T00:00:00\",\"2021-11-16T00:00:00\",\"2021-11-17T00:00:00\",\"2021-11-18T00:00:00\",\"2021-11-19T00:00:00\",\"2021-11-20T00:00:00\",\"2021-11-21T00:00:00\",\"2021-11-22T00:00:00\",\"2021-11-23T00:00:00\",\"2021-11-24T00:00:00\",\"2021-11-25T00:00:00\",\"2021-11-26T00:00:00\",\"2021-11-27T00:00:00\",\"2021-11-28T00:00:00\",\"2021-11-29T00:00:00\",\"2021-11-30T00:00:00\",\"2021-12-01T00:00:00\",\"2021-12-02T00:00:00\",\"2021-12-03T00:00:00\",\"2021-12-04T00:00:00\",\"2021-12-05T00:00:00\",\"2021-12-06T00:00:00\",\"2021-12-07T00:00:00\",\"2021-12-08T00:00:00\",\"2021-12-09T00:00:00\",\"2021-12-10T00:00:00\",\"2021-12-11T00:00:00\",\"2021-12-12T00:00:00\",\"2021-12-13T00:00:00\",\"2021-12-14T00:00:00\",\"2021-12-15T00:00:00\",\"2021-12-16T00:00:00\",\"2021-12-17T00:00:00\",\"2021-12-18T00:00:00\",\"2021-12-19T00:00:00\",\"2021-12-20T00:00:00\",\"2021-12-21T00:00:00\",\"2021-12-22T00:00:00\",\"2021-12-23T00:00:00\",\"2021-12-24T00:00:00\",\"2021-12-25T00:00:00\",\"2021-12-26T00:00:00\",\"2021-12-27T00:00:00\",\"2021-12-28T00:00:00\",\"2021-12-29T00:00:00\",\"2021-12-30T00:00:00\",\"2021-12-31T00:00:00\",\"2022-01-01T00:00:00\",\"2022-01-02T00:00:00\",\"2022-01-03T00:00:00\",\"2022-01-04T00:00:00\",\"2022-01-05T00:00:00\",\"2022-01-06T00:00:00\",\"2022-01-07T00:00:00\",\"2022-01-08T00:00:00\",\"2022-01-09T00:00:00\",\"2022-01-10T00:00:00\",\"2022-01-11T00:00:00\",\"2022-01-12T00:00:00\",\"2022-01-13T00:00:00\",\"2022-01-14T00:00:00\",\"2022-01-15T00:00:00\",\"2022-01-16T00:00:00\",\"2022-01-17T00:00:00\",\"2022-01-18T00:00:00\",\"2022-01-19T00:00:00\",\"2022-01-20T00:00:00\",\"2022-01-21T00:00:00\",\"2022-01-22T00:00:00\",\"2022-01-23T00:00:00\",\"2022-01-24T00:00:00\",\"2022-01-25T00:00:00\",\"2022-01-26T00:00:00\",\"2022-01-27T00:00:00\",\"2022-01-28T00:00:00\",\"2022-01-29T00:00:00\",\"2022-01-30T00:00:00\",\"2022-01-31T00:00:00\",\"2022-02-01T00:00:00\",\"2022-02-02T00:00:00\",\"2022-02-03T00:00:00\",\"2022-02-04T00:00:00\",\"2022-02-05T00:00:00\",\"2022-02-06T00:00:00\",\"2022-02-07T00:00:00\",\"2022-02-08T00:00:00\",\"2022-02-09T00:00:00\",\"2022-02-10T00:00:00\",\"2022-02-11T00:00:00\",\"2022-02-12T00:00:00\",\"2022-02-13T00:00:00\",\"2022-02-14T00:00:00\",\"2022-02-15T00:00:00\",\"2022-02-16T00:00:00\",\"2022-02-17T00:00:00\",\"2022-02-18T00:00:00\",\"2022-02-19T00:00:00\",\"2022-02-20T00:00:00\",\"2022-02-21T00:00:00\",\"2022-02-22T00:00:00\",\"2022-02-23T00:00:00\",\"2022-02-24T00:00:00\",\"2022-02-25T00:00:00\",\"2022-02-26T00:00:00\",\"2022-02-27T00:00:00\",\"2022-02-28T00:00:00\",\"2022-03-01T00:00:00\",\"2022-03-02T00:00:00\",\"2022-03-03T00:00:00\",\"2022-03-04T00:00:00\",\"2022-03-05T00:00:00\",\"2022-03-06T00:00:00\",\"2022-03-07T00:00:00\",\"2022-03-08T00:00:00\",\"2022-03-09T00:00:00\",\"2022-03-10T00:00:00\",\"2022-03-11T00:00:00\",\"2022-03-12T00:00:00\",\"2022-03-13T00:00:00\",\"2022-03-14T00:00:00\",\"2022-03-15T00:00:00\",\"2022-03-16T00:00:00\",\"2022-03-17T00:00:00\",\"2022-03-18T00:00:00\",\"2022-03-19T00:00:00\",\"2022-03-20T00:00:00\",\"2022-03-21T00:00:00\",\"2022-03-22T00:00:00\",\"2022-03-23T00:00:00\",\"2022-03-24T00:00:00\",\"2022-03-25T00:00:00\",\"2022-03-26T00:00:00\",\"2022-03-27T00:00:00\",\"2022-03-28T00:00:00\",\"2022-03-29T00:00:00\",\"2022-03-30T00:00:00\",\"2022-03-31T00:00:00\",\"2022-04-01T00:00:00\",\"2022-04-02T00:00:00\",\"2022-04-03T00:00:00\",\"2022-04-04T00:00:00\",\"2022-04-05T00:00:00\",\"2022-04-06T00:00:00\",\"2022-04-07T00:00:00\",\"2022-04-08T00:00:00\",\"2022-04-09T00:00:00\",\"2022-04-10T00:00:00\",\"2022-04-11T00:00:00\",\"2022-04-12T00:00:00\",\"2022-04-13T00:00:00\",\"2022-04-14T00:00:00\",\"2022-04-15T00:00:00\",\"2022-04-16T00:00:00\",\"2022-04-17T00:00:00\",\"2022-04-18T00:00:00\",\"2022-04-19T00:00:00\",\"2022-04-20T00:00:00\",\"2022-04-21T00:00:00\",\"2022-04-22T00:00:00\",\"2022-04-23T00:00:00\",\"2022-04-24T00:00:00\",\"2022-04-25T00:00:00\",\"2022-04-26T00:00:00\",\"2022-04-27T00:00:00\",\"2022-04-28T00:00:00\",\"2022-04-29T00:00:00\",\"2022-04-30T00:00:00\",\"2022-05-01T00:00:00\",\"2022-05-02T00:00:00\",\"2022-05-03T00:00:00\",\"2022-05-04T00:00:00\",\"2022-05-05T00:00:00\",\"2022-05-06T00:00:00\",\"2022-05-07T00:00:00\",\"2022-05-08T00:00:00\",\"2022-05-09T00:00:00\",\"2022-05-10T00:00:00\",\"2022-05-11T00:00:00\",\"2022-05-12T00:00:00\",\"2022-05-13T00:00:00\",\"2022-05-14T00:00:00\",\"2022-05-15T00:00:00\",\"2022-05-16T00:00:00\",\"2022-05-17T00:00:00\",\"2022-05-18T00:00:00\",\"2022-05-19T00:00:00\",\"2022-05-20T00:00:00\",\"2022-05-21T00:00:00\",\"2022-05-22T00:00:00\",\"2022-05-23T00:00:00\",\"2022-05-24T00:00:00\",\"2022-05-25T00:00:00\",\"2022-05-26T00:00:00\",\"2022-05-27T00:00:00\",\"2022-05-28T00:00:00\",\"2022-05-29T00:00:00\",\"2022-05-30T00:00:00\",\"2022-05-31T00:00:00\",\"2022-06-01T00:00:00\",\"2022-06-02T00:00:00\",\"2022-06-03T00:00:00\",\"2022-06-04T00:00:00\",\"2022-06-05T00:00:00\",\"2022-06-06T00:00:00\",\"2022-06-07T00:00:00\",\"2022-06-08T00:00:00\",\"2022-06-09T00:00:00\",\"2022-06-10T00:00:00\",\"2022-06-11T00:00:00\",\"2022-06-12T00:00:00\",\"2022-06-13T00:00:00\",\"2022-06-14T00:00:00\",\"2022-06-15T00:00:00\",\"2022-06-16T00:00:00\",\"2022-06-17T00:00:00\",\"2022-06-18T00:00:00\",\"2022-06-19T00:00:00\",\"2022-06-20T00:00:00\",\"2022-06-21T00:00:00\",\"2022-06-22T00:00:00\",\"2022-06-23T00:00:00\",\"2022-06-24T00:00:00\",\"2022-06-25T00:00:00\",\"2022-06-26T00:00:00\",\"2022-06-27T00:00:00\",\"2022-06-28T00:00:00\",\"2022-06-29T00:00:00\",\"2022-06-30T00:00:00\",\"2022-07-01T00:00:00\",\"2022-07-02T00:00:00\",\"2022-07-03T00:00:00\",\"2022-07-04T00:00:00\",\"2022-07-05T00:00:00\",\"2022-07-06T00:00:00\",\"2022-07-07T00:00:00\",\"2022-07-08T00:00:00\",\"2022-07-09T00:00:00\",\"2022-07-10T00:00:00\",\"2022-07-11T00:00:00\",\"2022-07-12T00:00:00\",\"2022-07-13T00:00:00\",\"2022-07-14T00:00:00\",\"2022-07-15T00:00:00\",\"2022-07-16T00:00:00\",\"2022-07-17T00:00:00\",\"2022-07-18T00:00:00\",\"2022-07-19T00:00:00\",\"2022-07-20T00:00:00\",\"2022-07-21T00:00:00\",\"2022-07-22T00:00:00\",\"2022-07-23T00:00:00\",\"2022-07-24T00:00:00\",\"2022-07-25T00:00:00\",\"2022-07-26T00:00:00\",\"2022-07-27T00:00:00\",\"2022-07-28T00:00:00\",\"2022-07-29T00:00:00\",\"2022-07-30T00:00:00\",\"2022-07-31T00:00:00\",\"2022-08-01T00:00:00\",\"2022-08-02T00:00:00\",\"2022-08-03T00:00:00\",\"2022-08-04T00:00:00\",\"2022-08-05T00:00:00\",\"2022-08-06T00:00:00\",\"2022-08-07T00:00:00\",\"2022-08-08T00:00:00\",\"2022-08-09T00:00:00\",\"2022-08-10T00:00:00\",\"2022-08-11T00:00:00\",\"2022-08-12T00:00:00\",\"2022-08-13T00:00:00\",\"2022-08-14T00:00:00\",\"2022-08-15T00:00:00\",\"2022-08-16T00:00:00\",\"2022-08-17T00:00:00\",\"2022-08-18T00:00:00\",\"2022-08-19T00:00:00\",\"2022-08-20T00:00:00\",\"2022-08-21T00:00:00\",\"2022-08-22T00:00:00\",\"2022-08-23T00:00:00\",\"2022-08-24T00:00:00\",\"2022-08-25T00:00:00\",\"2022-08-26T00:00:00\",\"2022-08-27T00:00:00\",\"2022-08-28T00:00:00\",\"2022-08-29T00:00:00\",\"2022-08-30T00:00:00\",\"2022-08-31T00:00:00\",\"2022-09-01T00:00:00\",\"2022-09-02T00:00:00\",\"2022-09-03T00:00:00\",\"2022-09-04T00:00:00\",\"2022-09-05T00:00:00\",\"2022-09-06T00:00:00\",\"2022-09-07T00:00:00\",\"2022-09-08T00:00:00\",\"2022-09-09T00:00:00\",\"2022-09-10T00:00:00\",\"2022-09-11T00:00:00\",\"2022-09-12T00:00:00\",\"2022-09-13T00:00:00\",\"2022-09-14T00:00:00\",\"2022-09-15T00:00:00\",\"2022-09-16T00:00:00\",\"2022-09-17T00:00:00\",\"2022-09-18T00:00:00\",\"2022-09-19T00:00:00\",\"2022-09-20T00:00:00\",\"2022-09-21T00:00:00\",\"2022-09-22T00:00:00\",\"2022-09-23T00:00:00\",\"2022-09-24T00:00:00\",\"2022-09-25T00:00:00\",\"2022-09-26T00:00:00\",\"2022-09-27T00:00:00\",\"2022-09-28T00:00:00\",\"2022-09-29T00:00:00\",\"2022-09-30T00:00:00\",\"2022-10-01T00:00:00\",\"2022-10-02T00:00:00\",\"2022-10-03T00:00:00\",\"2022-10-04T00:00:00\",\"2022-10-05T00:00:00\",\"2022-10-06T00:00:00\",\"2022-10-07T00:00:00\",\"2022-10-08T00:00:00\",\"2022-10-09T00:00:00\",\"2022-10-10T00:00:00\",\"2022-10-11T00:00:00\",\"2022-10-12T00:00:00\",\"2022-10-13T00:00:00\",\"2022-10-14T00:00:00\",\"2022-10-15T00:00:00\",\"2022-10-16T00:00:00\",\"2022-10-17T00:00:00\",\"2022-10-18T00:00:00\",\"2022-10-19T00:00:00\",\"2022-10-20T00:00:00\",\"2022-10-21T00:00:00\",\"2022-10-22T00:00:00\",\"2022-10-23T00:00:00\",\"2022-10-24T00:00:00\",\"2022-10-25T00:00:00\",\"2022-10-26T00:00:00\",\"2022-10-27T00:00:00\",\"2022-10-28T00:00:00\",\"2022-10-29T00:00:00\",\"2022-10-30T00:00:00\",\"2022-10-31T00:00:00\",\"2022-11-01T00:00:00\",\"2022-11-02T00:00:00\",\"2022-11-03T00:00:00\",\"2022-11-04T00:00:00\",\"2022-11-05T00:00:00\",\"2022-11-06T00:00:00\",\"2022-11-07T00:00:00\",\"2022-11-08T00:00:00\",\"2022-11-09T00:00:00\",\"2022-11-10T00:00:00\",\"2022-11-11T00:00:00\",\"2022-11-12T00:00:00\",\"2022-11-13T00:00:00\",\"2022-11-14T00:00:00\",\"2022-11-15T00:00:00\",\"2022-11-16T00:00:00\",\"2022-11-17T00:00:00\",\"2022-11-18T00:00:00\",\"2022-11-19T00:00:00\",\"2022-11-20T00:00:00\",\"2022-11-21T00:00:00\",\"2022-11-22T00:00:00\",\"2022-11-23T00:00:00\",\"2022-11-24T00:00:00\",\"2022-11-25T00:00:00\",\"2022-11-26T00:00:00\",\"2022-11-27T00:00:00\",\"2022-11-28T00:00:00\",\"2022-11-29T00:00:00\",\"2022-11-30T00:00:00\",\"2022-12-01T00:00:00\",\"2022-12-02T00:00:00\",\"2022-12-03T00:00:00\",\"2022-12-04T00:00:00\",\"2022-12-05T00:00:00\",\"2022-12-06T00:00:00\",\"2022-12-07T00:00:00\",\"2022-12-08T00:00:00\",\"2022-12-09T00:00:00\",\"2022-12-10T00:00:00\",\"2022-12-11T00:00:00\",\"2022-12-12T00:00:00\",\"2022-12-13T00:00:00\"],\"xaxis\":\"x\",\"y\":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,47.4,47.4,47.4,47.4,47.4,47.4,47.4,47.4,47.4,47.4,47.4,47.4,47.4,47.4,47.4,47.4,47.4,47.4,47.4,47.4,47.4,47.4,47.4,47.4,47.4,47.4,47.4,47.4,47.4,47.4,47.4,47.4,47.4,47.4,47.4,47.4,47.4,47.4,47.4,47.4,47.4,47.4,47.4,47.4,47.4,47.4,47.4,47.4,47.4,47.4,47.4,47.4,47.4,47.4,47.4,47.4,47.4,47.4,47.4,47.4,47.4,47.4,47.4,47.4,47.4,47.4,47.4,47.4,47.4,47.4,47.4,47.4,47.4,47.4,47.4,47.4,47.4,47.4,47.4,47.4,47.4,47.4,47.4,47.4,47.4,94.24,188.25,188.25,188.25,188.25,188.25,188.25,188.25,188.25,188.25,188.25,188.25,188.25,188.25,188.25,188.25,188.25,188.25,188.25,188.25,188.25,188.25,188.25,188.25,188.25,188.25,188.25,188.25,188.25,276.87,276.87,276.87,276.87,276.87,276.87,340.81,340.81,340.81,340.81,340.81,340.81,340.81,389.67,389.67,389.67,389.67,389.67,389.67,389.67,389.67,389.67,410.52,410.52,410.52,410.52,410.52,410.52,410.52,410.52,410.52,410.52,410.52,410.52,410.52,410.52,410.52,410.52,410.52,410.52,410.52,410.52,410.52,410.52,410.52,410.52,421.9,421.9,459.06,459.06,459.06,459.06,459.06,459.06,459.06,459.06,459.06,459.06,459.06,459.06,459.06,459.06,459.06,459.06,459.06,459.06,459.06,459.06,459.06,459.06,459.06,459.06,459.06,459.06,515.62,515.62,515.62,601.66,601.66,601.66,601.66,601.66,601.66,601.66,601.66,601.66,601.66,601.66,601.66,685.54,685.54,685.54,685.54,685.54,685.54,685.54,685.54,685.54,734.79,734.79,734.79,734.79,734.79,734.79,734.79,734.79,783.6,783.6,783.6,783.6,783.6,873.96,873.96,873.96,873.96,873.96,873.96,885.99,885.99,885.99,885.99,885.99,885.99,885.99,885.99,885.99,885.99,885.99,885.99,885.99,885.99,885.99,885.99,885.99,949.12,949.12,949.12,949.12,949.12,949.12,949.12,949.12,949.12,949.12,949.12,949.12,949.12,949.12,949.12,949.12,949.12,949.12,949.12,949.12,949.12,949.12,949.12,949.12,949.12,949.12,1027.38,1027.38,1027.38,1027.38,1027.38,1027.38,1027.38,1027.38,1027.38,1027.38,1027.38,1027.38,1027.38,1027.38,1027.38,1027.38,1027.38,1027.38,1027.38,1028.98,1028.98,1072.58,1072.58,1072.58,1072.58,1072.58,1072.58,1072.58,1072.58,1072.58,1072.58,1072.58,1072.58,1072.58,1072.58,1072.58,1072.58,1072.58,1072.58,1072.58,1072.58,1072.58,1072.58,1072.58,1072.58,1072.58,1072.58,1072.58,1072.58,1072.58,1098.66,1098.66,1098.66,1098.66,1098.66,1098.66,1098.66,1098.66,1098.66,1098.66,1098.66,1098.66,1098.66,1098.66,1098.66,1098.66,1098.66,1098.66,1098.66,1098.66,1098.66,1098.66,1098.66,1098.66,1098.66,1098.66,1098.66,1098.66,1098.66,1098.66,1098.66,1098.66,1098.66,1098.66,1098.66,1098.66,1098.66,1098.66,1183.34,1183.34,1183.34,1183.34,1183.34,1183.34,1183.34,1183.34,1198.95,1198.95,1198.95,1198.95,1296.81,1296.81,1296.81,1296.81,1296.81,1296.81,1296.81,1296.81,1296.81,1302.48,1302.48,1302.48,1302.48,1302.48,1345.84,1345.84,1345.84,1345.84,1345.84,1345.84,1345.84,1345.84,1345.84,1345.84,1345.84,1345.84,1345.84,1345.84,1345.84,1345.84,1385.49,1385.49,1385.49,1385.49,1385.49,1385.49,1454.54,1454.54,1454.54,1454.54,1454.54,1454.54,1454.54,1454.54,1454.54,1454.54,1454.54,1454.54,1454.54,1454.54,1454.54,1454.54,1454.54,1488.06,1488.06,1488.06,1488.06,1488.06,1488.06,1488.06,1488.06,1488.06,1488.06,1488.06,1488.06,1488.06,1488.06,1488.06,1488.06,1488.06,1488.06,1488.06,1495.17,1495.17,1495.17,1495.17,1495.17,1495.17,1495.17,1495.17,1495.17,1495.17,1495.17,1495.17,1495.17,1559.35,1559.35,1559.35,1559.35,1559.35,1559.35,1559.35,1559.35,1559.35,1638.66,1638.66,1638.66,1638.66,1638.66,1638.66,1638.66,1638.66,1638.66,1638.66,1638.66,1638.66,1638.66,1638.66,1638.66,1638.66,1638.66,1638.66,1638.66,1638.66,1748.76,1748.76,1748.76,1748.76,1748.76,1748.76,1748.76,1748.76,1748.76,1748.76,1748.76,1748.76,1748.76,1748.76,1748.76,1748.76,1748.76,1748.76,1748.76,1748.76,1748.76,1748.76,1748.76,1748.76,1748.76,1748.76,1748.76,1748.76,1748.76,1748.76,1748.76,1748.76,1748.76,1748.76,1748.76,1748.76,1748.76,1748.76,1748.76,1748.76,1748.76,1748.76,1748.76,1748.76,1748.76,1748.76,1748.76,1748.76,1748.76,1812.45,1812.45,1812.45,1812.45,1812.45,1812.45,1812.45,1812.45,1812.45,1812.45,1896.57,1896.57,1896.57,1896.57,1896.57,1896.57,1896.57,1896.57,1896.57,1936.66,1936.66,1936.66,1936.66,1936.66,2076.19,2076.19,2076.19,2156.23,2156.23,2156.23,2156.23,2156.23,2156.23,2156.23,2156.23,2156.23,2156.23,2156.23,2156.23,2156.23,2156.23,2156.23,2156.23,2157.18,2157.18,2157.18,2157.18,2157.18,2157.18,2157.18,2157.18,2157.18,2157.18,2157.18,2157.18,2157.18,2157.18,2157.18,2190.5,2190.5,2190.5,2190.5,2190.5,2190.5,2190.5,2190.5,2190.5,2190.5,2190.5,2190.5,2190.5,2190.5,2190.5,2190.5,2190.5,2190.5,2190.5,2190.5,2190.5,2190.5,2190.5,2190.5,2190.5,2190.5,2190.5,2190.5,2190.5,2190.5,2190.5,2262.35,2262.35,2262.35,2262.35,2262.35,2262.35,2262.35,2262.35,2262.35,2262.35,2262.35,2262.35,2262.35,2262.35,2262.35,2262.35,2262.35,2341.25,2341.25,2341.25,2341.25,2341.25,2341.25,2341.25,2341.25,2341.25,2341.25,2341.25,2341.25,2341.25,2341.25,2341.25,2341.25,2341.25,2341.25,2396.02,2396.02,2396.02,2396.02,2396.02,2396.02,2396.02,2396.02,2396.02,2396.02,2396.02,2396.02,2396.02,2396.02,2396.02,2446.23,2446.23,2446.23,2446.23,2446.23,2446.23,2492.38,2492.38,2492.38,2492.38,2492.38,2492.38,2492.38,2492.38,2492.38,2492.38,2492.38,2492.38,2492.38,2492.38,2492.38,2492.38,2492.38,2492.38,2492.38,2492.38,2492.38,2492.38,2492.38,2492.38,2492.38,2492.38,2492.38,2492.38,2492.38,2492.38,2492.38,2492.38,2492.38,2492.38,2492.38,2492.38,2492.38,2492.38,2492.38,2492.38,2492.38,2492.38,2492.38,2492.38,2492.38,2492.38,2561.33,2561.33,2561.33,2561.33,2561.33,2561.33,2561.33,2561.33,2561.33,2561.33,2561.33,2561.33,2561.33,2561.33,2561.33,2561.33,2561.33,2561.33,2561.33,2561.33,2630.5,2630.5,2630.5,2630.5,2630.5,2630.5,2630.5,2642.41,2642.41,2642.41,2642.41,2642.41,2642.41,2642.41,2642.41,2642.41,2642.41,2642.41,2642.41,2642.41,2642.41,2642.41,2642.41,2642.41,2657.55,2657.55,2657.55,2657.55,2657.55,2657.55,2657.55,2657.55,2657.55,2713.47,2713.47,2713.47,2713.47,2713.47,2713.47,2713.47,2713.47,2713.47,2713.47,2713.47,2713.47,2713.47,2713.47,2713.47,2713.47,2713.47,2713.47,2713.47,2713.47,2713.47,2713.47,2713.47,2713.47,2713.47,2713.47,2713.47,2713.47,2713.47,2713.47,2713.47,2713.47,2713.47,2713.47,2713.47,2713.47,2713.47,2713.47,2713.47,2713.47,2713.47,2713.47,2713.47,2713.47,2713.47,2713.47,2713.47,2713.47,2713.47,2713.47,2713.47,2713.47,2713.47,2713.47,2713.47,2713.47,2713.47,2713.47,2713.47,2713.47,2713.47,2713.47,2713.47,2713.47,2713.47,2713.47,2713.47,2713.47,2713.47,2713.47,2713.47,2713.47,2713.47,2713.47,2713.47,2713.47,2713.47,2713.47,2713.47,2713.47,2713.47,2835.9,2835.9,2835.9,2835.9,2835.9,2883.61,2883.61,2883.61,2906.74,2906.74,2906.74,2906.74,2906.74,2906.74,2906.74,2906.74,2906.74,2906.74,2906.74,2906.74,2906.74,2993.16,2993.16,2993.16,2993.16,2993.16,2993.16,3069.73,3069.73,3069.73,3069.73,3069.73,3069.73,3069.73,3069.73,3069.73,3069.73,3069.73,3069.73,3069.73,3069.73,3069.73,3069.73,3069.73,3069.73,3069.73,3069.73,3069.73,3069.73,3069.73,3069.73,3069.73,3069.73,3069.73,3069.73,3069.73,3069.73,3069.73,3092.24,3092.24,3139.75,3201.23,3201.23,3201.23,3201.23,3201.23,3201.23,3201.23,3201.23,3201.23,3201.23,3201.23,3201.23,3201.23,3201.23,3201.23,3201.23,3201.23,3201.23,3201.23,3201.23,3201.23,3201.23,3201.23,3201.23,3201.23,3201.23,3201.23,3201.23,3201.23,3201.23,3201.23,3201.23,3201.23,3201.23,3201.23,3201.23,3201.23,3201.23,3201.23,3201.23,3201.23,3201.23,3201.23,3201.23,3294.18,3294.18,3294.18,3294.18,3294.18,3294.18,3294.18,3294.18,3294.18,3294.18,3294.18,3294.18,3294.18,3294.18,3294.18,3294.18,3294.18,3294.18,3294.18,3294.18,3294.18,3294.18,3294.18,3294.18,3294.18,3353.11,3353.11,3353.11,3353.11,3353.11,3353.11,3353.11,3353.11,3353.11,3353.11,3353.11,3353.11,3353.11,3353.11,3353.11,3353.11,3353.11,3369.29,3369.29,3369.29,3369.29,3369.29,3468.54,3565.16],\"yaxis\":\"y\",\"type\":\"scattergl\"},{\"hovertemplate\":\"purchased_by=Bob&lt;br&gt;purchased_date=%{x}&lt;br&gt;Cumulative Dollars Spent=%{y}&lt;extra&gt;&lt;/extra&gt;\",\"legendgroup\":\"Bob\",\"line\":{\"color\":\"#EF553B\",\"dash\":\"solid\"},\"marker\":{\"symbol\":\"circle\"},\"mode\":\"lines\",\"name\":\"Bob\",\"showlegend\":true,\"x\":[\"2020-01-01T00:00:00\",\"2020-01-02T00:00:00\",\"2020-01-03T00:00:00\",\"2020-01-04T00:00:00\",\"2020-01-05T00:00:00\",\"2020-01-06T00:00:00\",\"2020-01-07T00:00:00\",\"2020-01-08T00:00:00\",\"2020-01-09T00:00:00\",\"2020-01-10T00:00:00\",\"2020-01-11T00:00:00\",\"2020-01-12T00:00:00\",\"2020-01-13T00:00:00\",\"2020-01-14T00:00:00\",\"2020-01-15T00:00:00\",\"2020-01-16T00:00:00\",\"2020-01-17T00:00:00\",\"2020-01-18T00:00:00\",\"2020-01-19T00:00:00\",\"2020-01-20T00:00:00\",\"2020-01-21T00:00:00\",\"2020-01-22T00:00:00\",\"2020-01-23T00:00:00\",\"2020-01-24T00:00:00\",\"2020-01-25T00:00:00\",\"2020-01-26T00:00:00\",\"2020-01-27T00:00:00\",\"2020-01-28T00:00:00\",\"2020-01-29T00:00:00\",\"2020-01-30T00:00:00\",\"2020-01-31T00:00:00\",\"2020-02-01T00:00:00\",\"2020-02-02T00:00:00\",\"2020-02-03T00:00:00\",\"2020-02-04T00:00:00\",\"2020-02-05T00:00:00\",\"2020-02-06T00:00:00\",\"2020-02-07T00:00:00\",\"2020-02-08T00:00:00\",\"2020-02-09T00:00:00\",\"2020-02-10T00:00:00\",\"2020-02-11T00:00:00\",\"2020-02-12T00:00:00\",\"2020-02-13T00:00:00\",\"2020-02-14T00:00:00\",\"2020-02-15T00:00:00\",\"2020-02-16T00:00:00\",\"2020-02-17T00:00:00\",\"2020-02-18T00:00:00\",\"2020-02-19T00:00:00\",\"2020-02-20T00:00:00\",\"2020-02-21T00:00:00\",\"2020-02-22T00:00:00\",\"2020-02-23T00:00:00\",\"2020-02-24T00:00:00\",\"2020-02-25T00:00:00\",\"2020-02-26T00:00:00\",\"2020-02-27T00:00:00\",\"2020-02-28T00:00:00\",\"2020-02-29T00:00:00\",\"2020-03-01T00:00:00\",\"2020-03-02T00:00:00\",\"2020-03-03T00:00:00\",\"2020-03-04T00:00:00\",\"2020-03-05T00:00:00\",\"2020-03-06T00:00:00\",\"2020-03-07T00:00:00\",\"2020-03-08T00:00:00\",\"2020-03-09T00:00:00\",\"2020-03-10T00:00:00\",\"2020-03-11T00:00:00\",\"2020-03-12T00:00:00\",\"2020-03-13T00:00:00\",\"2020-03-14T00:00:00\",\"2020-03-15T00:00:00\",\"2020-03-16T00:00:00\",\"2020-03-17T00:00:00\",\"2020-03-18T00:00:00\",\"2020-03-19T00:00:00\",\"2020-03-20T00:00:00\",\"2020-03-21T00:00:00\",\"2020-03-22T00:00:00\",\"2020-03-23T00:00:00\",\"2020-03-24T00:00:00\",\"2020-03-25T00:00:00\",\"2020-03-26T00:00:00\",\"2020-03-27T00:00:00\",\"2020-03-28T00:00:00\",\"2020-03-29T00:00:00\",\"2020-03-30T00:00:00\",\"2020-03-31T00:00:00\",\"2020-04-01T00:00:00\",\"2020-04-02T00:00:00\",\"2020-04-03T00:00:00\",\"2020-04-04T00:00:00\",\"2020-04-05T00:00:00\",\"2020-04-06T00:00:00\",\"2020-04-07T00:00:00\",\"2020-04-08T00:00:00\",\"2020-04-09T00:00:00\",\"2020-04-10T00:00:00\",\"2020-04-11T00:00:00\",\"2020-04-12T00:00:00\",\"2020-04-13T00:00:00\",\"2020-04-14T00:00:00\",\"2020-04-15T00:00:00\",\"2020-04-16T00:00:00\",\"2020-04-17T00:00:00\",\"2020-04-18T00:00:00\",\"2020-04-19T00:00:00\",\"2020-04-20T00:00:00\",\"2020-04-21T00:00:00\",\"2020-04-22T00:00:00\",\"2020-04-23T00:00:00\",\"2020-04-24T00:00:00\",\"2020-04-25T00:00:00\",\"2020-04-26T00:00:00\",\"2020-04-27T00:00:00\",\"2020-04-28T00:00:00\",\"2020-04-29T00:00:00\",\"2020-04-30T00:00:00\",\"2020-05-01T00:00:00\",\"2020-05-02T00:00:00\",\"2020-05-03T00:00:00\",\"2020-05-04T00:00:00\",\"2020-05-05T00:00:00\",\"2020-05-06T00:00:00\",\"2020-05-07T00:00:00\",\"2020-05-08T00:00:00\",\"2020-05-09T00:00:00\",\"2020-05-10T00:00:00\",\"2020-05-11T00:00:00\",\"2020-05-12T00:00:00\",\"2020-05-13T00:00:00\",\"2020-05-14T00:00:00\",\"2020-05-15T00:00:00\",\"2020-05-16T00:00:00\",\"2020-05-17T00:00:00\",\"2020-05-18T00:00:00\",\"2020-05-19T00:00:00\",\"2020-05-20T00:00:00\",\"2020-05-21T00:00:00\",\"2020-05-22T00:00:00\",\"2020-05-23T00:00:00\",\"2020-05-24T00:00:00\",\"2020-05-25T00:00:00\",\"2020-05-26T00:00:00\",\"2020-05-27T00:00:00\",\"2020-05-28T00:00:00\",\"2020-05-29T00:00:00\",\"2020-05-30T00:00:00\",\"2020-05-31T00:00:00\",\"2020-06-01T00:00:00\",\"2020-06-02T00:00:00\",\"2020-06-03T00:00:00\",\"2020-06-04T00:00:00\",\"2020-06-05T00:00:00\",\"2020-06-06T00:00:00\",\"2020-06-07T00:00:00\",\"2020-06-08T00:00:00\",\"2020-06-09T00:00:00\",\"2020-06-10T00:00:00\",\"2020-06-11T00:00:00\",\"2020-06-12T00:00:00\",\"2020-06-13T00:00:00\",\"2020-06-14T00:00:00\",\"2020-06-15T00:00:00\",\"2020-06-16T00:00:00\",\"2020-06-17T00:00:00\",\"2020-06-18T00:00:00\",\"2020-06-19T00:00:00\",\"2020-06-20T00:00:00\",\"2020-06-21T00:00:00\",\"2020-06-22T00:00:00\",\"2020-06-23T00:00:00\",\"2020-06-24T00:00:00\",\"2020-06-25T00:00:00\",\"2020-06-26T00:00:00\",\"2020-06-27T00:00:00\",\"2020-06-28T00:00:00\",\"2020-06-29T00:00:00\",\"2020-06-30T00:00:00\",\"2020-07-01T00:00:00\",\"2020-07-02T00:00:00\",\"2020-07-03T00:00:00\",\"2020-07-04T00:00:00\",\"2020-07-05T00:00:00\",\"2020-07-06T00:00:00\",\"2020-07-07T00:00:00\",\"2020-07-08T00:00:00\",\"2020-07-09T00:00:00\",\"2020-07-10T00:00:00\",\"2020-07-11T00:00:00\",\"2020-07-12T00:00:00\",\"2020-07-13T00:00:00\",\"2020-07-14T00:00:00\",\"2020-07-15T00:00:00\",\"2020-07-16T00:00:00\",\"2020-07-17T00:00:00\",\"2020-07-18T00:00:00\",\"2020-07-19T00:00:00\",\"2020-07-20T00:00:00\",\"2020-07-21T00:00:00\",\"2020-07-22T00:00:00\",\"2020-07-23T00:00:00\",\"2020-07-24T00:00:00\",\"2020-07-25T00:00:00\",\"2020-07-26T00:00:00\",\"2020-07-27T00:00:00\",\"2020-07-28T00:00:00\",\"2020-07-29T00:00:00\",\"2020-07-30T00:00:00\",\"2020-07-31T00:00:00\",\"2020-08-01T00:00:00\",\"2020-08-02T00:00:00\",\"2020-08-03T00:00:00\",\"2020-08-04T00:00:00\",\"2020-08-05T00:00:00\",\"2020-08-06T00:00:00\",\"2020-08-07T00:00:00\",\"2020-08-08T00:00:00\",\"2020-08-09T00:00:00\",\"2020-08-10T00:00:00\",\"2020-08-11T00:00:00\",\"2020-08-12T00:00:00\",\"2020-08-13T00:00:00\",\"2020-08-14T00:00:00\",\"2020-08-15T00:00:00\",\"2020-08-16T00:00:00\",\"2020-08-17T00:00:00\",\"2020-08-18T00:00:00\",\"2020-08-19T00:00:00\",\"2020-08-20T00:00:00\",\"2020-08-21T00:00:00\",\"2020-08-22T00:00:00\",\"2020-08-23T00:00:00\",\"2020-08-24T00:00:00\",\"2020-08-25T00:00:00\",\"2020-08-26T00:00:00\",\"2020-08-27T00:00:00\",\"2020-08-28T00:00:00\",\"2020-08-29T00:00:00\",\"2020-08-30T00:00:00\",\"2020-08-31T00:00:00\",\"2020-09-01T00:00:00\",\"2020-09-02T00:00:00\",\"2020-09-03T00:00:00\",\"2020-09-04T00:00:00\",\"2020-09-05T00:00:00\",\"2020-09-06T00:00:00\",\"2020-09-07T00:00:00\",\"2020-09-08T00:00:00\",\"2020-09-09T00:00:00\",\"2020-09-10T00:00:00\",\"2020-09-11T00:00:00\",\"2020-09-12T00:00:00\",\"2020-09-13T00:00:00\",\"2020-09-14T00:00:00\",\"2020-09-15T00:00:00\",\"2020-09-16T00:00:00\",\"2020-09-17T00:00:00\",\"2020-09-18T00:00:00\",\"2020-09-19T00:00:00\",\"2020-09-20T00:00:00\",\"2020-09-21T00:00:00\",\"2020-09-22T00:00:00\",\"2020-09-23T00:00:00\",\"2020-09-24T00:00:00\",\"2020-09-25T00:00:00\",\"2020-09-26T00:00:00\",\"2020-09-27T00:00:00\",\"2020-09-28T00:00:00\",\"2020-09-29T00:00:00\",\"2020-09-30T00:00:00\",\"2020-10-01T00:00:00\",\"2020-10-02T00:00:00\",\"2020-10-03T00:00:00\",\"2020-10-04T00:00:00\",\"2020-10-05T00:00:00\",\"2020-10-06T00:00:00\",\"2020-10-07T00:00:00\",\"2020-10-08T00:00:00\",\"2020-10-09T00:00:00\",\"2020-10-10T00:00:00\",\"2020-10-11T00:00:00\",\"2020-10-12T00:00:00\",\"2020-10-13T00:00:00\",\"2020-10-14T00:00:00\",\"2020-10-15T00:00:00\",\"2020-10-16T00:00:00\",\"2020-10-17T00:00:00\",\"2020-10-18T00:00:00\",\"2020-10-19T00:00:00\",\"2020-10-20T00:00:00\",\"2020-10-21T00:00:00\",\"2020-10-22T00:00:00\",\"2020-10-23T00:00:00\",\"2020-10-24T00:00:00\",\"2020-10-25T00:00:00\",\"2020-10-26T00:00:00\",\"2020-10-27T00:00:00\",\"2020-10-28T00:00:00\",\"2020-10-29T00:00:00\",\"2020-10-30T00:00:00\",\"2020-10-31T00:00:00\",\"2020-11-01T00:00:00\",\"2020-11-02T00:00:00\",\"2020-11-03T00:00:00\",\"2020-11-04T00:00:00\",\"2020-11-05T00:00:00\",\"2020-11-06T00:00:00\",\"2020-11-07T00:00:00\",\"2020-11-08T00:00:00\",\"2020-11-09T00:00:00\",\"2020-11-10T00:00:00\",\"2020-11-11T00:00:00\",\"2020-11-12T00:00:00\",\"2020-11-13T00:00:00\",\"2020-11-14T00:00:00\",\"2020-11-15T00:00:00\",\"2020-11-16T00:00:00\",\"2020-11-17T00:00:00\",\"2020-11-18T00:00:00\",\"2020-11-19T00:00:00\",\"2020-11-20T00:00:00\",\"2020-11-21T00:00:00\",\"2020-11-22T00:00:00\",\"2020-11-23T00:00:00\",\"2020-11-24T00:00:00\",\"2020-11-25T00:00:00\",\"2020-11-26T00:00:00\",\"2020-11-27T00:00:00\",\"2020-11-28T00:00:00\",\"2020-11-29T00:00:00\",\"2020-11-30T00:00:00\",\"2020-12-01T00:00:00\",\"2020-12-02T00:00:00\",\"2020-12-03T00:00:00\",\"2020-12-04T00:00:00\",\"2020-12-05T00:00:00\",\"2020-12-06T00:00:00\",\"2020-12-07T00:00:00\",\"2020-12-08T00:00:00\",\"2020-12-09T00:00:00\",\"2020-12-10T00:00:00\",\"2020-12-11T00:00:00\",\"2020-12-12T00:00:00\",\"2020-12-13T00:00:00\",\"2020-12-14T00:00:00\",\"2020-12-15T00:00:00\",\"2020-12-16T00:00:00\",\"2020-12-17T00:00:00\",\"2020-12-18T00:00:00\",\"2020-12-19T00:00:00\",\"2020-12-20T00:00:00\",\"2020-12-21T00:00:00\",\"2020-12-22T00:00:00\",\"2020-12-23T00:00:00\",\"2020-12-24T00:00:00\",\"2020-12-25T00:00:00\",\"2020-12-26T00:00:00\",\"2020-12-27T00:00:00\",\"2020-12-28T00:00:00\",\"2020-12-29T00:00:00\",\"2020-12-30T00:00:00\",\"2020-12-31T00:00:00\",\"2021-01-01T00:00:00\",\"2021-01-02T00:00:00\",\"2021-01-03T00:00:00\",\"2021-01-04T00:00:00\",\"2021-01-05T00:00:00\",\"2021-01-06T00:00:00\",\"2021-01-07T00:00:00\",\"2021-01-08T00:00:00\",\"2021-01-09T00:00:00\",\"2021-01-10T00:00:00\",\"2021-01-11T00:00:00\",\"2021-01-12T00:00:00\",\"2021-01-13T00:00:00\",\"2021-01-14T00:00:00\",\"2021-01-15T00:00:00\",\"2021-01-16T00:00:00\",\"2021-01-17T00:00:00\",\"2021-01-18T00:00:00\",\"2021-01-19T00:00:00\",\"2021-01-20T00:00:00\",\"2021-01-21T00:00:00\",\"2021-01-22T00:00:00\",\"2021-01-23T00:00:00\",\"2021-01-24T00:00:00\",\"2021-01-25T00:00:00\",\"2021-01-26T00:00:00\",\"2021-01-27T00:00:00\",\"2021-01-28T00:00:00\",\"2021-01-29T00:00:00\",\"2021-01-30T00:00:00\",\"2021-01-31T00:00:00\",\"2021-02-01T00:00:00\",\"2021-02-02T00:00:00\",\"2021-02-03T00:00:00\",\"2021-02-04T00:00:00\",\"2021-02-05T00:00:00\",\"2021-02-06T00:00:00\",\"2021-02-07T00:00:00\",\"2021-02-08T00:00:00\",\"2021-02-09T00:00:00\",\"2021-02-10T00:00:00\",\"2021-02-11T00:00:00\",\"2021-02-12T00:00:00\",\"2021-02-13T00:00:00\",\"2021-02-14T00:00:00\",\"2021-02-15T00:00:00\",\"2021-02-16T00:00:00\",\"2021-02-17T00:00:00\",\"2021-02-18T00:00:00\",\"2021-02-19T00:00:00\",\"2021-02-20T00:00:00\",\"2021-02-21T00:00:00\",\"2021-02-22T00:00:00\",\"2021-02-23T00:00:00\",\"2021-02-24T00:00:00\",\"2021-02-25T00:00:00\",\"2021-02-26T00:00:00\",\"2021-02-27T00:00:00\",\"2021-02-28T00:00:00\",\"2021-03-01T00:00:00\",\"2021-03-02T00:00:00\",\"2021-03-03T00:00:00\",\"2021-03-04T00:00:00\",\"2021-03-05T00:00:00\",\"2021-03-06T00:00:00\",\"2021-03-07T00:00:00\",\"2021-03-08T00:00:00\",\"2021-03-09T00:00:00\",\"2021-03-10T00:00:00\",\"2021-03-11T00:00:00\",\"2021-03-12T00:00:00\",\"2021-03-13T00:00:00\",\"2021-03-14T00:00:00\",\"2021-03-15T00:00:00\",\"2021-03-16T00:00:00\",\"2021-03-17T00:00:00\",\"2021-03-18T00:00:00\",\"2021-03-19T00:00:00\",\"2021-03-20T00:00:00\",\"2021-03-21T00:00:00\",\"2021-03-22T00:00:00\",\"2021-03-23T00:00:00\",\"2021-03-24T00:00:00\",\"2021-03-25T00:00:00\",\"2021-03-26T00:00:00\",\"2021-03-27T00:00:00\",\"2021-03-28T00:00:00\",\"2021-03-29T00:00:00\",\"2021-03-30T00:00:00\",\"2021-03-31T00:00:00\",\"2021-04-01T00:00:00\",\"2021-04-02T00:00:00\",\"2021-04-03T00:00:00\",\"2021-04-04T00:00:00\",\"2021-04-05T00:00:00\",\"2021-04-06T00:00:00\",\"2021-04-07T00:00:00\",\"2021-04-08T00:00:00\",\"2021-04-09T00:00:00\",\"2021-04-10T00:00:00\",\"2021-04-11T00:00:00\",\"2021-04-12T00:00:00\",\"2021-04-13T00:00:00\",\"2021-04-14T00:00:00\",\"2021-04-15T00:00:00\",\"2021-04-16T00:00:00\",\"2021-04-17T00:00:00\",\"2021-04-18T00:00:00\",\"2021-04-19T00:00:00\",\"2021-04-20T00:00:00\",\"2021-04-21T00:00:00\",\"2021-04-22T00:00:00\",\"2021-04-23T00:00:00\",\"2021-04-24T00:00:00\",\"2021-04-25T00:00:00\",\"2021-04-26T00:00:00\",\"2021-04-27T00:00:00\",\"2021-04-28T00:00:00\",\"2021-04-29T00:00:00\",\"2021-04-30T00:00:00\",\"2021-05-01T00:00:00\",\"2021-05-02T00:00:00\",\"2021-05-03T00:00:00\",\"2021-05-04T00:00:00\",\"2021-05-05T00:00:00\",\"2021-05-06T00:00:00\",\"2021-05-07T00:00:00\",\"2021-05-08T00:00:00\",\"2021-05-09T00:00:00\",\"2021-05-10T00:00:00\",\"2021-05-11T00:00:00\",\"2021-05-12T00:00:00\",\"2021-05-13T00:00:00\",\"2021-05-14T00:00:00\",\"2021-05-15T00:00:00\",\"2021-05-16T00:00:00\",\"2021-05-17T00:00:00\",\"2021-05-18T00:00:00\",\"2021-05-19T00:00:00\",\"2021-05-20T00:00:00\",\"2021-05-21T00:00:00\",\"2021-05-22T00:00:00\",\"2021-05-23T00:00:00\",\"2021-05-24T00:00:00\",\"2021-05-25T00:00:00\",\"2021-05-26T00:00:00\",\"2021-05-27T00:00:00\",\"2021-05-28T00:00:00\",\"2021-05-29T00:00:00\",\"2021-05-30T00:00:00\",\"2021-05-31T00:00:00\",\"2021-06-01T00:00:00\",\"2021-06-02T00:00:00\",\"2021-06-03T00:00:00\",\"2021-06-04T00:00:00\",\"2021-06-05T00:00:00\",\"2021-06-06T00:00:00\",\"2021-06-07T00:00:00\",\"2021-06-08T00:00:00\",\"2021-06-09T00:00:00\",\"2021-06-10T00:00:00\",\"2021-06-11T00:00:00\",\"2021-06-12T00:00:00\",\"2021-06-13T00:00:00\",\"2021-06-14T00:00:00\",\"2021-06-15T00:00:00\",\"2021-06-16T00:00:00\",\"2021-06-17T00:00:00\",\"2021-06-18T00:00:00\",\"2021-06-19T00:00:00\",\"2021-06-20T00:00:00\",\"2021-06-21T00:00:00\",\"2021-06-22T00:00:00\",\"2021-06-23T00:00:00\",\"2021-06-24T00:00:00\",\"2021-06-25T00:00:00\",\"2021-06-26T00:00:00\",\"2021-06-27T00:00:00\",\"2021-06-28T00:00:00\",\"2021-06-29T00:00:00\",\"2021-06-30T00:00:00\",\"2021-07-01T00:00:00\",\"2021-07-02T00:00:00\",\"2021-07-03T00:00:00\",\"2021-07-04T00:00:00\",\"2021-07-05T00:00:00\",\"2021-07-06T00:00:00\",\"2021-07-07T00:00:00\",\"2021-07-08T00:00:00\",\"2021-07-09T00:00:00\",\"2021-07-10T00:00:00\",\"2021-07-11T00:00:00\",\"2021-07-12T00:00:00\",\"2021-07-13T00:00:00\",\"2021-07-14T00:00:00\",\"2021-07-15T00:00:00\",\"2021-07-16T00:00:00\",\"2021-07-17T00:00:00\",\"2021-07-18T00:00:00\",\"2021-07-19T00:00:00\",\"2021-07-20T00:00:00\",\"2021-07-21T00:00:00\",\"2021-07-22T00:00:00\",\"2021-07-23T00:00:00\",\"2021-07-24T00:00:00\",\"2021-07-25T00:00:00\",\"2021-07-26T00:00:00\",\"2021-07-27T00:00:00\",\"2021-07-28T00:00:00\",\"2021-07-29T00:00:00\",\"2021-07-30T00:00:00\",\"2021-07-31T00:00:00\",\"2021-08-01T00:00:00\",\"2021-08-02T00:00:00\",\"2021-08-03T00:00:00\",\"2021-08-04T00:00:00\",\"2021-08-05T00:00:00\",\"2021-08-06T00:00:00\",\"2021-08-07T00:00:00\",\"2021-08-08T00:00:00\",\"2021-08-09T00:00:00\",\"2021-08-10T00:00:00\",\"2021-08-11T00:00:00\",\"2021-08-12T00:00:00\",\"2021-08-13T00:00:00\",\"2021-08-14T00:00:00\",\"2021-08-15T00:00:00\",\"2021-08-16T00:00:00\",\"2021-08-17T00:00:00\",\"2021-08-18T00:00:00\",\"2021-08-19T00:00:00\",\"2021-08-20T00:00:00\",\"2021-08-21T00:00:00\",\"2021-08-22T00:00:00\",\"2021-08-23T00:00:00\",\"2021-08-24T00:00:00\",\"2021-08-25T00:00:00\",\"2021-08-26T00:00:00\",\"2021-08-27T00:00:00\",\"2021-08-28T00:00:00\",\"2021-08-29T00:00:00\",\"2021-08-30T00:00:00\",\"2021-08-31T00:00:00\",\"2021-09-01T00:00:00\",\"2021-09-02T00:00:00\",\"2021-09-03T00:00:00\",\"2021-09-04T00:00:00\",\"2021-09-05T00:00:00\",\"2021-09-06T00:00:00\",\"2021-09-07T00:00:00\",\"2021-09-08T00:00:00\",\"2021-09-09T00:00:00\",\"2021-09-10T00:00:00\",\"2021-09-11T00:00:00\",\"2021-09-12T00:00:00\",\"2021-09-13T00:00:00\",\"2021-09-14T00:00:00\",\"2021-09-15T00:00:00\",\"2021-09-16T00:00:00\",\"2021-09-17T00:00:00\",\"2021-09-18T00:00:00\",\"2021-09-19T00:00:00\",\"2021-09-20T00:00:00\",\"2021-09-21T00:00:00\",\"2021-09-22T00:00:00\",\"2021-09-23T00:00:00\",\"2021-09-24T00:00:00\",\"2021-09-25T00:00:00\",\"2021-09-26T00:00:00\",\"2021-09-27T00:00:00\",\"2021-09-28T00:00:00\",\"2021-09-29T00:00:00\",\"2021-09-30T00:00:00\",\"2021-10-01T00:00:00\",\"2021-10-02T00:00:00\",\"2021-10-03T00:00:00\",\"2021-10-04T00:00:00\",\"2021-10-05T00:00:00\",\"2021-10-06T00:00:00\",\"2021-10-07T00:00:00\",\"2021-10-08T00:00:00\",\"2021-10-09T00:00:00\",\"2021-10-10T00:00:00\",\"2021-10-11T00:00:00\",\"2021-10-12T00:00:00\",\"2021-10-13T00:00:00\",\"2021-10-14T00:00:00\",\"2021-10-15T00:00:00\",\"2021-10-16T00:00:00\",\"2021-10-17T00:00:00\",\"2021-10-18T00:00:00\",\"2021-10-19T00:00:00\",\"2021-10-20T00:00:00\",\"2021-10-21T00:00:00\",\"2021-10-22T00:00:00\",\"2021-10-23T00:00:00\",\"2021-10-24T00:00:00\",\"2021-10-25T00:00:00\",\"2021-10-26T00:00:00\",\"2021-10-27T00:00:00\",\"2021-10-28T00:00:00\",\"2021-10-29T00:00:00\",\"2021-10-30T00:00:00\",\"2021-10-31T00:00:00\",\"2021-11-01T00:00:00\",\"2021-11-02T00:00:00\",\"2021-11-03T00:00:00\",\"2021-11-04T00:00:00\",\"2021-11-05T00:00:00\",\"2021-11-06T00:00:00\",\"2021-11-07T00:00:00\",\"2021-11-08T00:00:00\",\"2021-11-09T00:00:00\",\"2021-11-10T00:00:00\",\"2021-11-11T00:00:00\",\"2021-11-12T00:00:00\",\"2021-11-13T00:00:00\",\"2021-11-14T00:00:00\",\"2021-11-15T00:00:00\",\"2021-11-16T00:00:00\",\"2021-11-17T00:00:00\",\"2021-11-18T00:00:00\",\"2021-11-19T00:00:00\",\"2021-11-20T00:00:00\",\"2021-11-21T00:00:00\",\"2021-11-22T00:00:00\",\"2021-11-23T00:00:00\",\"2021-11-24T00:00:00\",\"2021-11-25T00:00:00\",\"2021-11-26T00:00:00\",\"2021-11-27T00:00:00\",\"2021-11-28T00:00:00\",\"2021-11-29T00:00:00\",\"2021-11-30T00:00:00\",\"2021-12-01T00:00:00\",\"2021-12-02T00:00:00\",\"2021-12-03T00:00:00\",\"2021-12-04T00:00:00\",\"2021-12-05T00:00:00\",\"2021-12-06T00:00:00\",\"2021-12-07T00:00:00\",\"2021-12-08T00:00:00\",\"2021-12-09T00:00:00\",\"2021-12-10T00:00:00\",\"2021-12-11T00:00:00\",\"2021-12-12T00:00:00\",\"2021-12-13T00:00:00\",\"2021-12-14T00:00:00\",\"2021-12-15T00:00:00\",\"2021-12-16T00:00:00\",\"2021-12-17T00:00:00\",\"2021-12-18T00:00:00\",\"2021-12-19T00:00:00\",\"2021-12-20T00:00:00\",\"2021-12-21T00:00:00\",\"2021-12-22T00:00:00\",\"2021-12-23T00:00:00\",\"2021-12-24T00:00:00\",\"2021-12-25T00:00:00\",\"2021-12-26T00:00:00\",\"2021-12-27T00:00:00\",\"2021-12-28T00:00:00\",\"2021-12-29T00:00:00\",\"2021-12-30T00:00:00\",\"2021-12-31T00:00:00\",\"2022-01-01T00:00:00\",\"2022-01-02T00:00:00\",\"2022-01-03T00:00:00\",\"2022-01-04T00:00:00\",\"2022-01-05T00:00:00\",\"2022-01-06T00:00:00\",\"2022-01-07T00:00:00\",\"2022-01-08T00:00:00\",\"2022-01-09T00:00:00\",\"2022-01-10T00:00:00\",\"2022-01-11T00:00:00\",\"2022-01-12T00:00:00\",\"2022-01-13T00:00:00\",\"2022-01-14T00:00:00\",\"2022-01-15T00:00:00\",\"2022-01-16T00:00:00\",\"2022-01-17T00:00:00\",\"2022-01-18T00:00:00\",\"2022-01-19T00:00:00\",\"2022-01-20T00:00:00\",\"2022-01-21T00:00:00\",\"2022-01-22T00:00:00\",\"2022-01-23T00:00:00\",\"2022-01-24T00:00:00\",\"2022-01-25T00:00:00\",\"2022-01-26T00:00:00\",\"2022-01-27T00:00:00\",\"2022-01-28T00:00:00\",\"2022-01-29T00:00:00\",\"2022-01-30T00:00:00\",\"2022-01-31T00:00:00\",\"2022-02-01T00:00:00\",\"2022-02-02T00:00:00\",\"2022-02-03T00:00:00\",\"2022-02-04T00:00:00\",\"2022-02-05T00:00:00\",\"2022-02-06T00:00:00\",\"2022-02-07T00:00:00\",\"2022-02-08T00:00:00\",\"2022-02-09T00:00:00\",\"2022-02-10T00:00:00\",\"2022-02-11T00:00:00\",\"2022-02-12T00:00:00\",\"2022-02-13T00:00:00\",\"2022-02-14T00:00:00\",\"2022-02-15T00:00:00\",\"2022-02-16T00:00:00\",\"2022-02-17T00:00:00\",\"2022-02-18T00:00:00\",\"2022-02-19T00:00:00\",\"2022-02-20T00:00:00\",\"2022-02-21T00:00:00\",\"2022-02-22T00:00:00\",\"2022-02-23T00:00:00\",\"2022-02-24T00:00:00\",\"2022-02-25T00:00:00\",\"2022-02-26T00:00:00\",\"2022-02-27T00:00:00\",\"2022-02-28T00:00:00\",\"2022-03-01T00:00:00\",\"2022-03-02T00:00:00\",\"2022-03-03T00:00:00\",\"2022-03-04T00:00:00\",\"2022-03-05T00:00:00\",\"2022-03-06T00:00:00\",\"2022-03-07T00:00:00\",\"2022-03-08T00:00:00\",\"2022-03-09T00:00:00\",\"2022-03-10T00:00:00\",\"2022-03-11T00:00:00\",\"2022-03-12T00:00:00\",\"2022-03-13T00:00:00\",\"2022-03-14T00:00:00\",\"2022-03-15T00:00:00\",\"2022-03-16T00:00:00\",\"2022-03-17T00:00:00\",\"2022-03-18T00:00:00\",\"2022-03-19T00:00:00\",\"2022-03-20T00:00:00\",\"2022-03-21T00:00:00\",\"2022-03-22T00:00:00\",\"2022-03-23T00:00:00\",\"2022-03-24T00:00:00\",\"2022-03-25T00:00:00\",\"2022-03-26T00:00:00\",\"2022-03-27T00:00:00\",\"2022-03-28T00:00:00\",\"2022-03-29T00:00:00\",\"2022-03-30T00:00:00\",\"2022-03-31T00:00:00\",\"2022-04-01T00:00:00\",\"2022-04-02T00:00:00\",\"2022-04-03T00:00:00\",\"2022-04-04T00:00:00\",\"2022-04-05T00:00:00\",\"2022-04-06T00:00:00\",\"2022-04-07T00:00:00\",\"2022-04-08T00:00:00\",\"2022-04-09T00:00:00\",\"2022-04-10T00:00:00\",\"2022-04-11T00:00:00\",\"2022-04-12T00:00:00\",\"2022-04-13T00:00:00\",\"2022-04-14T00:00:00\",\"2022-04-15T00:00:00\",\"2022-04-16T00:00:00\",\"2022-04-17T00:00:00\",\"2022-04-18T00:00:00\",\"2022-04-19T00:00:00\",\"2022-04-20T00:00:00\",\"2022-04-21T00:00:00\",\"2022-04-22T00:00:00\",\"2022-04-23T00:00:00\",\"2022-04-24T00:00:00\",\"2022-04-25T00:00:00\",\"2022-04-26T00:00:00\",\"2022-04-27T00:00:00\",\"2022-04-28T00:00:00\",\"2022-04-29T00:00:00\",\"2022-04-30T00:00:00\",\"2022-05-01T00:00:00\",\"2022-05-02T00:00:00\",\"2022-05-03T00:00:00\",\"2022-05-04T00:00:00\",\"2022-05-05T00:00:00\",\"2022-05-06T00:00:00\",\"2022-05-07T00:00:00\",\"2022-05-08T00:00:00\",\"2022-05-09T00:00:00\",\"2022-05-10T00:00:00\",\"2022-05-11T00:00:00\",\"2022-05-12T00:00:00\",\"2022-05-13T00:00:00\",\"2022-05-14T00:00:00\",\"2022-05-15T00:00:00\",\"2022-05-16T00:00:00\",\"2022-05-17T00:00:00\",\"2022-05-18T00:00:00\",\"2022-05-19T00:00:00\",\"2022-05-20T00:00:00\",\"2022-05-21T00:00:00\",\"2022-05-22T00:00:00\",\"2022-05-23T00:00:00\",\"2022-05-24T00:00:00\",\"2022-05-25T00:00:00\",\"2022-05-26T00:00:00\",\"2022-05-27T00:00:00\",\"2022-05-28T00:00:00\",\"2022-05-29T00:00:00\",\"2022-05-30T00:00:00\",\"2022-05-31T00:00:00\",\"2022-06-01T00:00:00\",\"2022-06-02T00:00:00\",\"2022-06-03T00:00:00\",\"2022-06-04T00:00:00\",\"2022-06-05T00:00:00\",\"2022-06-06T00:00:00\",\"2022-06-07T00:00:00\",\"2022-06-08T00:00:00\",\"2022-06-09T00:00:00\",\"2022-06-10T00:00:00\",\"2022-06-11T00:00:00\",\"2022-06-12T00:00:00\",\"2022-06-13T00:00:00\",\"2022-06-14T00:00:00\",\"2022-06-15T00:00:00\",\"2022-06-16T00:00:00\",\"2022-06-17T00:00:00\",\"2022-06-18T00:00:00\",\"2022-06-19T00:00:00\",\"2022-06-20T00:00:00\",\"2022-06-21T00:00:00\",\"2022-06-22T00:00:00\",\"2022-06-23T00:00:00\",\"2022-06-24T00:00:00\",\"2022-06-25T00:00:00\",\"2022-06-26T00:00:00\",\"2022-06-27T00:00:00\",\"2022-06-28T00:00:00\",\"2022-06-29T00:00:00\",\"2022-06-30T00:00:00\",\"2022-07-01T00:00:00\",\"2022-07-02T00:00:00\",\"2022-07-03T00:00:00\",\"2022-07-04T00:00:00\",\"2022-07-05T00:00:00\",\"2022-07-06T00:00:00\",\"2022-07-07T00:00:00\",\"2022-07-08T00:00:00\",\"2022-07-09T00:00:00\",\"2022-07-10T00:00:00\",\"2022-07-11T00:00:00\",\"2022-07-12T00:00:00\",\"2022-07-13T00:00:00\",\"2022-07-14T00:00:00\",\"2022-07-15T00:00:00\",\"2022-07-16T00:00:00\",\"2022-07-17T00:00:00\",\"2022-07-18T00:00:00\",\"2022-07-19T00:00:00\",\"2022-07-20T00:00:00\",\"2022-07-21T00:00:00\",\"2022-07-22T00:00:00\",\"2022-07-23T00:00:00\",\"2022-07-24T00:00:00\",\"2022-07-25T00:00:00\",\"2022-07-26T00:00:00\",\"2022-07-27T00:00:00\",\"2022-07-28T00:00:00\",\"2022-07-29T00:00:00\",\"2022-07-30T00:00:00\",\"2022-07-31T00:00:00\",\"2022-08-01T00:00:00\",\"2022-08-02T00:00:00\",\"2022-08-03T00:00:00\",\"2022-08-04T00:00:00\",\"2022-08-05T00:00:00\",\"2022-08-06T00:00:00\",\"2022-08-07T00:00:00\",\"2022-08-08T00:00:00\",\"2022-08-09T00:00:00\",\"2022-08-10T00:00:00\",\"2022-08-11T00:00:00\",\"2022-08-12T00:00:00\",\"2022-08-13T00:00:00\",\"2022-08-14T00:00:00\",\"2022-08-15T00:00:00\",\"2022-08-16T00:00:00\",\"2022-08-17T00:00:00\",\"2022-08-18T00:00:00\",\"2022-08-19T00:00:00\",\"2022-08-20T00:00:00\",\"2022-08-21T00:00:00\",\"2022-08-22T00:00:00\",\"2022-08-23T00:00:00\",\"2022-08-24T00:00:00\",\"2022-08-25T00:00:00\",\"2022-08-26T00:00:00\",\"2022-08-27T00:00:00\",\"2022-08-28T00:00:00\",\"2022-08-29T00:00:00\",\"2022-08-30T00:00:00\",\"2022-08-31T00:00:00\",\"2022-09-01T00:00:00\",\"2022-09-02T00:00:00\",\"2022-09-03T00:00:00\",\"2022-09-04T00:00:00\",\"2022-09-05T00:00:00\",\"2022-09-06T00:00:00\",\"2022-09-07T00:00:00\",\"2022-09-08T00:00:00\",\"2022-09-09T00:00:00\",\"2022-09-10T00:00:00\",\"2022-09-11T00:00:00\",\"2022-09-12T00:00:00\",\"2022-09-13T00:00:00\",\"2022-09-14T00:00:00\",\"2022-09-15T00:00:00\",\"2022-09-16T00:00:00\",\"2022-09-17T00:00:00\",\"2022-09-18T00:00:00\",\"2022-09-19T00:00:00\",\"2022-09-20T00:00:00\",\"2022-09-21T00:00:00\",\"2022-09-22T00:00:00\",\"2022-09-23T00:00:00\",\"2022-09-24T00:00:00\",\"2022-09-25T00:00:00\",\"2022-09-26T00:00:00\",\"2022-09-27T00:00:00\",\"2022-09-28T00:00:00\",\"2022-09-29T00:00:00\",\"2022-09-30T00:00:00\",\"2022-10-01T00:00:00\",\"2022-10-02T00:00:00\",\"2022-10-03T00:00:00\",\"2022-10-04T00:00:00\",\"2022-10-05T00:00:00\",\"2022-10-06T00:00:00\",\"2022-10-07T00:00:00\",\"2022-10-08T00:00:00\",\"2022-10-09T00:00:00\",\"2022-10-10T00:00:00\",\"2022-10-11T00:00:00\",\"2022-10-12T00:00:00\",\"2022-10-13T00:00:00\",\"2022-10-14T00:00:00\",\"2022-10-15T00:00:00\",\"2022-10-16T00:00:00\",\"2022-10-17T00:00:00\",\"2022-10-18T00:00:00\",\"2022-10-19T00:00:00\",\"2022-10-20T00:00:00\",\"2022-10-21T00:00:00\",\"2022-10-22T00:00:00\",\"2022-10-23T00:00:00\",\"2022-10-24T00:00:00\",\"2022-10-25T00:00:00\",\"2022-10-26T00:00:00\",\"2022-10-27T00:00:00\",\"2022-10-28T00:00:00\",\"2022-10-29T00:00:00\",\"2022-10-30T00:00:00\",\"2022-10-31T00:00:00\",\"2022-11-01T00:00:00\",\"2022-11-02T00:00:00\",\"2022-11-03T00:00:00\",\"2022-11-04T00:00:00\",\"2022-11-05T00:00:00\",\"2022-11-06T00:00:00\",\"2022-11-07T00:00:00\",\"2022-11-08T00:00:00\",\"2022-11-09T00:00:00\",\"2022-11-10T00:00:00\",\"2022-11-11T00:00:00\",\"2022-11-12T00:00:00\",\"2022-11-13T00:00:00\",\"2022-11-14T00:00:00\",\"2022-11-15T00:00:00\",\"2022-11-16T00:00:00\",\"2022-11-17T00:00:00\",\"2022-11-18T00:00:00\",\"2022-11-19T00:00:00\",\"2022-11-20T00:00:00\",\"2022-11-21T00:00:00\",\"2022-11-22T00:00:00\",\"2022-11-23T00:00:00\",\"2022-11-24T00:00:00\",\"2022-11-25T00:00:00\",\"2022-11-26T00:00:00\",\"2022-11-27T00:00:00\",\"2022-11-28T00:00:00\",\"2022-11-29T00:00:00\",\"2022-11-30T00:00:00\",\"2022-12-01T00:00:00\",\"2022-12-02T00:00:00\",\"2022-12-03T00:00:00\",\"2022-12-04T00:00:00\",\"2022-12-05T00:00:00\",\"2022-12-06T00:00:00\",\"2022-12-07T00:00:00\",\"2022-12-08T00:00:00\",\"2022-12-09T00:00:00\",\"2022-12-10T00:00:00\",\"2022-12-11T00:00:00\",\"2022-12-12T00:00:00\",\"2022-12-13T00:00:00\"],\"xaxis\":\"x\",\"y\":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,70.72,70.72,70.72,70.72,70.72,70.72,70.72,70.72,70.72,70.72,70.72,70.72,70.72,70.72,70.72,70.72,70.72,70.72,70.72,70.72,70.72,70.72,70.72,70.72,70.72,70.72,70.72,70.72,70.72,70.72,70.72,70.72,70.72,70.72,70.72,70.72,70.72,70.72,70.72,70.72,70.72,134.94,134.94,134.94,134.94,134.94,134.94,134.94,134.94,134.94,134.94,134.94,220.14,220.14,220.14,220.14,220.14,220.14,220.14,220.14,220.14,220.14,220.14,220.14,220.14,220.14,220.14,220.14,267.52,267.52,267.52,267.52,267.52,267.52,267.52,267.52,267.52,267.52,267.52,267.52,267.52,267.52,267.52,267.52,267.52,267.52,267.52,267.52,267.52,267.52,267.52,267.52,267.52,267.52,267.52,267.52,267.52,267.52,336.7,336.7,336.7,336.7,336.7,336.7,336.7,336.7,336.7,336.7,336.7,336.7,336.7,336.7,336.7,399.53,425.81,511.71,511.71,511.71,511.71,511.71,511.71,511.71,511.71,511.71,511.71,511.71,511.71,511.71,511.71,511.71,511.71,511.71,511.71,511.71,511.71,511.71,591.93,591.93,591.93,591.93,591.93,591.93,591.93,591.93,591.93,591.93,591.93,591.93,591.93,591.93,591.93,591.93,591.93,591.93,591.93,591.93,591.93,591.93,591.93,591.93,591.93,591.93,591.93,591.93,591.93,591.93,591.93,591.93,591.93,591.93,591.93,591.93,591.93,591.93,591.93,591.93,591.93,591.93,591.93,591.93,591.93,591.93,591.93,591.93,591.93,591.93,591.93,591.93,591.93,591.93,591.93,591.93,591.93,591.93,591.93,591.93,591.93,591.93,591.93,591.93,591.93,591.93,591.93,591.93,591.93,650.89,650.89,650.89,650.89,650.89,650.89,650.89,650.89,650.89,650.89,650.89,650.89,650.89,650.89,650.89,693.42,693.42,693.42,693.42,693.42,693.42,693.42,693.42,693.42,693.42,707.15,707.15,707.15,707.15,707.15,707.15,707.15,707.15,707.15,707.15,707.15,707.15,707.15,707.15,707.15,707.15,707.15,707.15,707.15,707.15,707.15,707.15,707.15,707.15,707.15,707.15,707.15,707.15,707.15,707.15,707.15,707.15,707.15,733.1,733.1,733.1,733.1,733.1,733.1,733.1,733.1,733.1,733.1,733.1,733.1,733.1,733.1,758.94,758.94,758.94,758.94,758.94,758.94,758.94,758.94,758.94,758.94,758.94,758.94,758.94,758.94,758.94,758.94,758.94,758.94,758.94,758.94,758.94,758.94,758.94,758.94,758.94,758.94,758.94,758.94,758.94,758.94,758.94,794.01,794.01,794.01,794.01,794.01,794.01,794.01,794.01,794.01,794.01,794.01,794.01,794.01,794.01,794.01,794.01,794.01,840.03,840.03,840.03,840.03,840.03,840.03,840.03,840.03,840.03,840.03,840.03,840.03,840.03,840.03,840.03,840.03,840.03,901.42,901.42,901.42,901.42,901.42,901.42,901.42,901.42,901.42,901.42,901.42,901.42,901.42,901.42,907.14,907.14,907.14,907.14,907.14,907.14,907.14,961.67,961.67,961.67,961.67,961.67,961.67,961.67,961.67,961.67,961.67,961.67,961.67,961.67,961.67,961.67,961.67,961.67,961.67,961.67,961.67,961.67,961.67,961.67,961.67,961.67,961.67,976.15,976.15,976.15,1040.91,1040.91,1040.91,1040.91,1040.91,1040.91,1040.91,1040.91,1040.91,1040.91,1040.91,1040.91,1040.91,1040.91,1040.91,1040.91,1040.91,1040.91,1040.91,1040.91,1040.91,1040.91,1040.91,1040.91,1040.91,1040.91,1040.91,1040.91,1040.91,1040.91,1040.91,1040.91,1040.91,1040.91,1040.91,1040.91,1040.91,1040.91,1040.91,1040.91,1040.91,1040.91,1040.91,1040.91,1040.91,1040.91,1040.91,1040.91,1040.91,1040.91,1040.91,1040.91,1040.91,1040.91,1040.91,1077.14,1077.14,1077.14,1077.14,1077.14,1077.14,1077.14,1077.14,1077.14,1077.14,1077.14,1077.14,1077.14,1077.14,1077.14,1077.14,1077.14,1077.14,1077.14,1077.14,1077.14,1077.14,1077.14,1077.14,1077.14,1077.14,1077.14,1077.14,1077.14,1077.14,1112.8,1112.8,1112.8,1112.8,1112.8,1112.8,1112.8,1112.8,1112.8,1112.8,1112.8,1112.8,1112.8,1112.8,1112.8,1112.8,1112.8,1112.8,1112.8,1112.8,1112.8,1112.8,1112.8,1112.8,1112.8,1112.8,1112.8,1112.8,1112.8,1112.8,1112.8,1112.8,1112.8,1112.8,1112.8,1112.8,1112.8,1112.8,1112.8,1112.8,1112.8,1112.8,1112.8,1112.8,1112.8,1112.8,1112.8,1112.8,1112.8,1112.8,1112.8,1112.8,1112.8,1112.8,1112.8,1112.8,1112.8,1112.8,1112.8,1112.8,1112.8,1112.8,1112.8,1112.8,1112.8,1199.05,1199.05,1199.05,1199.05,1199.05,1199.05,1199.05,1199.05,1317.2,1317.2,1317.2,1317.2,1317.2,1317.2,1317.2,1317.2,1317.2,1317.2,1317.2,1317.2,1317.2,1317.2,1317.2,1382.47,1382.47,1382.47,1382.47,1416.92,1416.92,1416.92,1416.92,1416.92,1416.92,1416.92,1416.92,1416.92,1416.92,1416.92,1416.92,1416.92,1416.92,1416.92,1416.92,1416.92,1416.92,1416.92,1416.92,1416.92,1416.92,1416.92,1416.92,1416.92,1465.42,1465.42,1465.42,1465.42,1465.42,1465.42,1465.42,1465.42,1465.42,1465.42,1465.42,1465.42,1465.42,1465.42,1465.42,1465.42,1465.42,1465.42,1465.42,1465.42,1465.42,1465.42,1465.42,1465.42,1465.42,1465.42,1465.42,1465.42,1465.42,1465.42,1465.42,1465.42,1465.42,1465.42,1465.42,1465.42,1465.42,1465.42,1510.35,1510.35,1510.35,1510.35,1510.35,1510.35,1516.62,1516.62,1516.62,1516.62,1516.62,1516.62,1516.62,1516.62,1516.62,1516.62,1516.62,1516.62,1516.62,1516.62,1516.62,1516.62,1516.62,1516.62,1516.62,1516.62,1516.62,1516.62,1516.62,1516.62,1516.62,1516.62,1597.59,1597.59,1597.59,1597.59,1669.27,1669.27,1669.27,1669.27,1669.27,1669.27,1669.27,1669.27,1716.73,1716.73,1716.73,1716.73,1716.73,1716.73,1716.73,1716.73,1716.73,1716.73,1716.73,1716.73,1716.73,1751.54,1751.54,1751.54,1751.54,1751.54,1751.54,1751.54,1751.54,1751.54,1751.54,1751.54,1751.54,1751.54,1751.54,1751.54,1751.54,1751.54,1751.54,1751.54,1751.54,1751.54,1751.54,1751.54,1751.54,1751.54,1751.54,1751.54,1751.54,1751.54,1751.54,1862.15,1862.15,1862.15,1862.15,1862.15,1923.67,1923.67,1923.67,1923.67,1923.67,1923.67,1923.67,1923.67,1923.67,1923.67,1923.67,1923.67,1923.67,1923.67,1923.67,1923.67,1923.67,1923.67,1923.67,1923.67,1923.67,1923.67,1923.67,1923.67,1923.67,1923.67,1923.67,1923.67,1923.67,1923.67,1923.67,1923.67,1923.67,1923.67,2005.91,2005.91,2005.91,2005.91,2005.91,2005.91,2005.91,2094.78,2094.78,2094.78,2094.78,2094.78,2094.78,2143.35,2143.35,2198.9,2198.9,2198.9,2198.9,2198.9,2198.9,2198.9,2198.9,2198.9,2198.9,2198.9,2198.9,2198.9,2198.9,2198.9,2198.9,2198.9,2198.9,2198.9,2233.51,2233.51,2233.51,2233.51,2233.51,2233.51,2233.51,2233.51,2233.51,2233.51,2233.51,2233.51,2233.51,2233.51,2233.51,2233.51,2307.44,2307.44,2307.44,2369.09,2369.09,2369.09,2369.09,2369.09,2369.09,2369.09,2369.09,2369.09,2369.09,2369.09,2369.09,2369.09,2369.09,2369.09,2369.09,2369.09,2369.09,2369.09,2369.09,2369.09,2369.09,2369.09,2369.09,2369.09,2369.09,2369.09,2369.09,2369.09,2369.09,2369.09,2369.09,2369.09,2408.14,2408.14,2408.14,2408.14,2408.14,2408.14,2408.14,2408.14,2408.14,2408.14,2408.14,2465.99,2465.99,2465.99,2529.26,2529.26,2529.26,2529.26,2529.26,2529.26,2529.26,2529.26,2529.26,2529.26,2529.26,2529.26,2529.26,2529.26,2529.26,2529.26,2529.26,2529.26,2529.26,2529.26,2529.26,2529.26,2529.26,2529.26,2529.26,2529.26,2576.07,2576.07,2576.07,2576.07,2576.07,2576.07,2576.07,2576.07,2576.07,2576.07,2576.07,2576.07,2576.07,2576.07,2576.07,2576.07,2576.07,2576.07,2576.07,2576.07,2576.07,2576.07,2670.9,2670.9,2670.9,2670.9,2670.9,2670.9,2670.9,2670.9,2670.9,2670.9,2670.9,2670.9,2670.9,2754.61,2754.61,2754.61,2754.61,2754.61,2754.61,2754.61,2757.19,2757.19,2757.19,2757.19,2757.19,2757.19,2757.19,2757.19,2757.19,2757.19,2757.19,2757.19,2757.19,2757.19,2757.19,2757.19,2757.19,2757.19,2757.19,2757.19,2757.19,2757.19,2757.19,2757.19,2757.19,2757.19,2757.19,2757.19,2757.19,2757.19,2757.19,2757.19,2757.19,2757.19,2757.19,2757.19,2757.19,2757.19,2757.19,2773.36,2773.36,2773.36,2773.36,2773.36,2773.36,2773.36,2802.28,2802.28,2802.28,2802.28,2802.28,2802.28,2802.28,2802.28,2802.28,2802.28,2802.28,2802.28,2802.28,2860.3,2860.3,2860.3,2860.3,2860.3,2860.3,2860.3,2860.3,2860.3,2860.3,2860.3,2860.3,2860.3,2860.3,2860.3,2860.3,2860.3,2860.3,2860.3,2860.3,2860.3,2860.3,2860.3,2888.26,2888.26,2888.26,2888.26,2888.26,2888.26,2888.26,2888.26,2888.26,2888.26,2888.26,2890.73,2890.73,2890.73,2890.73,2890.73,2890.73,2890.73,2890.73,2890.73,2948.75,2948.75,2948.75,2948.75,2948.75,2948.75,2948.75,2948.75,2948.75,2948.75,2948.75,2948.75,2948.75,2948.75,2948.75,2948.75,2948.75,2948.75,2948.75,2948.75,2948.75,2948.75,2948.75,2956.94,2956.94,2956.94,2956.94,3019.81,3019.81,3019.81,3019.81],\"yaxis\":\"y\",\"type\":\"scattergl\"},{\"hovertemplate\":\"purchased_by=Chuck&lt;br&gt;purchased_date=%{x}&lt;br&gt;Cumulative Dollars Spent=%{y}&lt;extra&gt;&lt;/extra&gt;\",\"legendgroup\":\"Chuck\",\"line\":{\"color\":\"#00cc96\",\"dash\":\"solid\"},\"marker\":{\"symbol\":\"circle\"},\"mode\":\"lines\",\"name\":\"Chuck\",\"showlegend\":true,\"x\":[\"2020-01-01T00:00:00\",\"2020-01-02T00:00:00\",\"2020-01-03T00:00:00\",\"2020-01-04T00:00:00\",\"2020-01-05T00:00:00\",\"2020-01-06T00:00:00\",\"2020-01-07T00:00:00\",\"2020-01-08T00:00:00\",\"2020-01-09T00:00:00\",\"2020-01-10T00:00:00\",\"2020-01-11T00:00:00\",\"2020-01-12T00:00:00\",\"2020-01-13T00:00:00\",\"2020-01-14T00:00:00\",\"2020-01-15T00:00:00\",\"2020-01-16T00:00:00\",\"2020-01-17T00:00:00\",\"2020-01-18T00:00:00\",\"2020-01-19T00:00:00\",\"2020-01-20T00:00:00\",\"2020-01-21T00:00:00\",\"2020-01-22T00:00:00\",\"2020-01-23T00:00:00\",\"2020-01-24T00:00:00\",\"2020-01-25T00:00:00\",\"2020-01-26T00:00:00\",\"2020-01-27T00:00:00\",\"2020-01-28T00:00:00\",\"2020-01-29T00:00:00\",\"2020-01-30T00:00:00\",\"2020-01-31T00:00:00\",\"2020-02-01T00:00:00\",\"2020-02-02T00:00:00\",\"2020-02-03T00:00:00\",\"2020-02-04T00:00:00\",\"2020-02-05T00:00:00\",\"2020-02-06T00:00:00\",\"2020-02-07T00:00:00\",\"2020-02-08T00:00:00\",\"2020-02-09T00:00:00\",\"2020-02-10T00:00:00\",\"2020-02-11T00:00:00\",\"2020-02-12T00:00:00\",\"2020-02-13T00:00:00\",\"2020-02-14T00:00:00\",\"2020-02-15T00:00:00\",\"2020-02-16T00:00:00\",\"2020-02-17T00:00:00\",\"2020-02-18T00:00:00\",\"2020-02-19T00:00:00\",\"2020-02-20T00:00:00\",\"2020-02-21T00:00:00\",\"2020-02-22T00:00:00\",\"2020-02-23T00:00:00\",\"2020-02-24T00:00:00\",\"2020-02-25T00:00:00\",\"2020-02-26T00:00:00\",\"2020-02-27T00:00:00\",\"2020-02-28T00:00:00\",\"2020-02-29T00:00:00\",\"2020-03-01T00:00:00\",\"2020-03-02T00:00:00\",\"2020-03-03T00:00:00\",\"2020-03-04T00:00:00\",\"2020-03-05T00:00:00\",\"2020-03-06T00:00:00\",\"2020-03-07T00:00:00\",\"2020-03-08T00:00:00\",\"2020-03-09T00:00:00\",\"2020-03-10T00:00:00\",\"2020-03-11T00:00:00\",\"2020-03-12T00:00:00\",\"2020-03-13T00:00:00\",\"2020-03-14T00:00:00\",\"2020-03-15T00:00:00\",\"2020-03-16T00:00:00\",\"2020-03-17T00:00:00\",\"2020-03-18T00:00:00\",\"2020-03-19T00:00:00\",\"2020-03-20T00:00:00\",\"2020-03-21T00:00:00\",\"2020-03-22T00:00:00\",\"2020-03-23T00:00:00\",\"2020-03-24T00:00:00\",\"2020-03-25T00:00:00\",\"2020-03-26T00:00:00\",\"2020-03-27T00:00:00\",\"2020-03-28T00:00:00\",\"2020-03-29T00:00:00\",\"2020-03-30T00:00:00\",\"2020-03-31T00:00:00\",\"2020-04-01T00:00:00\",\"2020-04-02T00:00:00\",\"2020-04-03T00:00:00\",\"2020-04-04T00:00:00\",\"2020-04-05T00:00:00\",\"2020-04-06T00:00:00\",\"2020-04-07T00:00:00\",\"2020-04-08T00:00:00\",\"2020-04-09T00:00:00\",\"2020-04-10T00:00:00\",\"2020-04-11T00:00:00\",\"2020-04-12T00:00:00\",\"2020-04-13T00:00:00\",\"2020-04-14T00:00:00\",\"2020-04-15T00:00:00\",\"2020-04-16T00:00:00\",\"2020-04-17T00:00:00\",\"2020-04-18T00:00:00\",\"2020-04-19T00:00:00\",\"2020-04-20T00:00:00\",\"2020-04-21T00:00:00\",\"2020-04-22T00:00:00\",\"2020-04-23T00:00:00\",\"2020-04-24T00:00:00\",\"2020-04-25T00:00:00\",\"2020-04-26T00:00:00\",\"2020-04-27T00:00:00\",\"2020-04-28T00:00:00\",\"2020-04-29T00:00:00\",\"2020-04-30T00:00:00\",\"2020-05-01T00:00:00\",\"2020-05-02T00:00:00\",\"2020-05-03T00:00:00\",\"2020-05-04T00:00:00\",\"2020-05-05T00:00:00\",\"2020-05-06T00:00:00\",\"2020-05-07T00:00:00\",\"2020-05-08T00:00:00\",\"2020-05-09T00:00:00\",\"2020-05-10T00:00:00\",\"2020-05-11T00:00:00\",\"2020-05-12T00:00:00\",\"2020-05-13T00:00:00\",\"2020-05-14T00:00:00\",\"2020-05-15T00:00:00\",\"2020-05-16T00:00:00\",\"2020-05-17T00:00:00\",\"2020-05-18T00:00:00\",\"2020-05-19T00:00:00\",\"2020-05-20T00:00:00\",\"2020-05-21T00:00:00\",\"2020-05-22T00:00:00\",\"2020-05-23T00:00:00\",\"2020-05-24T00:00:00\",\"2020-05-25T00:00:00\",\"2020-05-26T00:00:00\",\"2020-05-27T00:00:00\",\"2020-05-28T00:00:00\",\"2020-05-29T00:00:00\",\"2020-05-30T00:00:00\",\"2020-05-31T00:00:00\",\"2020-06-01T00:00:00\",\"2020-06-02T00:00:00\",\"2020-06-03T00:00:00\",\"2020-06-04T00:00:00\",\"2020-06-05T00:00:00\",\"2020-06-06T00:00:00\",\"2020-06-07T00:00:00\",\"2020-06-08T00:00:00\",\"2020-06-09T00:00:00\",\"2020-06-10T00:00:00\",\"2020-06-11T00:00:00\",\"2020-06-12T00:00:00\",\"2020-06-13T00:00:00\",\"2020-06-14T00:00:00\",\"2020-06-15T00:00:00\",\"2020-06-16T00:00:00\",\"2020-06-17T00:00:00\",\"2020-06-18T00:00:00\",\"2020-06-19T00:00:00\",\"2020-06-20T00:00:00\",\"2020-06-21T00:00:00\",\"2020-06-22T00:00:00\",\"2020-06-23T00:00:00\",\"2020-06-24T00:00:00\",\"2020-06-25T00:00:00\",\"2020-06-26T00:00:00\",\"2020-06-27T00:00:00\",\"2020-06-28T00:00:00\",\"2020-06-29T00:00:00\",\"2020-06-30T00:00:00\",\"2020-07-01T00:00:00\",\"2020-07-02T00:00:00\",\"2020-07-03T00:00:00\",\"2020-07-04T00:00:00\",\"2020-07-05T00:00:00\",\"2020-07-06T00:00:00\",\"2020-07-07T00:00:00\",\"2020-07-08T00:00:00\",\"2020-07-09T00:00:00\",\"2020-07-10T00:00:00\",\"2020-07-11T00:00:00\",\"2020-07-12T00:00:00\",\"2020-07-13T00:00:00\",\"2020-07-14T00:00:00\",\"2020-07-15T00:00:00\",\"2020-07-16T00:00:00\",\"2020-07-17T00:00:00\",\"2020-07-18T00:00:00\",\"2020-07-19T00:00:00\",\"2020-07-20T00:00:00\",\"2020-07-21T00:00:00\",\"2020-07-22T00:00:00\",\"2020-07-23T00:00:00\",\"2020-07-24T00:00:00\",\"2020-07-25T00:00:00\",\"2020-07-26T00:00:00\",\"2020-07-27T00:00:00\",\"2020-07-28T00:00:00\",\"2020-07-29T00:00:00\",\"2020-07-30T00:00:00\",\"2020-07-31T00:00:00\",\"2020-08-01T00:00:00\",\"2020-08-02T00:00:00\",\"2020-08-03T00:00:00\",\"2020-08-04T00:00:00\",\"2020-08-05T00:00:00\",\"2020-08-06T00:00:00\",\"2020-08-07T00:00:00\",\"2020-08-08T00:00:00\",\"2020-08-09T00:00:00\",\"2020-08-10T00:00:00\",\"2020-08-11T00:00:00\",\"2020-08-12T00:00:00\",\"2020-08-13T00:00:00\",\"2020-08-14T00:00:00\",\"2020-08-15T00:00:00\",\"2020-08-16T00:00:00\",\"2020-08-17T00:00:00\",\"2020-08-18T00:00:00\",\"2020-08-19T00:00:00\",\"2020-08-20T00:00:00\",\"2020-08-21T00:00:00\",\"2020-08-22T00:00:00\",\"2020-08-23T00:00:00\",\"2020-08-24T00:00:00\",\"2020-08-25T00:00:00\",\"2020-08-26T00:00:00\",\"2020-08-27T00:00:00\",\"2020-08-28T00:00:00\",\"2020-08-29T00:00:00\",\"2020-08-30T00:00:00\",\"2020-08-31T00:00:00\",\"2020-09-01T00:00:00\",\"2020-09-02T00:00:00\",\"2020-09-03T00:00:00\",\"2020-09-04T00:00:00\",\"2020-09-05T00:00:00\",\"2020-09-06T00:00:00\",\"2020-09-07T00:00:00\",\"2020-09-08T00:00:00\",\"2020-09-09T00:00:00\",\"2020-09-10T00:00:00\",\"2020-09-11T00:00:00\",\"2020-09-12T00:00:00\",\"2020-09-13T00:00:00\",\"2020-09-14T00:00:00\",\"2020-09-15T00:00:00\",\"2020-09-16T00:00:00\",\"2020-09-17T00:00:00\",\"2020-09-18T00:00:00\",\"2020-09-19T00:00:00\",\"2020-09-20T00:00:00\",\"2020-09-21T00:00:00\",\"2020-09-22T00:00:00\",\"2020-09-23T00:00:00\",\"2020-09-24T00:00:00\",\"2020-09-25T00:00:00\",\"2020-09-26T00:00:00\",\"2020-09-27T00:00:00\",\"2020-09-28T00:00:00\",\"2020-09-29T00:00:00\",\"2020-09-30T00:00:00\",\"2020-10-01T00:00:00\",\"2020-10-02T00:00:00\",\"2020-10-03T00:00:00\",\"2020-10-04T00:00:00\",\"2020-10-05T00:00:00\",\"2020-10-06T00:00:00\",\"2020-10-07T00:00:00\",\"2020-10-08T00:00:00\",\"2020-10-09T00:00:00\",\"2020-10-10T00:00:00\",\"2020-10-11T00:00:00\",\"2020-10-12T00:00:00\",\"2020-10-13T00:00:00\",\"2020-10-14T00:00:00\",\"2020-10-15T00:00:00\",\"2020-10-16T00:00:00\",\"2020-10-17T00:00:00\",\"2020-10-18T00:00:00\",\"2020-10-19T00:00:00\",\"2020-10-20T00:00:00\",\"2020-10-21T00:00:00\",\"2020-10-22T00:00:00\",\"2020-10-23T00:00:00\",\"2020-10-24T00:00:00\",\"2020-10-25T00:00:00\",\"2020-10-26T00:00:00\",\"2020-10-27T00:00:00\",\"2020-10-28T00:00:00\",\"2020-10-29T00:00:00\",\"2020-10-30T00:00:00\",\"2020-10-31T00:00:00\",\"2020-11-01T00:00:00\",\"2020-11-02T00:00:00\",\"2020-11-03T00:00:00\",\"2020-11-04T00:00:00\",\"2020-11-05T00:00:00\",\"2020-11-06T00:00:00\",\"2020-11-07T00:00:00\",\"2020-11-08T00:00:00\",\"2020-11-09T00:00:00\",\"2020-11-10T00:00:00\",\"2020-11-11T00:00:00\",\"2020-11-12T00:00:00\",\"2020-11-13T00:00:00\",\"2020-11-14T00:00:00\",\"2020-11-15T00:00:00\",\"2020-11-16T00:00:00\",\"2020-11-17T00:00:00\",\"2020-11-18T00:00:00\",\"2020-11-19T00:00:00\",\"2020-11-20T00:00:00\",\"2020-11-21T00:00:00\",\"2020-11-22T00:00:00\",\"2020-11-23T00:00:00\",\"2020-11-24T00:00:00\",\"2020-11-25T00:00:00\",\"2020-11-26T00:00:00\",\"2020-11-27T00:00:00\",\"2020-11-28T00:00:00\",\"2020-11-29T00:00:00\",\"2020-11-30T00:00:00\",\"2020-12-01T00:00:00\",\"2020-12-02T00:00:00\",\"2020-12-03T00:00:00\",\"2020-12-04T00:00:00\",\"2020-12-05T00:00:00\",\"2020-12-06T00:00:00\",\"2020-12-07T00:00:00\",\"2020-12-08T00:00:00\",\"2020-12-09T00:00:00\",\"2020-12-10T00:00:00\",\"2020-12-11T00:00:00\",\"2020-12-12T00:00:00\",\"2020-12-13T00:00:00\",\"2020-12-14T00:00:00\",\"2020-12-15T00:00:00\",\"2020-12-16T00:00:00\",\"2020-12-17T00:00:00\",\"2020-12-18T00:00:00\",\"2020-12-19T00:00:00\",\"2020-12-20T00:00:00\",\"2020-12-21T00:00:00\",\"2020-12-22T00:00:00\",\"2020-12-23T00:00:00\",\"2020-12-24T00:00:00\",\"2020-12-25T00:00:00\",\"2020-12-26T00:00:00\",\"2020-12-27T00:00:00\",\"2020-12-28T00:00:00\",\"2020-12-29T00:00:00\",\"2020-12-30T00:00:00\",\"2020-12-31T00:00:00\",\"2021-01-01T00:00:00\",\"2021-01-02T00:00:00\",\"2021-01-03T00:00:00\",\"2021-01-04T00:00:00\",\"2021-01-05T00:00:00\",\"2021-01-06T00:00:00\",\"2021-01-07T00:00:00\",\"2021-01-08T00:00:00\",\"2021-01-09T00:00:00\",\"2021-01-10T00:00:00\",\"2021-01-11T00:00:00\",\"2021-01-12T00:00:00\",\"2021-01-13T00:00:00\",\"2021-01-14T00:00:00\",\"2021-01-15T00:00:00\",\"2021-01-16T00:00:00\",\"2021-01-17T00:00:00\",\"2021-01-18T00:00:00\",\"2021-01-19T00:00:00\",\"2021-01-20T00:00:00\",\"2021-01-21T00:00:00\",\"2021-01-22T00:00:00\",\"2021-01-23T00:00:00\",\"2021-01-24T00:00:00\",\"2021-01-25T00:00:00\",\"2021-01-26T00:00:00\",\"2021-01-27T00:00:00\",\"2021-01-28T00:00:00\",\"2021-01-29T00:00:00\",\"2021-01-30T00:00:00\",\"2021-01-31T00:00:00\",\"2021-02-01T00:00:00\",\"2021-02-02T00:00:00\",\"2021-02-03T00:00:00\",\"2021-02-04T00:00:00\",\"2021-02-05T00:00:00\",\"2021-02-06T00:00:00\",\"2021-02-07T00:00:00\",\"2021-02-08T00:00:00\",\"2021-02-09T00:00:00\",\"2021-02-10T00:00:00\",\"2021-02-11T00:00:00\",\"2021-02-12T00:00:00\",\"2021-02-13T00:00:00\",\"2021-02-14T00:00:00\",\"2021-02-15T00:00:00\",\"2021-02-16T00:00:00\",\"2021-02-17T00:00:00\",\"2021-02-18T00:00:00\",\"2021-02-19T00:00:00\",\"2021-02-20T00:00:00\",\"2021-02-21T00:00:00\",\"2021-02-22T00:00:00\",\"2021-02-23T00:00:00\",\"2021-02-24T00:00:00\",\"2021-02-25T00:00:00\",\"2021-02-26T00:00:00\",\"2021-02-27T00:00:00\",\"2021-02-28T00:00:00\",\"2021-03-01T00:00:00\",\"2021-03-02T00:00:00\",\"2021-03-03T00:00:00\",\"2021-03-04T00:00:00\",\"2021-03-05T00:00:00\",\"2021-03-06T00:00:00\",\"2021-03-07T00:00:00\",\"2021-03-08T00:00:00\",\"2021-03-09T00:00:00\",\"2021-03-10T00:00:00\",\"2021-03-11T00:00:00\",\"2021-03-12T00:00:00\",\"2021-03-13T00:00:00\",\"2021-03-14T00:00:00\",\"2021-03-15T00:00:00\",\"2021-03-16T00:00:00\",\"2021-03-17T00:00:00\",\"2021-03-18T00:00:00\",\"2021-03-19T00:00:00\",\"2021-03-20T00:00:00\",\"2021-03-21T00:00:00\",\"2021-03-22T00:00:00\",\"2021-03-23T00:00:00\",\"2021-03-24T00:00:00\",\"2021-03-25T00:00:00\",\"2021-03-26T00:00:00\",\"2021-03-27T00:00:00\",\"2021-03-28T00:00:00\",\"2021-03-29T00:00:00\",\"2021-03-30T00:00:00\",\"2021-03-31T00:00:00\",\"2021-04-01T00:00:00\",\"2021-04-02T00:00:00\",\"2021-04-03T00:00:00\",\"2021-04-04T00:00:00\",\"2021-04-05T00:00:00\",\"2021-04-06T00:00:00\",\"2021-04-07T00:00:00\",\"2021-04-08T00:00:00\",\"2021-04-09T00:00:00\",\"2021-04-10T00:00:00\",\"2021-04-11T00:00:00\",\"2021-04-12T00:00:00\",\"2021-04-13T00:00:00\",\"2021-04-14T00:00:00\",\"2021-04-15T00:00:00\",\"2021-04-16T00:00:00\",\"2021-04-17T00:00:00\",\"2021-04-18T00:00:00\",\"2021-04-19T00:00:00\",\"2021-04-20T00:00:00\",\"2021-04-21T00:00:00\",\"2021-04-22T00:00:00\",\"2021-04-23T00:00:00\",\"2021-04-24T00:00:00\",\"2021-04-25T00:00:00\",\"2021-04-26T00:00:00\",\"2021-04-27T00:00:00\",\"2021-04-28T00:00:00\",\"2021-04-29T00:00:00\",\"2021-04-30T00:00:00\",\"2021-05-01T00:00:00\",\"2021-05-02T00:00:00\",\"2021-05-03T00:00:00\",\"2021-05-04T00:00:00\",\"2021-05-05T00:00:00\",\"2021-05-06T00:00:00\",\"2021-05-07T00:00:00\",\"2021-05-08T00:00:00\",\"2021-05-09T00:00:00\",\"2021-05-10T00:00:00\",\"2021-05-11T00:00:00\",\"2021-05-12T00:00:00\",\"2021-05-13T00:00:00\",\"2021-05-14T00:00:00\",\"2021-05-15T00:00:00\",\"2021-05-16T00:00:00\",\"2021-05-17T00:00:00\",\"2021-05-18T00:00:00\",\"2021-05-19T00:00:00\",\"2021-05-20T00:00:00\",\"2021-05-21T00:00:00\",\"2021-05-22T00:00:00\",\"2021-05-23T00:00:00\",\"2021-05-24T00:00:00\",\"2021-05-25T00:00:00\",\"2021-05-26T00:00:00\",\"2021-05-27T00:00:00\",\"2021-05-28T00:00:00\",\"2021-05-29T00:00:00\",\"2021-05-30T00:00:00\",\"2021-05-31T00:00:00\",\"2021-06-01T00:00:00\",\"2021-06-02T00:00:00\",\"2021-06-03T00:00:00\",\"2021-06-04T00:00:00\",\"2021-06-05T00:00:00\",\"2021-06-06T00:00:00\",\"2021-06-07T00:00:00\",\"2021-06-08T00:00:00\",\"2021-06-09T00:00:00\",\"2021-06-10T00:00:00\",\"2021-06-11T00:00:00\",\"2021-06-12T00:00:00\",\"2021-06-13T00:00:00\",\"2021-06-14T00:00:00\",\"2021-06-15T00:00:00\",\"2021-06-16T00:00:00\",\"2021-06-17T00:00:00\",\"2021-06-18T00:00:00\",\"2021-06-19T00:00:00\",\"2021-06-20T00:00:00\",\"2021-06-21T00:00:00\",\"2021-06-22T00:00:00\",\"2021-06-23T00:00:00\",\"2021-06-24T00:00:00\",\"2021-06-25T00:00:00\",\"2021-06-26T00:00:00\",\"2021-06-27T00:00:00\",\"2021-06-28T00:00:00\",\"2021-06-29T00:00:00\",\"2021-06-30T00:00:00\",\"2021-07-01T00:00:00\",\"2021-07-02T00:00:00\",\"2021-07-03T00:00:00\",\"2021-07-04T00:00:00\",\"2021-07-05T00:00:00\",\"2021-07-06T00:00:00\",\"2021-07-07T00:00:00\",\"2021-07-08T00:00:00\",\"2021-07-09T00:00:00\",\"2021-07-10T00:00:00\",\"2021-07-11T00:00:00\",\"2021-07-12T00:00:00\",\"2021-07-13T00:00:00\",\"2021-07-14T00:00:00\",\"2021-07-15T00:00:00\",\"2021-07-16T00:00:00\",\"2021-07-17T00:00:00\",\"2021-07-18T00:00:00\",\"2021-07-19T00:00:00\",\"2021-07-20T00:00:00\",\"2021-07-21T00:00:00\",\"2021-07-22T00:00:00\",\"2021-07-23T00:00:00\",\"2021-07-24T00:00:00\",\"2021-07-25T00:00:00\",\"2021-07-26T00:00:00\",\"2021-07-27T00:00:00\",\"2021-07-28T00:00:00\",\"2021-07-29T00:00:00\",\"2021-07-30T00:00:00\",\"2021-07-31T00:00:00\",\"2021-08-01T00:00:00\",\"2021-08-02T00:00:00\",\"2021-08-03T00:00:00\",\"2021-08-04T00:00:00\",\"2021-08-05T00:00:00\",\"2021-08-06T00:00:00\",\"2021-08-07T00:00:00\",\"2021-08-08T00:00:00\",\"2021-08-09T00:00:00\",\"2021-08-10T00:00:00\",\"2021-08-11T00:00:00\",\"2021-08-12T00:00:00\",\"2021-08-13T00:00:00\",\"2021-08-14T00:00:00\",\"2021-08-15T00:00:00\",\"2021-08-16T00:00:00\",\"2021-08-17T00:00:00\",\"2021-08-18T00:00:00\",\"2021-08-19T00:00:00\",\"2021-08-20T00:00:00\",\"2021-08-21T00:00:00\",\"2021-08-22T00:00:00\",\"2021-08-23T00:00:00\",\"2021-08-24T00:00:00\",\"2021-08-25T00:00:00\",\"2021-08-26T00:00:00\",\"2021-08-27T00:00:00\",\"2021-08-28T00:00:00\",\"2021-08-29T00:00:00\",\"2021-08-30T00:00:00\",\"2021-08-31T00:00:00\",\"2021-09-01T00:00:00\",\"2021-09-02T00:00:00\",\"2021-09-03T00:00:00\",\"2021-09-04T00:00:00\",\"2021-09-05T00:00:00\",\"2021-09-06T00:00:00\",\"2021-09-07T00:00:00\",\"2021-09-08T00:00:00\",\"2021-09-09T00:00:00\",\"2021-09-10T00:00:00\",\"2021-09-11T00:00:00\",\"2021-09-12T00:00:00\",\"2021-09-13T00:00:00\",\"2021-09-14T00:00:00\",\"2021-09-15T00:00:00\",\"2021-09-16T00:00:00\",\"2021-09-17T00:00:00\",\"2021-09-18T00:00:00\",\"2021-09-19T00:00:00\",\"2021-09-20T00:00:00\",\"2021-09-21T00:00:00\",\"2021-09-22T00:00:00\",\"2021-09-23T00:00:00\",\"2021-09-24T00:00:00\",\"2021-09-25T00:00:00\",\"2021-09-26T00:00:00\",\"2021-09-27T00:00:00\",\"2021-09-28T00:00:00\",\"2021-09-29T00:00:00\",\"2021-09-30T00:00:00\",\"2021-10-01T00:00:00\",\"2021-10-02T00:00:00\",\"2021-10-03T00:00:00\",\"2021-10-04T00:00:00\",\"2021-10-05T00:00:00\",\"2021-10-06T00:00:00\",\"2021-10-07T00:00:00\",\"2021-10-08T00:00:00\",\"2021-10-09T00:00:00\",\"2021-10-10T00:00:00\",\"2021-10-11T00:00:00\",\"2021-10-12T00:00:00\",\"2021-10-13T00:00:00\",\"2021-10-14T00:00:00\",\"2021-10-15T00:00:00\",\"2021-10-16T00:00:00\",\"2021-10-17T00:00:00\",\"2021-10-18T00:00:00\",\"2021-10-19T00:00:00\",\"2021-10-20T00:00:00\",\"2021-10-21T00:00:00\",\"2021-10-22T00:00:00\",\"2021-10-23T00:00:00\",\"2021-10-24T00:00:00\",\"2021-10-25T00:00:00\",\"2021-10-26T00:00:00\",\"2021-10-27T00:00:00\",\"2021-10-28T00:00:00\",\"2021-10-29T00:00:00\",\"2021-10-30T00:00:00\",\"2021-10-31T00:00:00\",\"2021-11-01T00:00:00\",\"2021-11-02T00:00:00\",\"2021-11-03T00:00:00\",\"2021-11-04T00:00:00\",\"2021-11-05T00:00:00\",\"2021-11-06T00:00:00\",\"2021-11-07T00:00:00\",\"2021-11-08T00:00:00\",\"2021-11-09T00:00:00\",\"2021-11-10T00:00:00\",\"2021-11-11T00:00:00\",\"2021-11-12T00:00:00\",\"2021-11-13T00:00:00\",\"2021-11-14T00:00:00\",\"2021-11-15T00:00:00\",\"2021-11-16T00:00:00\",\"2021-11-17T00:00:00\",\"2021-11-18T00:00:00\",\"2021-11-19T00:00:00\",\"2021-11-20T00:00:00\",\"2021-11-21T00:00:00\",\"2021-11-22T00:00:00\",\"2021-11-23T00:00:00\",\"2021-11-24T00:00:00\",\"2021-11-25T00:00:00\",\"2021-11-26T00:00:00\",\"2021-11-27T00:00:00\",\"2021-11-28T00:00:00\",\"2021-11-29T00:00:00\",\"2021-11-30T00:00:00\",\"2021-12-01T00:00:00\",\"2021-12-02T00:00:00\",\"2021-12-03T00:00:00\",\"2021-12-04T00:00:00\",\"2021-12-05T00:00:00\",\"2021-12-06T00:00:00\",\"2021-12-07T00:00:00\",\"2021-12-08T00:00:00\",\"2021-12-09T00:00:00\",\"2021-12-10T00:00:00\",\"2021-12-11T00:00:00\",\"2021-12-12T00:00:00\",\"2021-12-13T00:00:00\",\"2021-12-14T00:00:00\",\"2021-12-15T00:00:00\",\"2021-12-16T00:00:00\",\"2021-12-17T00:00:00\",\"2021-12-18T00:00:00\",\"2021-12-19T00:00:00\",\"2021-12-20T00:00:00\",\"2021-12-21T00:00:00\",\"2021-12-22T00:00:00\",\"2021-12-23T00:00:00\",\"2021-12-24T00:00:00\",\"2021-12-25T00:00:00\",\"2021-12-26T00:00:00\",\"2021-12-27T00:00:00\",\"2021-12-28T00:00:00\",\"2021-12-29T00:00:00\",\"2021-12-30T00:00:00\",\"2021-12-31T00:00:00\",\"2022-01-01T00:00:00\",\"2022-01-02T00:00:00\",\"2022-01-03T00:00:00\",\"2022-01-04T00:00:00\",\"2022-01-05T00:00:00\",\"2022-01-06T00:00:00\",\"2022-01-07T00:00:00\",\"2022-01-08T00:00:00\",\"2022-01-09T00:00:00\",\"2022-01-10T00:00:00\",\"2022-01-11T00:00:00\",\"2022-01-12T00:00:00\",\"2022-01-13T00:00:00\",\"2022-01-14T00:00:00\",\"2022-01-15T00:00:00\",\"2022-01-16T00:00:00\",\"2022-01-17T00:00:00\",\"2022-01-18T00:00:00\",\"2022-01-19T00:00:00\",\"2022-01-20T00:00:00\",\"2022-01-21T00:00:00\",\"2022-01-22T00:00:00\",\"2022-01-23T00:00:00\",\"2022-01-24T00:00:00\",\"2022-01-25T00:00:00\",\"2022-01-26T00:00:00\",\"2022-01-27T00:00:00\",\"2022-01-28T00:00:00\",\"2022-01-29T00:00:00\",\"2022-01-30T00:00:00\",\"2022-01-31T00:00:00\",\"2022-02-01T00:00:00\",\"2022-02-02T00:00:00\",\"2022-02-03T00:00:00\",\"2022-02-04T00:00:00\",\"2022-02-05T00:00:00\",\"2022-02-06T00:00:00\",\"2022-02-07T00:00:00\",\"2022-02-08T00:00:00\",\"2022-02-09T00:00:00\",\"2022-02-10T00:00:00\",\"2022-02-11T00:00:00\",\"2022-02-12T00:00:00\",\"2022-02-13T00:00:00\",\"2022-02-14T00:00:00\",\"2022-02-15T00:00:00\",\"2022-02-16T00:00:00\",\"2022-02-17T00:00:00\",\"2022-02-18T00:00:00\",\"2022-02-19T00:00:00\",\"2022-02-20T00:00:00\",\"2022-02-21T00:00:00\",\"2022-02-22T00:00:00\",\"2022-02-23T00:00:00\",\"2022-02-24T00:00:00\",\"2022-02-25T00:00:00\",\"2022-02-26T00:00:00\",\"2022-02-27T00:00:00\",\"2022-02-28T00:00:00\",\"2022-03-01T00:00:00\",\"2022-03-02T00:00:00\",\"2022-03-03T00:00:00\",\"2022-03-04T00:00:00\",\"2022-03-05T00:00:00\",\"2022-03-06T00:00:00\",\"2022-03-07T00:00:00\",\"2022-03-08T00:00:00\",\"2022-03-09T00:00:00\",\"2022-03-10T00:00:00\",\"2022-03-11T00:00:00\",\"2022-03-12T00:00:00\",\"2022-03-13T00:00:00\",\"2022-03-14T00:00:00\",\"2022-03-15T00:00:00\",\"2022-03-16T00:00:00\",\"2022-03-17T00:00:00\",\"2022-03-18T00:00:00\",\"2022-03-19T00:00:00\",\"2022-03-20T00:00:00\",\"2022-03-21T00:00:00\",\"2022-03-22T00:00:00\",\"2022-03-23T00:00:00\",\"2022-03-24T00:00:00\",\"2022-03-25T00:00:00\",\"2022-03-26T00:00:00\",\"2022-03-27T00:00:00\",\"2022-03-28T00:00:00\",\"2022-03-29T00:00:00\",\"2022-03-30T00:00:00\",\"2022-03-31T00:00:00\",\"2022-04-01T00:00:00\",\"2022-04-02T00:00:00\",\"2022-04-03T00:00:00\",\"2022-04-04T00:00:00\",\"2022-04-05T00:00:00\",\"2022-04-06T00:00:00\",\"2022-04-07T00:00:00\",\"2022-04-08T00:00:00\",\"2022-04-09T00:00:00\",\"2022-04-10T00:00:00\",\"2022-04-11T00:00:00\",\"2022-04-12T00:00:00\",\"2022-04-13T00:00:00\",\"2022-04-14T00:00:00\",\"2022-04-15T00:00:00\",\"2022-04-16T00:00:00\",\"2022-04-17T00:00:00\",\"2022-04-18T00:00:00\",\"2022-04-19T00:00:00\",\"2022-04-20T00:00:00\",\"2022-04-21T00:00:00\",\"2022-04-22T00:00:00\",\"2022-04-23T00:00:00\",\"2022-04-24T00:00:00\",\"2022-04-25T00:00:00\",\"2022-04-26T00:00:00\",\"2022-04-27T00:00:00\",\"2022-04-28T00:00:00\",\"2022-04-29T00:00:00\",\"2022-04-30T00:00:00\",\"2022-05-01T00:00:00\",\"2022-05-02T00:00:00\",\"2022-05-03T00:00:00\",\"2022-05-04T00:00:00\",\"2022-05-05T00:00:00\",\"2022-05-06T00:00:00\",\"2022-05-07T00:00:00\",\"2022-05-08T00:00:00\",\"2022-05-09T00:00:00\",\"2022-05-10T00:00:00\",\"2022-05-11T00:00:00\",\"2022-05-12T00:00:00\",\"2022-05-13T00:00:00\",\"2022-05-14T00:00:00\",\"2022-05-15T00:00:00\",\"2022-05-16T00:00:00\",\"2022-05-17T00:00:00\",\"2022-05-18T00:00:00\",\"2022-05-19T00:00:00\",\"2022-05-20T00:00:00\",\"2022-05-21T00:00:00\",\"2022-05-22T00:00:00\",\"2022-05-23T00:00:00\",\"2022-05-24T00:00:00\",\"2022-05-25T00:00:00\",\"2022-05-26T00:00:00\",\"2022-05-27T00:00:00\",\"2022-05-28T00:00:00\",\"2022-05-29T00:00:00\",\"2022-05-30T00:00:00\",\"2022-05-31T00:00:00\",\"2022-06-01T00:00:00\",\"2022-06-02T00:00:00\",\"2022-06-03T00:00:00\",\"2022-06-04T00:00:00\",\"2022-06-05T00:00:00\",\"2022-06-06T00:00:00\",\"2022-06-07T00:00:00\",\"2022-06-08T00:00:00\",\"2022-06-09T00:00:00\",\"2022-06-10T00:00:00\",\"2022-06-11T00:00:00\",\"2022-06-12T00:00:00\",\"2022-06-13T00:00:00\",\"2022-06-14T00:00:00\",\"2022-06-15T00:00:00\",\"2022-06-16T00:00:00\",\"2022-06-17T00:00:00\",\"2022-06-18T00:00:00\",\"2022-06-19T00:00:00\",\"2022-06-20T00:00:00\",\"2022-06-21T00:00:00\",\"2022-06-22T00:00:00\",\"2022-06-23T00:00:00\",\"2022-06-24T00:00:00\",\"2022-06-25T00:00:00\",\"2022-06-26T00:00:00\",\"2022-06-27T00:00:00\",\"2022-06-28T00:00:00\",\"2022-06-29T00:00:00\",\"2022-06-30T00:00:00\",\"2022-07-01T00:00:00\",\"2022-07-02T00:00:00\",\"2022-07-03T00:00:00\",\"2022-07-04T00:00:00\",\"2022-07-05T00:00:00\",\"2022-07-06T00:00:00\",\"2022-07-07T00:00:00\",\"2022-07-08T00:00:00\",\"2022-07-09T00:00:00\",\"2022-07-10T00:00:00\",\"2022-07-11T00:00:00\",\"2022-07-12T00:00:00\",\"2022-07-13T00:00:00\",\"2022-07-14T00:00:00\",\"2022-07-15T00:00:00\",\"2022-07-16T00:00:00\",\"2022-07-17T00:00:00\",\"2022-07-18T00:00:00\",\"2022-07-19T00:00:00\",\"2022-07-20T00:00:00\",\"2022-07-21T00:00:00\",\"2022-07-22T00:00:00\",\"2022-07-23T00:00:00\",\"2022-07-24T00:00:00\",\"2022-07-25T00:00:00\",\"2022-07-26T00:00:00\",\"2022-07-27T00:00:00\",\"2022-07-28T00:00:00\",\"2022-07-29T00:00:00\",\"2022-07-30T00:00:00\",\"2022-07-31T00:00:00\",\"2022-08-01T00:00:00\",\"2022-08-02T00:00:00\",\"2022-08-03T00:00:00\",\"2022-08-04T00:00:00\",\"2022-08-05T00:00:00\",\"2022-08-06T00:00:00\",\"2022-08-07T00:00:00\",\"2022-08-08T00:00:00\",\"2022-08-09T00:00:00\",\"2022-08-10T00:00:00\",\"2022-08-11T00:00:00\",\"2022-08-12T00:00:00\",\"2022-08-13T00:00:00\",\"2022-08-14T00:00:00\",\"2022-08-15T00:00:00\",\"2022-08-16T00:00:00\",\"2022-08-17T00:00:00\",\"2022-08-18T00:00:00\",\"2022-08-19T00:00:00\",\"2022-08-20T00:00:00\",\"2022-08-21T00:00:00\",\"2022-08-22T00:00:00\",\"2022-08-23T00:00:00\",\"2022-08-24T00:00:00\",\"2022-08-25T00:00:00\",\"2022-08-26T00:00:00\",\"2022-08-27T00:00:00\",\"2022-08-28T00:00:00\",\"2022-08-29T00:00:00\",\"2022-08-30T00:00:00\",\"2022-08-31T00:00:00\",\"2022-09-01T00:00:00\",\"2022-09-02T00:00:00\",\"2022-09-03T00:00:00\",\"2022-09-04T00:00:00\",\"2022-09-05T00:00:00\",\"2022-09-06T00:00:00\",\"2022-09-07T00:00:00\",\"2022-09-08T00:00:00\",\"2022-09-09T00:00:00\",\"2022-09-10T00:00:00\",\"2022-09-11T00:00:00\",\"2022-09-12T00:00:00\",\"2022-09-13T00:00:00\",\"2022-09-14T00:00:00\",\"2022-09-15T00:00:00\",\"2022-09-16T00:00:00\",\"2022-09-17T00:00:00\",\"2022-09-18T00:00:00\",\"2022-09-19T00:00:00\",\"2022-09-20T00:00:00\",\"2022-09-21T00:00:00\",\"2022-09-22T00:00:00\",\"2022-09-23T00:00:00\",\"2022-09-24T00:00:00\",\"2022-09-25T00:00:00\",\"2022-09-26T00:00:00\",\"2022-09-27T00:00:00\",\"2022-09-28T00:00:00\",\"2022-09-29T00:00:00\",\"2022-09-30T00:00:00\",\"2022-10-01T00:00:00\",\"2022-10-02T00:00:00\",\"2022-10-03T00:00:00\",\"2022-10-04T00:00:00\",\"2022-10-05T00:00:00\",\"2022-10-06T00:00:00\",\"2022-10-07T00:00:00\",\"2022-10-08T00:00:00\",\"2022-10-09T00:00:00\",\"2022-10-10T00:00:00\",\"2022-10-11T00:00:00\",\"2022-10-12T00:00:00\",\"2022-10-13T00:00:00\",\"2022-10-14T00:00:00\",\"2022-10-15T00:00:00\",\"2022-10-16T00:00:00\",\"2022-10-17T00:00:00\",\"2022-10-18T00:00:00\",\"2022-10-19T00:00:00\",\"2022-10-20T00:00:00\",\"2022-10-21T00:00:00\",\"2022-10-22T00:00:00\",\"2022-10-23T00:00:00\",\"2022-10-24T00:00:00\",\"2022-10-25T00:00:00\",\"2022-10-26T00:00:00\",\"2022-10-27T00:00:00\",\"2022-10-28T00:00:00\",\"2022-10-29T00:00:00\",\"2022-10-30T00:00:00\",\"2022-10-31T00:00:00\",\"2022-11-01T00:00:00\",\"2022-11-02T00:00:00\",\"2022-11-03T00:00:00\",\"2022-11-04T00:00:00\",\"2022-11-05T00:00:00\",\"2022-11-06T00:00:00\",\"2022-11-07T00:00:00\",\"2022-11-08T00:00:00\",\"2022-11-09T00:00:00\",\"2022-11-10T00:00:00\",\"2022-11-11T00:00:00\",\"2022-11-12T00:00:00\",\"2022-11-13T00:00:00\",\"2022-11-14T00:00:00\",\"2022-11-15T00:00:00\",\"2022-11-16T00:00:00\",\"2022-11-17T00:00:00\",\"2022-11-18T00:00:00\",\"2022-11-19T00:00:00\",\"2022-11-20T00:00:00\",\"2022-11-21T00:00:00\",\"2022-11-22T00:00:00\",\"2022-11-23T00:00:00\",\"2022-11-24T00:00:00\",\"2022-11-25T00:00:00\",\"2022-11-26T00:00:00\",\"2022-11-27T00:00:00\",\"2022-11-28T00:00:00\",\"2022-11-29T00:00:00\",\"2022-11-30T00:00:00\",\"2022-12-01T00:00:00\",\"2022-12-02T00:00:00\",\"2022-12-03T00:00:00\",\"2022-12-04T00:00:00\",\"2022-12-05T00:00:00\",\"2022-12-06T00:00:00\",\"2022-12-07T00:00:00\",\"2022-12-08T00:00:00\",\"2022-12-09T00:00:00\",\"2022-12-10T00:00:00\",\"2022-12-11T00:00:00\",\"2022-12-12T00:00:00\",\"2022-12-13T00:00:00\"],\"xaxis\":\"x\",\"y\":[15.66,15.66,15.66,15.66,15.66,15.66,15.66,15.66,15.66,15.66,15.66,15.66,15.66,15.66,15.66,15.66,15.66,15.66,15.66,15.66,15.66,15.66,15.66,15.66,15.66,115.48,115.48,115.48,115.48,115.48,115.48,141.79,141.79,141.79,185.53,185.53,185.53,185.53,185.53,185.53,185.53,185.53,222.14,222.14,301.18,301.18,301.18,301.18,301.18,301.18,301.18,301.18,301.18,301.18,301.18,301.18,301.18,301.18,301.18,301.18,301.18,301.18,302.57,302.57,302.57,302.57,302.57,302.57,302.57,302.57,302.57,302.57,302.57,302.57,302.57,302.57,302.57,302.57,302.57,396.55,396.55,396.55,396.55,396.55,396.55,396.55,396.55,396.55,396.55,396.55,396.55,396.55,396.55,396.55,495.41,495.41,495.41,495.41,495.41,495.41,495.41,495.41,509.21,509.21,538.56,538.56,538.56,538.56,538.56,538.56,538.56,538.56,538.56,538.56,538.56,538.56,538.56,538.56,538.56,538.56,538.56,538.56,538.56,538.56,538.56,538.56,538.56,538.56,538.56,538.56,538.56,538.56,538.56,538.56,538.56,538.56,538.56,538.56,538.56,538.56,538.56,538.56,538.56,538.56,538.56,538.56,538.56,538.56,538.56,538.56,538.56,538.56,538.56,538.56,538.56,538.56,538.56,538.56,538.56,625.17,625.17,625.17,625.17,625.17,625.17,625.17,625.17,625.17,625.17,625.17,625.17,625.17,625.17,625.17,625.17,625.17,625.17,625.17,625.17,684.23,684.23,684.23,730.04,730.04,730.04,730.04,730.04,730.04,730.04,730.04,730.04,730.04,730.04,730.04,730.04,730.04,730.04,730.04,730.04,730.04,730.04,730.04,730.04,730.04,730.04,730.04,730.04,730.04,730.04,730.04,730.04,730.04,730.04,730.04,730.04,730.04,730.04,730.04,730.04,730.04,730.04,730.04,730.04,730.04,730.04,730.04,730.04,730.04,730.04,730.04,730.04,730.04,730.04,828.33,828.33,828.33,828.33,870.05,870.05,870.05,870.05,870.05,870.05,870.05,870.05,870.05,870.05,870.05,870.05,870.05,870.05,870.05,870.05,870.05,870.05,900.89,983.47,983.47,983.47,983.47,983.47,983.47,983.47,983.47,983.47,983.47,1072.8,1154.55,1154.55,1154.55,1154.55,1154.55,1154.55,1154.55,1154.55,1184.15,1184.15,1184.15,1184.15,1184.15,1184.15,1184.15,1184.15,1184.15,1184.15,1184.15,1184.15,1198.93,1198.93,1198.93,1198.93,1198.93,1198.93,1198.93,1198.93,1198.93,1198.93,1198.93,1198.93,1302.76,1302.76,1302.76,1302.76,1302.76,1302.76,1302.76,1302.76,1302.76,1302.76,1302.76,1302.76,1302.76,1302.76,1377.75,1377.75,1377.75,1377.75,1377.75,1377.75,1377.75,1377.75,1377.75,1377.75,1377.75,1377.75,1377.75,1377.75,1377.75,1377.75,1377.75,1377.75,1377.75,1377.75,1377.75,1377.75,1377.75,1377.75,1452.35,1452.35,1452.35,1452.35,1452.35,1452.35,1452.35,1452.35,1452.35,1452.35,1452.35,1452.35,1452.35,1452.35,1452.35,1452.35,1452.35,1508.79,1508.79,1508.79,1508.79,1508.79,1508.79,1508.79,1508.79,1508.79,1508.79,1508.79,1508.79,1508.79,1508.79,1508.79,1508.79,1508.79,1508.79,1508.79,1508.79,1508.79,1508.79,1508.79,1508.79,1508.79,1508.79,1508.79,1508.79,1508.79,1508.79,1508.79,1508.79,1508.79,1508.79,1508.79,1508.79,1517.06,1584.01,1584.01,1584.01,1584.01,1584.01,1584.01,1584.01,1584.01,1584.01,1584.01,1584.01,1584.01,1584.01,1584.01,1584.01,1584.01,1584.01,1584.01,1584.01,1584.01,1584.01,1584.01,1584.01,1584.01,1584.01,1584.01,1584.01,1584.01,1584.01,1584.01,1584.01,1584.01,1584.01,1584.01,1584.01,1584.01,1584.01,1674.78,1674.78,1674.78,1674.78,1674.78,1674.78,1674.78,1674.78,1674.78,1674.78,1674.78,1674.78,1674.78,1674.78,1674.78,1674.78,1674.78,1674.78,1674.78,1674.78,1674.78,1674.78,1674.78,1674.78,1674.78,1674.78,1674.78,1674.78,1674.78,1674.78,1674.78,1674.78,1674.78,1674.78,1674.78,1674.78,1674.78,1674.78,1674.78,1674.78,1674.78,1674.78,1674.78,1674.78,1674.78,1674.78,1682.54,1682.54,1682.54,1682.54,1682.54,1682.54,1682.54,1682.54,1682.54,1682.54,1682.54,1682.54,1682.54,1769.82,1769.82,1769.82,1769.82,1769.82,1769.82,1769.82,1769.82,1769.82,1769.82,1815.21,1815.21,1815.21,1815.21,1815.21,1912.83,1912.83,1912.83,1912.83,1912.83,1912.83,1912.83,2011.04,2011.04,2011.04,2011.04,2011.04,2011.04,2011.04,2011.04,2011.04,2011.04,2011.04,2054.92,2054.92,2054.92,2054.92,2054.92,2073.38,2154.96,2154.96,2154.96,2154.96,2154.96,2154.96,2154.96,2154.96,2154.96,2154.96,2154.96,2154.96,2165.45,2165.45,2165.45,2165.45,2217.09,2217.09,2217.09,2217.09,2217.09,2217.09,2217.09,2217.09,2217.09,2217.09,2217.09,2217.09,2217.09,2218.87,2218.87,2218.87,2218.87,2218.87,2218.87,2218.87,2218.87,2218.87,2218.87,2218.87,2218.87,2218.87,2218.87,2218.87,2218.87,2218.87,2218.87,2218.87,2218.87,2218.87,2218.87,2218.87,2304.62,2304.62,2373.54,2373.54,2452.49,2452.49,2452.49,2452.49,2452.49,2452.49,2452.49,2479.76,2479.76,2479.76,2479.76,2479.76,2479.76,2479.76,2479.76,2479.76,2479.76,2479.76,2479.76,2479.76,2479.76,2479.76,2479.76,2479.76,2479.76,2479.76,2479.76,2479.76,2479.76,2479.76,2479.76,2479.76,2479.76,2479.76,2479.76,2479.76,2479.76,2479.76,2479.76,2479.76,2566.32,2566.32,2652.97,2652.97,2709.77,2709.77,2709.77,2709.77,2709.77,2709.77,2709.77,2709.77,2709.77,2709.77,2709.77,2709.77,2709.77,2709.77,2709.77,2709.77,2709.77,2709.77,2709.77,2787.09,2787.09,2787.09,2787.09,2787.09,2787.09,2787.09,2853.95,2853.95,2853.95,2853.95,2853.95,2860.06,2860.06,2860.06,2860.06,2860.06,2860.06,2860.06,2860.06,2860.06,2860.06,2860.06,2860.06,2860.06,2860.06,2860.06,2860.06,2860.06,2860.06,2860.06,2860.06,2860.06,2860.06,2860.06,2860.06,2860.06,2860.06,2860.06,2868.18,2868.18,2868.18,2868.18,2868.18,2868.18,2868.18,2868.18,2868.18,2868.18,2868.18,2868.18,2868.18,2868.18,2868.18,2868.18,2868.18,2868.18,2868.18,2868.18,2868.18,2868.18,2868.18,2868.18,2868.18,2868.18,2868.18,2868.18,2868.18,2868.18,2868.18,2868.18,2959.09,2959.09,2959.09,2959.09,2959.09,2959.09,2959.09,2959.09,2959.09,2959.09,2959.09,2959.09,2959.09,2959.09,2959.09,2979.36,2979.36,2979.36,2979.36,2979.36,2979.36,3034.3,3034.3,3034.3,3034.3,3034.3,3058.22,3058.22,3058.22,3058.22,3058.22,3058.22,3058.22,3058.22,3058.22,3058.22,3058.22,3058.22,3058.22,3058.22,3141.37,3141.37,3141.37,3141.37,3141.37,3141.37,3141.37,3141.37,3141.37,3141.37,3141.37,3141.37,3141.37,3141.37,3141.37,3141.37,3141.37,3141.37,3158.6,3158.6,3158.6,3158.6,3158.6,3158.6,3158.6,3158.6,3158.6,3158.6,3158.6,3158.6,3158.6,3158.6,3158.6,3158.6,3158.6,3158.6,3158.6,3158.6,3158.6,3158.6,3158.6,3158.6,3158.6,3158.6,3158.6,3158.6,3158.6,3158.6,3158.6,3158.6,3158.6,3158.6,3237.47,3237.47,3237.47,3237.47,3237.47,3237.47,3237.47,3237.47,3237.47,3237.47,3237.47,3237.47,3237.47,3237.47,3237.47,3237.47,3237.47,3237.47,3237.47,3237.47,3237.47,3237.47,3237.47,3237.47,3237.47,3237.47,3237.47,3237.47,3237.47,3237.47,3237.47,3237.47,3237.47,3237.47,3237.47,3288.7,3306.18,3306.18,3306.18,3306.18,3306.18,3306.18,3306.18,3306.18,3306.18,3306.18,3306.18,3306.18,3306.18,3306.18,3306.18,3306.18,3306.18,3306.18,3306.18,3342.87,3342.87,3342.87,3342.87,3342.87,3342.87,3342.87,3342.87,3342.87,3342.87,3342.87,3342.87,3342.87,3342.87,3380.07,3380.07,3380.07,3380.07,3380.07,3380.07,3380.07,3380.07,3380.07,3380.07,3380.07,3380.07,3380.07,3380.07,3380.07,3406.67,3406.67,3406.67,3406.67,3406.67,3406.67,3406.67,3406.67,3406.67,3406.67,3406.67,3406.67,3406.67,3406.67,3406.67,3406.67,3406.67,3406.67,3406.67,3406.67,3406.67,3406.67,3406.67,3406.67,3406.67,3406.67,3406.67,3406.67,3406.67,3406.67,3438.27,3438.27,3438.27,3438.27,3438.27,3438.27,3438.27,3438.27,3438.27,3438.27,3438.27,3438.27,3438.27,3438.27,3438.27,3438.27,3438.27,3438.27,3438.27,3438.27,3438.27,3438.27,3438.27,3438.27,3438.27,3438.27,3438.27,3438.27,3438.27,3438.27,3438.27,3438.27,3438.27,3438.27,3438.27,3438.27,3438.27,3438.27,3438.27,3438.27,3438.27,3438.27,3438.27,3438.27,3438.27,3438.27,3438.27,3438.27,3438.27,3438.27,3438.27,3438.27,3438.27,3441.32,3441.32,3441.32,3441.32,3441.32,3441.32,3441.32,3441.32,3480.53,3480.53,3480.53,3480.53,3480.53,3480.53,3480.53,3480.53,3480.53,3480.53,3480.53,3480.53,3574.99,3574.99,3574.99,3574.99,3574.99,3574.99,3574.99,3574.99,3574.99,3574.99,3574.99,3574.99,3574.99,3574.99,3574.99,3574.99,3574.99,3574.99,3574.99,3574.99,3574.99,3574.99,3574.99,3574.99,3574.99,3574.99,3574.99,3574.99,3574.99,3574.99,3574.99,3586.35,3586.35,3586.35,3586.35,3586.35,3638.23,3638.23,3638.23,3638.23,3638.23,3638.23,3638.23,3638.23,3638.23,3638.23,3638.23,3638.23,3638.23,3638.23,3726.33,3726.33,3726.33,3726.33,3726.33,3726.33,3765.36,3765.36,3765.36,3765.36,3826.65,3826.65,3826.65,3826.65,3826.65,3826.65,3826.65,3826.65,3826.65,3826.65,3826.65,3826.65,3826.65,3826.65,3826.65,3826.65,3826.65,3826.65,3826.65,3826.65,3826.65,3826.65,3826.65],\"yaxis\":\"y\",\"type\":\"scattergl\"},{\"hovertemplate\":\"purchased_by=All&lt;br&gt;purchased_date=%{x}&lt;br&gt;Cumulative Dollars Spent=%{y}&lt;extra&gt;&lt;/extra&gt;\",\"legendgroup\":\"All\",\"line\":{\"color\":\"#ab63fa\",\"dash\":\"solid\"},\"marker\":{\"symbol\":\"circle\"},\"mode\":\"lines\",\"name\":\"All\",\"showlegend\":true,\"x\":[\"2020-01-01T00:00:00\",\"2020-01-02T00:00:00\",\"2020-01-03T00:00:00\",\"2020-01-04T00:00:00\",\"2020-01-05T00:00:00\",\"2020-01-06T00:00:00\",\"2020-01-07T00:00:00\",\"2020-01-08T00:00:00\",\"2020-01-09T00:00:00\",\"2020-01-10T00:00:00\",\"2020-01-11T00:00:00\",\"2020-01-12T00:00:00\",\"2020-01-13T00:00:00\",\"2020-01-14T00:00:00\",\"2020-01-15T00:00:00\",\"2020-01-16T00:00:00\",\"2020-01-17T00:00:00\",\"2020-01-18T00:00:00\",\"2020-01-19T00:00:00\",\"2020-01-20T00:00:00\",\"2020-01-21T00:00:00\",\"2020-01-22T00:00:00\",\"2020-01-23T00:00:00\",\"2020-01-24T00:00:00\",\"2020-01-25T00:00:00\",\"2020-01-26T00:00:00\",\"2020-01-27T00:00:00\",\"2020-01-28T00:00:00\",\"2020-01-29T00:00:00\",\"2020-01-30T00:00:00\",\"2020-01-31T00:00:00\",\"2020-02-01T00:00:00\",\"2020-02-02T00:00:00\",\"2020-02-03T00:00:00\",\"2020-02-04T00:00:00\",\"2020-02-05T00:00:00\",\"2020-02-06T00:00:00\",\"2020-02-07T00:00:00\",\"2020-02-08T00:00:00\",\"2020-02-09T00:00:00\",\"2020-02-10T00:00:00\",\"2020-02-11T00:00:00\",\"2020-02-12T00:00:00\",\"2020-02-13T00:00:00\",\"2020-02-14T00:00:00\",\"2020-02-15T00:00:00\",\"2020-02-16T00:00:00\",\"2020-02-17T00:00:00\",\"2020-02-18T00:00:00\",\"2020-02-19T00:00:00\",\"2020-02-20T00:00:00\",\"2020-02-21T00:00:00\",\"2020-02-22T00:00:00\",\"2020-02-23T00:00:00\",\"2020-02-24T00:00:00\",\"2020-02-25T00:00:00\",\"2020-02-26T00:00:00\",\"2020-02-27T00:00:00\",\"2020-02-28T00:00:00\",\"2020-02-29T00:00:00\",\"2020-03-01T00:00:00\",\"2020-03-02T00:00:00\",\"2020-03-03T00:00:00\",\"2020-03-04T00:00:00\",\"2020-03-05T00:00:00\",\"2020-03-06T00:00:00\",\"2020-03-07T00:00:00\",\"2020-03-08T00:00:00\",\"2020-03-09T00:00:00\",\"2020-03-10T00:00:00\",\"2020-03-11T00:00:00\",\"2020-03-12T00:00:00\",\"2020-03-13T00:00:00\",\"2020-03-14T00:00:00\",\"2020-03-15T00:00:00\",\"2020-03-16T00:00:00\",\"2020-03-17T00:00:00\",\"2020-03-18T00:00:00\",\"2020-03-19T00:00:00\",\"2020-03-20T00:00:00\",\"2020-03-21T00:00:00\",\"2020-03-22T00:00:00\",\"2020-03-23T00:00:00\",\"2020-03-24T00:00:00\",\"2020-03-25T00:00:00\",\"2020-03-26T00:00:00\",\"2020-03-27T00:00:00\",\"2020-03-28T00:00:00\",\"2020-03-29T00:00:00\",\"2020-03-30T00:00:00\",\"2020-03-31T00:00:00\",\"2020-04-01T00:00:00\",\"2020-04-02T00:00:00\",\"2020-04-03T00:00:00\",\"2020-04-04T00:00:00\",\"2020-04-05T00:00:00\",\"2020-04-06T00:00:00\",\"2020-04-07T00:00:00\",\"2020-04-08T00:00:00\",\"2020-04-09T00:00:00\",\"2020-04-10T00:00:00\",\"2020-04-11T00:00:00\",\"2020-04-12T00:00:00\",\"2020-04-13T00:00:00\",\"2020-04-14T00:00:00\",\"2020-04-15T00:00:00\",\"2020-04-16T00:00:00\",\"2020-04-17T00:00:00\",\"2020-04-18T00:00:00\",\"2020-04-19T00:00:00\",\"2020-04-20T00:00:00\",\"2020-04-21T00:00:00\",\"2020-04-22T00:00:00\",\"2020-04-23T00:00:00\",\"2020-04-24T00:00:00\",\"2020-04-25T00:00:00\",\"2020-04-26T00:00:00\",\"2020-04-27T00:00:00\",\"2020-04-28T00:00:00\",\"2020-04-29T00:00:00\",\"2020-04-30T00:00:00\",\"2020-05-01T00:00:00\",\"2020-05-02T00:00:00\",\"2020-05-03T00:00:00\",\"2020-05-04T00:00:00\",\"2020-05-05T00:00:00\",\"2020-05-06T00:00:00\",\"2020-05-07T00:00:00\",\"2020-05-08T00:00:00\",\"2020-05-09T00:00:00\",\"2020-05-10T00:00:00\",\"2020-05-11T00:00:00\",\"2020-05-12T00:00:00\",\"2020-05-13T00:00:00\",\"2020-05-14T00:00:00\",\"2020-05-15T00:00:00\",\"2020-05-16T00:00:00\",\"2020-05-17T00:00:00\",\"2020-05-18T00:00:00\",\"2020-05-19T00:00:00\",\"2020-05-20T00:00:00\",\"2020-05-21T00:00:00\",\"2020-05-22T00:00:00\",\"2020-05-23T00:00:00\",\"2020-05-24T00:00:00\",\"2020-05-25T00:00:00\",\"2020-05-26T00:00:00\",\"2020-05-27T00:00:00\",\"2020-05-28T00:00:00\",\"2020-05-29T00:00:00\",\"2020-05-30T00:00:00\",\"2020-05-31T00:00:00\",\"2020-06-01T00:00:00\",\"2020-06-02T00:00:00\",\"2020-06-03T00:00:00\",\"2020-06-04T00:00:00\",\"2020-06-05T00:00:00\",\"2020-06-06T00:00:00\",\"2020-06-07T00:00:00\",\"2020-06-08T00:00:00\",\"2020-06-09T00:00:00\",\"2020-06-10T00:00:00\",\"2020-06-11T00:00:00\",\"2020-06-12T00:00:00\",\"2020-06-13T00:00:00\",\"2020-06-14T00:00:00\",\"2020-06-15T00:00:00\",\"2020-06-16T00:00:00\",\"2020-06-17T00:00:00\",\"2020-06-18T00:00:00\",\"2020-06-19T00:00:00\",\"2020-06-20T00:00:00\",\"2020-06-21T00:00:00\",\"2020-06-22T00:00:00\",\"2020-06-23T00:00:00\",\"2020-06-24T00:00:00\",\"2020-06-25T00:00:00\",\"2020-06-26T00:00:00\",\"2020-06-27T00:00:00\",\"2020-06-28T00:00:00\",\"2020-06-29T00:00:00\",\"2020-06-30T00:00:00\",\"2020-07-01T00:00:00\",\"2020-07-02T00:00:00\",\"2020-07-03T00:00:00\",\"2020-07-04T00:00:00\",\"2020-07-05T00:00:00\",\"2020-07-06T00:00:00\",\"2020-07-07T00:00:00\",\"2020-07-08T00:00:00\",\"2020-07-09T00:00:00\",\"2020-07-10T00:00:00\",\"2020-07-11T00:00:00\",\"2020-07-12T00:00:00\",\"2020-07-13T00:00:00\",\"2020-07-14T00:00:00\",\"2020-07-15T00:00:00\",\"2020-07-16T00:00:00\",\"2020-07-17T00:00:00\",\"2020-07-18T00:00:00\",\"2020-07-19T00:00:00\",\"2020-07-20T00:00:00\",\"2020-07-21T00:00:00\",\"2020-07-22T00:00:00\",\"2020-07-23T00:00:00\",\"2020-07-24T00:00:00\",\"2020-07-25T00:00:00\",\"2020-07-26T00:00:00\",\"2020-07-27T00:00:00\",\"2020-07-28T00:00:00\",\"2020-07-29T00:00:00\",\"2020-07-30T00:00:00\",\"2020-07-31T00:00:00\",\"2020-08-01T00:00:00\",\"2020-08-02T00:00:00\",\"2020-08-03T00:00:00\",\"2020-08-04T00:00:00\",\"2020-08-05T00:00:00\",\"2020-08-06T00:00:00\",\"2020-08-07T00:00:00\",\"2020-08-08T00:00:00\",\"2020-08-09T00:00:00\",\"2020-08-10T00:00:00\",\"2020-08-11T00:00:00\",\"2020-08-12T00:00:00\",\"2020-08-13T00:00:00\",\"2020-08-14T00:00:00\",\"2020-08-15T00:00:00\",\"2020-08-16T00:00:00\",\"2020-08-17T00:00:00\",\"2020-08-18T00:00:00\",\"2020-08-19T00:00:00\",\"2020-08-20T00:00:00\",\"2020-08-21T00:00:00\",\"2020-08-22T00:00:00\",\"2020-08-23T00:00:00\",\"2020-08-24T00:00:00\",\"2020-08-25T00:00:00\",\"2020-08-26T00:00:00\",\"2020-08-27T00:00:00\",\"2020-08-28T00:00:00\",\"2020-08-29T00:00:00\",\"2020-08-30T00:00:00\",\"2020-08-31T00:00:00\",\"2020-09-01T00:00:00\",\"2020-09-02T00:00:00\",\"2020-09-03T00:00:00\",\"2020-09-04T00:00:00\",\"2020-09-05T00:00:00\",\"2020-09-06T00:00:00\",\"2020-09-07T00:00:00\",\"2020-09-08T00:00:00\",\"2020-09-09T00:00:00\",\"2020-09-10T00:00:00\",\"2020-09-11T00:00:00\",\"2020-09-12T00:00:00\",\"2020-09-13T00:00:00\",\"2020-09-14T00:00:00\",\"2020-09-15T00:00:00\",\"2020-09-16T00:00:00\",\"2020-09-17T00:00:00\",\"2020-09-18T00:00:00\",\"2020-09-19T00:00:00\",\"2020-09-20T00:00:00\",\"2020-09-21T00:00:00\",\"2020-09-22T00:00:00\",\"2020-09-23T00:00:00\",\"2020-09-24T00:00:00\",\"2020-09-25T00:00:00\",\"2020-09-26T00:00:00\",\"2020-09-27T00:00:00\",\"2020-09-28T00:00:00\",\"2020-09-29T00:00:00\",\"2020-09-30T00:00:00\",\"2020-10-01T00:00:00\",\"2020-10-02T00:00:00\",\"2020-10-03T00:00:00\",\"2020-10-04T00:00:00\",\"2020-10-05T00:00:00\",\"2020-10-06T00:00:00\",\"2020-10-07T00:00:00\",\"2020-10-08T00:00:00\",\"2020-10-09T00:00:00\",\"2020-10-10T00:00:00\",\"2020-10-11T00:00:00\",\"2020-10-12T00:00:00\",\"2020-10-13T00:00:00\",\"2020-10-14T00:00:00\",\"2020-10-15T00:00:00\",\"2020-10-16T00:00:00\",\"2020-10-17T00:00:00\",\"2020-10-18T00:00:00\",\"2020-10-19T00:00:00\",\"2020-10-20T00:00:00\",\"2020-10-21T00:00:00\",\"2020-10-22T00:00:00\",\"2020-10-23T00:00:00\",\"2020-10-24T00:00:00\",\"2020-10-25T00:00:00\",\"2020-10-26T00:00:00\",\"2020-10-27T00:00:00\",\"2020-10-28T00:00:00\",\"2020-10-29T00:00:00\",\"2020-10-30T00:00:00\",\"2020-10-31T00:00:00\",\"2020-11-01T00:00:00\",\"2020-11-02T00:00:00\",\"2020-11-03T00:00:00\",\"2020-11-04T00:00:00\",\"2020-11-05T00:00:00\",\"2020-11-06T00:00:00\",\"2020-11-07T00:00:00\",\"2020-11-08T00:00:00\",\"2020-11-09T00:00:00\",\"2020-11-10T00:00:00\",\"2020-11-11T00:00:00\",\"2020-11-12T00:00:00\",\"2020-11-13T00:00:00\",\"2020-11-14T00:00:00\",\"2020-11-15T00:00:00\",\"2020-11-16T00:00:00\",\"2020-11-17T00:00:00\",\"2020-11-18T00:00:00\",\"2020-11-19T00:00:00\",\"2020-11-20T00:00:00\",\"2020-11-21T00:00:00\",\"2020-11-22T00:00:00\",\"2020-11-23T00:00:00\",\"2020-11-24T00:00:00\",\"2020-11-25T00:00:00\",\"2020-11-26T00:00:00\",\"2020-11-27T00:00:00\",\"2020-11-28T00:00:00\",\"2020-11-29T00:00:00\",\"2020-11-30T00:00:00\",\"2020-12-01T00:00:00\",\"2020-12-02T00:00:00\",\"2020-12-03T00:00:00\",\"2020-12-04T00:00:00\",\"2020-12-05T00:00:00\",\"2020-12-06T00:00:00\",\"2020-12-07T00:00:00\",\"2020-12-08T00:00:00\",\"2020-12-09T00:00:00\",\"2020-12-10T00:00:00\",\"2020-12-11T00:00:00\",\"2020-12-12T00:00:00\",\"2020-12-13T00:00:00\",\"2020-12-14T00:00:00\",\"2020-12-15T00:00:00\",\"2020-12-16T00:00:00\",\"2020-12-17T00:00:00\",\"2020-12-18T00:00:00\",\"2020-12-19T00:00:00\",\"2020-12-20T00:00:00\",\"2020-12-21T00:00:00\",\"2020-12-22T00:00:00\",\"2020-12-23T00:00:00\",\"2020-12-24T00:00:00\",\"2020-12-25T00:00:00\",\"2020-12-26T00:00:00\",\"2020-12-27T00:00:00\",\"2020-12-28T00:00:00\",\"2020-12-29T00:00:00\",\"2020-12-30T00:00:00\",\"2020-12-31T00:00:00\",\"2021-01-01T00:00:00\",\"2021-01-02T00:00:00\",\"2021-01-03T00:00:00\",\"2021-01-04T00:00:00\",\"2021-01-05T00:00:00\",\"2021-01-06T00:00:00\",\"2021-01-07T00:00:00\",\"2021-01-08T00:00:00\",\"2021-01-09T00:00:00\",\"2021-01-10T00:00:00\",\"2021-01-11T00:00:00\",\"2021-01-12T00:00:00\",\"2021-01-13T00:00:00\",\"2021-01-14T00:00:00\",\"2021-01-15T00:00:00\",\"2021-01-16T00:00:00\",\"2021-01-17T00:00:00\",\"2021-01-18T00:00:00\",\"2021-01-19T00:00:00\",\"2021-01-20T00:00:00\",\"2021-01-21T00:00:00\",\"2021-01-22T00:00:00\",\"2021-01-23T00:00:00\",\"2021-01-24T00:00:00\",\"2021-01-25T00:00:00\",\"2021-01-26T00:00:00\",\"2021-01-27T00:00:00\",\"2021-01-28T00:00:00\",\"2021-01-29T00:00:00\",\"2021-01-30T00:00:00\",\"2021-01-31T00:00:00\",\"2021-02-01T00:00:00\",\"2021-02-02T00:00:00\",\"2021-02-03T00:00:00\",\"2021-02-04T00:00:00\",\"2021-02-05T00:00:00\",\"2021-02-06T00:00:00\",\"2021-02-07T00:00:00\",\"2021-02-08T00:00:00\",\"2021-02-09T00:00:00\",\"2021-02-10T00:00:00\",\"2021-02-11T00:00:00\",\"2021-02-12T00:00:00\",\"2021-02-13T00:00:00\",\"2021-02-14T00:00:00\",\"2021-02-15T00:00:00\",\"2021-02-16T00:00:00\",\"2021-02-17T00:00:00\",\"2021-02-18T00:00:00\",\"2021-02-19T00:00:00\",\"2021-02-20T00:00:00\",\"2021-02-21T00:00:00\",\"2021-02-22T00:00:00\",\"2021-02-23T00:00:00\",\"2021-02-24T00:00:00\",\"2021-02-25T00:00:00\",\"2021-02-26T00:00:00\",\"2021-02-27T00:00:00\",\"2021-02-28T00:00:00\",\"2021-03-01T00:00:00\",\"2021-03-02T00:00:00\",\"2021-03-03T00:00:00\",\"2021-03-04T00:00:00\",\"2021-03-05T00:00:00\",\"2021-03-06T00:00:00\",\"2021-03-07T00:00:00\",\"2021-03-08T00:00:00\",\"2021-03-09T00:00:00\",\"2021-03-10T00:00:00\",\"2021-03-11T00:00:00\",\"2021-03-12T00:00:00\",\"2021-03-13T00:00:00\",\"2021-03-14T00:00:00\",\"2021-03-15T00:00:00\",\"2021-03-16T00:00:00\",\"2021-03-17T00:00:00\",\"2021-03-18T00:00:00\",\"2021-03-19T00:00:00\",\"2021-03-20T00:00:00\",\"2021-03-21T00:00:00\",\"2021-03-22T00:00:00\",\"2021-03-23T00:00:00\",\"2021-03-24T00:00:00\",\"2021-03-25T00:00:00\",\"2021-03-26T00:00:00\",\"2021-03-27T00:00:00\",\"2021-03-28T00:00:00\",\"2021-03-29T00:00:00\",\"2021-03-30T00:00:00\",\"2021-03-31T00:00:00\",\"2021-04-01T00:00:00\",\"2021-04-02T00:00:00\",\"2021-04-03T00:00:00\",\"2021-04-04T00:00:00\",\"2021-04-05T00:00:00\",\"2021-04-06T00:00:00\",\"2021-04-07T00:00:00\",\"2021-04-08T00:00:00\",\"2021-04-09T00:00:00\",\"2021-04-10T00:00:00\",\"2021-04-11T00:00:00\",\"2021-04-12T00:00:00\",\"2021-04-13T00:00:00\",\"2021-04-14T00:00:00\",\"2021-04-15T00:00:00\",\"2021-04-16T00:00:00\",\"2021-04-17T00:00:00\",\"2021-04-18T00:00:00\",\"2021-04-19T00:00:00\",\"2021-04-20T00:00:00\",\"2021-04-21T00:00:00\",\"2021-04-22T00:00:00\",\"2021-04-23T00:00:00\",\"2021-04-24T00:00:00\",\"2021-04-25T00:00:00\",\"2021-04-26T00:00:00\",\"2021-04-27T00:00:00\",\"2021-04-28T00:00:00\",\"2021-04-29T00:00:00\",\"2021-04-30T00:00:00\",\"2021-05-01T00:00:00\",\"2021-05-02T00:00:00\",\"2021-05-03T00:00:00\",\"2021-05-04T00:00:00\",\"2021-05-05T00:00:00\",\"2021-05-06T00:00:00\",\"2021-05-07T00:00:00\",\"2021-05-08T00:00:00\",\"2021-05-09T00:00:00\",\"2021-05-10T00:00:00\",\"2021-05-11T00:00:00\",\"2021-05-12T00:00:00\",\"2021-05-13T00:00:00\",\"2021-05-14T00:00:00\",\"2021-05-15T00:00:00\",\"2021-05-16T00:00:00\",\"2021-05-17T00:00:00\",\"2021-05-18T00:00:00\",\"2021-05-19T00:00:00\",\"2021-05-20T00:00:00\",\"2021-05-21T00:00:00\",\"2021-05-22T00:00:00\",\"2021-05-23T00:00:00\",\"2021-05-24T00:00:00\",\"2021-05-25T00:00:00\",\"2021-05-26T00:00:00\",\"2021-05-27T00:00:00\",\"2021-05-28T00:00:00\",\"2021-05-29T00:00:00\",\"2021-05-30T00:00:00\",\"2021-05-31T00:00:00\",\"2021-06-01T00:00:00\",\"2021-06-02T00:00:00\",\"2021-06-03T00:00:00\",\"2021-06-04T00:00:00\",\"2021-06-05T00:00:00\",\"2021-06-06T00:00:00\",\"2021-06-07T00:00:00\",\"2021-06-08T00:00:00\",\"2021-06-09T00:00:00\",\"2021-06-10T00:00:00\",\"2021-06-11T00:00:00\",\"2021-06-12T00:00:00\",\"2021-06-13T00:00:00\",\"2021-06-14T00:00:00\",\"2021-06-15T00:00:00\",\"2021-06-16T00:00:00\",\"2021-06-17T00:00:00\",\"2021-06-18T00:00:00\",\"2021-06-19T00:00:00\",\"2021-06-20T00:00:00\",\"2021-06-21T00:00:00\",\"2021-06-22T00:00:00\",\"2021-06-23T00:00:00\",\"2021-06-24T00:00:00\",\"2021-06-25T00:00:00\",\"2021-06-26T00:00:00\",\"2021-06-27T00:00:00\",\"2021-06-28T00:00:00\",\"2021-06-29T00:00:00\",\"2021-06-30T00:00:00\",\"2021-07-01T00:00:00\",\"2021-07-02T00:00:00\",\"2021-07-03T00:00:00\",\"2021-07-04T00:00:00\",\"2021-07-05T00:00:00\",\"2021-07-06T00:00:00\",\"2021-07-07T00:00:00\",\"2021-07-08T00:00:00\",\"2021-07-09T00:00:00\",\"2021-07-10T00:00:00\",\"2021-07-11T00:00:00\",\"2021-07-12T00:00:00\",\"2021-07-13T00:00:00\",\"2021-07-14T00:00:00\",\"2021-07-15T00:00:00\",\"2021-07-16T00:00:00\",\"2021-07-17T00:00:00\",\"2021-07-18T00:00:00\",\"2021-07-19T00:00:00\",\"2021-07-20T00:00:00\",\"2021-07-21T00:00:00\",\"2021-07-22T00:00:00\",\"2021-07-23T00:00:00\",\"2021-07-24T00:00:00\",\"2021-07-25T00:00:00\",\"2021-07-26T00:00:00\",\"2021-07-27T00:00:00\",\"2021-07-28T00:00:00\",\"2021-07-29T00:00:00\",\"2021-07-30T00:00:00\",\"2021-07-31T00:00:00\",\"2021-08-01T00:00:00\",\"2021-08-02T00:00:00\",\"2021-08-03T00:00:00\",\"2021-08-04T00:00:00\",\"2021-08-05T00:00:00\",\"2021-08-06T00:00:00\",\"2021-08-07T00:00:00\",\"2021-08-08T00:00:00\",\"2021-08-09T00:00:00\",\"2021-08-10T00:00:00\",\"2021-08-11T00:00:00\",\"2021-08-12T00:00:00\",\"2021-08-13T00:00:00\",\"2021-08-14T00:00:00\",\"2021-08-15T00:00:00\",\"2021-08-16T00:00:00\",\"2021-08-17T00:00:00\",\"2021-08-18T00:00:00\",\"2021-08-19T00:00:00\",\"2021-08-20T00:00:00\",\"2021-08-21T00:00:00\",\"2021-08-22T00:00:00\",\"2021-08-23T00:00:00\",\"2021-08-24T00:00:00\",\"2021-08-25T00:00:00\",\"2021-08-26T00:00:00\",\"2021-08-27T00:00:00\",\"2021-08-28T00:00:00\",\"2021-08-29T00:00:00\",\"2021-08-30T00:00:00\",\"2021-08-31T00:00:00\",\"2021-09-01T00:00:00\",\"2021-09-02T00:00:00\",\"2021-09-03T00:00:00\",\"2021-09-04T00:00:00\",\"2021-09-05T00:00:00\",\"2021-09-06T00:00:00\",\"2021-09-07T00:00:00\",\"2021-09-08T00:00:00\",\"2021-09-09T00:00:00\",\"2021-09-10T00:00:00\",\"2021-09-11T00:00:00\",\"2021-09-12T00:00:00\",\"2021-09-13T00:00:00\",\"2021-09-14T00:00:00\",\"2021-09-15T00:00:00\",\"2021-09-16T00:00:00\",\"2021-09-17T00:00:00\",\"2021-09-18T00:00:00\",\"2021-09-19T00:00:00\",\"2021-09-20T00:00:00\",\"2021-09-21T00:00:00\",\"2021-09-22T00:00:00\",\"2021-09-23T00:00:00\",\"2021-09-24T00:00:00\",\"2021-09-25T00:00:00\",\"2021-09-26T00:00:00\",\"2021-09-27T00:00:00\",\"2021-09-28T00:00:00\",\"2021-09-29T00:00:00\",\"2021-09-30T00:00:00\",\"2021-10-01T00:00:00\",\"2021-10-02T00:00:00\",\"2021-10-03T00:00:00\",\"2021-10-04T00:00:00\",\"2021-10-05T00:00:00\",\"2021-10-06T00:00:00\",\"2021-10-07T00:00:00\",\"2021-10-08T00:00:00\",\"2021-10-09T00:00:00\",\"2021-10-10T00:00:00\",\"2021-10-11T00:00:00\",\"2021-10-12T00:00:00\",\"2021-10-13T00:00:00\",\"2021-10-14T00:00:00\",\"2021-10-15T00:00:00\",\"2021-10-16T00:00:00\",\"2021-10-17T00:00:00\",\"2021-10-18T00:00:00\",\"2021-10-19T00:00:00\",\"2021-10-20T00:00:00\",\"2021-10-21T00:00:00\",\"2021-10-22T00:00:00\",\"2021-10-23T00:00:00\",\"2021-10-24T00:00:00\",\"2021-10-25T00:00:00\",\"2021-10-26T00:00:00\",\"2021-10-27T00:00:00\",\"2021-10-28T00:00:00\",\"2021-10-29T00:00:00\",\"2021-10-30T00:00:00\",\"2021-10-31T00:00:00\",\"2021-11-01T00:00:00\",\"2021-11-02T00:00:00\",\"2021-11-03T00:00:00\",\"2021-11-04T00:00:00\",\"2021-11-05T00:00:00\",\"2021-11-06T00:00:00\",\"2021-11-07T00:00:00\",\"2021-11-08T00:00:00\",\"2021-11-09T00:00:00\",\"2021-11-10T00:00:00\",\"2021-11-11T00:00:00\",\"2021-11-12T00:00:00\",\"2021-11-13T00:00:00\",\"2021-11-14T00:00:00\",\"2021-11-15T00:00:00\",\"2021-11-16T00:00:00\",\"2021-11-17T00:00:00\",\"2021-11-18T00:00:00\",\"2021-11-19T00:00:00\",\"2021-11-20T00:00:00\",\"2021-11-21T00:00:00\",\"2021-11-22T00:00:00\",\"2021-11-23T00:00:00\",\"2021-11-24T00:00:00\",\"2021-11-25T00:00:00\",\"2021-11-26T00:00:00\",\"2021-11-27T00:00:00\",\"2021-11-28T00:00:00\",\"2021-11-29T00:00:00\",\"2021-11-30T00:00:00\",\"2021-12-01T00:00:00\",\"2021-12-02T00:00:00\",\"2021-12-03T00:00:00\",\"2021-12-04T00:00:00\",\"2021-12-05T00:00:00\",\"2021-12-06T00:00:00\",\"2021-12-07T00:00:00\",\"2021-12-08T00:00:00\",\"2021-12-09T00:00:00\",\"2021-12-10T00:00:00\",\"2021-12-11T00:00:00\",\"2021-12-12T00:00:00\",\"2021-12-13T00:00:00\",\"2021-12-14T00:00:00\",\"2021-12-15T00:00:00\",\"2021-12-16T00:00:00\",\"2021-12-17T00:00:00\",\"2021-12-18T00:00:00\",\"2021-12-19T00:00:00\",\"2021-12-20T00:00:00\",\"2021-12-21T00:00:00\",\"2021-12-22T00:00:00\",\"2021-12-23T00:00:00\",\"2021-12-24T00:00:00\",\"2021-12-25T00:00:00\",\"2021-12-26T00:00:00\",\"2021-12-27T00:00:00\",\"2021-12-28T00:00:00\",\"2021-12-29T00:00:00\",\"2021-12-30T00:00:00\",\"2021-12-31T00:00:00\",\"2022-01-01T00:00:00\",\"2022-01-02T00:00:00\",\"2022-01-03T00:00:00\",\"2022-01-04T00:00:00\",\"2022-01-05T00:00:00\",\"2022-01-06T00:00:00\",\"2022-01-07T00:00:00\",\"2022-01-08T00:00:00\",\"2022-01-09T00:00:00\",\"2022-01-10T00:00:00\",\"2022-01-11T00:00:00\",\"2022-01-12T00:00:00\",\"2022-01-13T00:00:00\",\"2022-01-14T00:00:00\",\"2022-01-15T00:00:00\",\"2022-01-16T00:00:00\",\"2022-01-17T00:00:00\",\"2022-01-18T00:00:00\",\"2022-01-19T00:00:00\",\"2022-01-20T00:00:00\",\"2022-01-21T00:00:00\",\"2022-01-22T00:00:00\",\"2022-01-23T00:00:00\",\"2022-01-24T00:00:00\",\"2022-01-25T00:00:00\",\"2022-01-26T00:00:00\",\"2022-01-27T00:00:00\",\"2022-01-28T00:00:00\",\"2022-01-29T00:00:00\",\"2022-01-30T00:00:00\",\"2022-01-31T00:00:00\",\"2022-02-01T00:00:00\",\"2022-02-02T00:00:00\",\"2022-02-03T00:00:00\",\"2022-02-04T00:00:00\",\"2022-02-05T00:00:00\",\"2022-02-06T00:00:00\",\"2022-02-07T00:00:00\",\"2022-02-08T00:00:00\",\"2022-02-09T00:00:00\",\"2022-02-10T00:00:00\",\"2022-02-11T00:00:00\",\"2022-02-12T00:00:00\",\"2022-02-13T00:00:00\",\"2022-02-14T00:00:00\",\"2022-02-15T00:00:00\",\"2022-02-16T00:00:00\",\"2022-02-17T00:00:00\",\"2022-02-18T00:00:00\",\"2022-02-19T00:00:00\",\"2022-02-20T00:00:00\",\"2022-02-21T00:00:00\",\"2022-02-22T00:00:00\",\"2022-02-23T00:00:00\",\"2022-02-24T00:00:00\",\"2022-02-25T00:00:00\",\"2022-02-26T00:00:00\",\"2022-02-27T00:00:00\",\"2022-02-28T00:00:00\",\"2022-03-01T00:00:00\",\"2022-03-02T00:00:00\",\"2022-03-03T00:00:00\",\"2022-03-04T00:00:00\",\"2022-03-05T00:00:00\",\"2022-03-06T00:00:00\",\"2022-03-07T00:00:00\",\"2022-03-08T00:00:00\",\"2022-03-09T00:00:00\",\"2022-03-10T00:00:00\",\"2022-03-11T00:00:00\",\"2022-03-12T00:00:00\",\"2022-03-13T00:00:00\",\"2022-03-14T00:00:00\",\"2022-03-15T00:00:00\",\"2022-03-16T00:00:00\",\"2022-03-17T00:00:00\",\"2022-03-18T00:00:00\",\"2022-03-19T00:00:00\",\"2022-03-20T00:00:00\",\"2022-03-21T00:00:00\",\"2022-03-22T00:00:00\",\"2022-03-23T00:00:00\",\"2022-03-24T00:00:00\",\"2022-03-25T00:00:00\",\"2022-03-26T00:00:00\",\"2022-03-27T00:00:00\",\"2022-03-28T00:00:00\",\"2022-03-29T00:00:00\",\"2022-03-30T00:00:00\",\"2022-03-31T00:00:00\",\"2022-04-01T00:00:00\",\"2022-04-02T00:00:00\",\"2022-04-03T00:00:00\",\"2022-04-04T00:00:00\",\"2022-04-05T00:00:00\",\"2022-04-06T00:00:00\",\"2022-04-07T00:00:00\",\"2022-04-08T00:00:00\",\"2022-04-09T00:00:00\",\"2022-04-10T00:00:00\",\"2022-04-11T00:00:00\",\"2022-04-12T00:00:00\",\"2022-04-13T00:00:00\",\"2022-04-14T00:00:00\",\"2022-04-15T00:00:00\",\"2022-04-16T00:00:00\",\"2022-04-17T00:00:00\",\"2022-04-18T00:00:00\",\"2022-04-19T00:00:00\",\"2022-04-20T00:00:00\",\"2022-04-21T00:00:00\",\"2022-04-22T00:00:00\",\"2022-04-23T00:00:00\",\"2022-04-24T00:00:00\",\"2022-04-25T00:00:00\",\"2022-04-26T00:00:00\",\"2022-04-27T00:00:00\",\"2022-04-28T00:00:00\",\"2022-04-29T00:00:00\",\"2022-04-30T00:00:00\",\"2022-05-01T00:00:00\",\"2022-05-02T00:00:00\",\"2022-05-03T00:00:00\",\"2022-05-04T00:00:00\",\"2022-05-05T00:00:00\",\"2022-05-06T00:00:00\",\"2022-05-07T00:00:00\",\"2022-05-08T00:00:00\",\"2022-05-09T00:00:00\",\"2022-05-10T00:00:00\",\"2022-05-11T00:00:00\",\"2022-05-12T00:00:00\",\"2022-05-13T00:00:00\",\"2022-05-14T00:00:00\",\"2022-05-15T00:00:00\",\"2022-05-16T00:00:00\",\"2022-05-17T00:00:00\",\"2022-05-18T00:00:00\",\"2022-05-19T00:00:00\",\"2022-05-20T00:00:00\",\"2022-05-21T00:00:00\",\"2022-05-22T00:00:00\",\"2022-05-23T00:00:00\",\"2022-05-24T00:00:00\",\"2022-05-25T00:00:00\",\"2022-05-26T00:00:00\",\"2022-05-27T00:00:00\",\"2022-05-28T00:00:00\",\"2022-05-29T00:00:00\",\"2022-05-30T00:00:00\",\"2022-05-31T00:00:00\",\"2022-06-01T00:00:00\",\"2022-06-02T00:00:00\",\"2022-06-03T00:00:00\",\"2022-06-04T00:00:00\",\"2022-06-05T00:00:00\",\"2022-06-06T00:00:00\",\"2022-06-07T00:00:00\",\"2022-06-08T00:00:00\",\"2022-06-09T00:00:00\",\"2022-06-10T00:00:00\",\"2022-06-11T00:00:00\",\"2022-06-12T00:00:00\",\"2022-06-13T00:00:00\",\"2022-06-14T00:00:00\",\"2022-06-15T00:00:00\",\"2022-06-16T00:00:00\",\"2022-06-17T00:00:00\",\"2022-06-18T00:00:00\",\"2022-06-19T00:00:00\",\"2022-06-20T00:00:00\",\"2022-06-21T00:00:00\",\"2022-06-22T00:00:00\",\"2022-06-23T00:00:00\",\"2022-06-24T00:00:00\",\"2022-06-25T00:00:00\",\"2022-06-26T00:00:00\",\"2022-06-27T00:00:00\",\"2022-06-28T00:00:00\",\"2022-06-29T00:00:00\",\"2022-06-30T00:00:00\",\"2022-07-01T00:00:00\",\"2022-07-02T00:00:00\",\"2022-07-03T00:00:00\",\"2022-07-04T00:00:00\",\"2022-07-05T00:00:00\",\"2022-07-06T00:00:00\",\"2022-07-07T00:00:00\",\"2022-07-08T00:00:00\",\"2022-07-09T00:00:00\",\"2022-07-10T00:00:00\",\"2022-07-11T00:00:00\",\"2022-07-12T00:00:00\",\"2022-07-13T00:00:00\",\"2022-07-14T00:00:00\",\"2022-07-15T00:00:00\",\"2022-07-16T00:00:00\",\"2022-07-17T00:00:00\",\"2022-07-18T00:00:00\",\"2022-07-19T00:00:00\",\"2022-07-20T00:00:00\",\"2022-07-21T00:00:00\",\"2022-07-22T00:00:00\",\"2022-07-23T00:00:00\",\"2022-07-24T00:00:00\",\"2022-07-25T00:00:00\",\"2022-07-26T00:00:00\",\"2022-07-27T00:00:00\",\"2022-07-28T00:00:00\",\"2022-07-29T00:00:00\",\"2022-07-30T00:00:00\",\"2022-07-31T00:00:00\",\"2022-08-01T00:00:00\",\"2022-08-02T00:00:00\",\"2022-08-03T00:00:00\",\"2022-08-04T00:00:00\",\"2022-08-05T00:00:00\",\"2022-08-06T00:00:00\",\"2022-08-07T00:00:00\",\"2022-08-08T00:00:00\",\"2022-08-09T00:00:00\",\"2022-08-10T00:00:00\",\"2022-08-11T00:00:00\",\"2022-08-12T00:00:00\",\"2022-08-13T00:00:00\",\"2022-08-14T00:00:00\",\"2022-08-15T00:00:00\",\"2022-08-16T00:00:00\",\"2022-08-17T00:00:00\",\"2022-08-18T00:00:00\",\"2022-08-19T00:00:00\",\"2022-08-20T00:00:00\",\"2022-08-21T00:00:00\",\"2022-08-22T00:00:00\",\"2022-08-23T00:00:00\",\"2022-08-24T00:00:00\",\"2022-08-25T00:00:00\",\"2022-08-26T00:00:00\",\"2022-08-27T00:00:00\",\"2022-08-28T00:00:00\",\"2022-08-29T00:00:00\",\"2022-08-30T00:00:00\",\"2022-08-31T00:00:00\",\"2022-09-01T00:00:00\",\"2022-09-02T00:00:00\",\"2022-09-03T00:00:00\",\"2022-09-04T00:00:00\",\"2022-09-05T00:00:00\",\"2022-09-06T00:00:00\",\"2022-09-07T00:00:00\",\"2022-09-08T00:00:00\",\"2022-09-09T00:00:00\",\"2022-09-10T00:00:00\",\"2022-09-11T00:00:00\",\"2022-09-12T00:00:00\",\"2022-09-13T00:00:00\",\"2022-09-14T00:00:00\",\"2022-09-15T00:00:00\",\"2022-09-16T00:00:00\",\"2022-09-17T00:00:00\",\"2022-09-18T00:00:00\",\"2022-09-19T00:00:00\",\"2022-09-20T00:00:00\",\"2022-09-21T00:00:00\",\"2022-09-22T00:00:00\",\"2022-09-23T00:00:00\",\"2022-09-24T00:00:00\",\"2022-09-25T00:00:00\",\"2022-09-26T00:00:00\",\"2022-09-27T00:00:00\",\"2022-09-28T00:00:00\",\"2022-09-29T00:00:00\",\"2022-09-30T00:00:00\",\"2022-10-01T00:00:00\",\"2022-10-02T00:00:00\",\"2022-10-03T00:00:00\",\"2022-10-04T00:00:00\",\"2022-10-05T00:00:00\",\"2022-10-06T00:00:00\",\"2022-10-07T00:00:00\",\"2022-10-08T00:00:00\",\"2022-10-09T00:00:00\",\"2022-10-10T00:00:00\",\"2022-10-11T00:00:00\",\"2022-10-12T00:00:00\",\"2022-10-13T00:00:00\",\"2022-10-14T00:00:00\",\"2022-10-15T00:00:00\",\"2022-10-16T00:00:00\",\"2022-10-17T00:00:00\",\"2022-10-18T00:00:00\",\"2022-10-19T00:00:00\",\"2022-10-20T00:00:00\",\"2022-10-21T00:00:00\",\"2022-10-22T00:00:00\",\"2022-10-23T00:00:00\",\"2022-10-24T00:00:00\",\"2022-10-25T00:00:00\",\"2022-10-26T00:00:00\",\"2022-10-27T00:00:00\",\"2022-10-28T00:00:00\",\"2022-10-29T00:00:00\",\"2022-10-30T00:00:00\",\"2022-10-31T00:00:00\",\"2022-11-01T00:00:00\",\"2022-11-02T00:00:00\",\"2022-11-03T00:00:00\",\"2022-11-04T00:00:00\",\"2022-11-05T00:00:00\",\"2022-11-06T00:00:00\",\"2022-11-07T00:00:00\",\"2022-11-08T00:00:00\",\"2022-11-09T00:00:00\",\"2022-11-10T00:00:00\",\"2022-11-11T00:00:00\",\"2022-11-12T00:00:00\",\"2022-11-13T00:00:00\",\"2022-11-14T00:00:00\",\"2022-11-15T00:00:00\",\"2022-11-16T00:00:00\",\"2022-11-17T00:00:00\",\"2022-11-18T00:00:00\",\"2022-11-19T00:00:00\",\"2022-11-20T00:00:00\",\"2022-11-21T00:00:00\",\"2022-11-22T00:00:00\",\"2022-11-23T00:00:00\",\"2022-11-24T00:00:00\",\"2022-11-25T00:00:00\",\"2022-11-26T00:00:00\",\"2022-11-27T00:00:00\",\"2022-11-28T00:00:00\",\"2022-11-29T00:00:00\",\"2022-11-30T00:00:00\",\"2022-12-01T00:00:00\",\"2022-12-02T00:00:00\",\"2022-12-03T00:00:00\",\"2022-12-04T00:00:00\",\"2022-12-05T00:00:00\",\"2022-12-06T00:00:00\",\"2022-12-07T00:00:00\",\"2022-12-08T00:00:00\",\"2022-12-09T00:00:00\",\"2022-12-10T00:00:00\",\"2022-12-11T00:00:00\",\"2022-12-12T00:00:00\",\"2022-12-13T00:00:00\"],\"xaxis\":\"x\",\"y\":[15.66,15.66,15.66,15.66,15.66,15.66,15.66,15.66,15.66,15.66,15.66,15.66,15.66,15.66,15.66,15.66,15.66,15.66,15.66,86.38,86.38,86.38,86.38,86.38,86.38,186.2,186.2,186.2,186.2,186.2,186.2,212.51,212.51,212.51,256.25,256.25,256.25,256.25,256.25,256.25,256.25,256.25,292.86,292.86,371.9,371.9,371.9,371.9,371.9,371.9,371.9,371.9,371.9,371.9,371.9,371.9,371.9,371.9,371.9,371.9,436.12,436.12,484.91,484.91,484.91,484.91,484.91,484.91,484.91,484.91,484.91,570.11,570.11,570.11,570.11,570.11,570.11,570.11,570.11,664.09,664.09,664.09,664.09,664.09,664.09,664.09,664.09,711.47,711.47,711.47,711.47,711.47,711.47,711.47,810.33,810.33,810.33,810.33,810.33,810.33,810.33,810.33,824.13,824.13,853.48,853.48,853.48,853.48,853.48,853.48,853.48,853.48,853.48,853.48,853.48,853.48,853.48,922.66,922.66,922.66,922.66,922.66,922.66,922.66,922.66,922.66,922.66,922.66,922.66,922.66,922.66,922.66,985.49,1011.77,1097.67,1097.67,1097.67,1097.67,1097.67,1097.67,1097.67,1097.67,1097.67,1097.67,1097.67,1097.67,1097.67,1144.51,1238.52,1238.52,1238.52,1238.52,1238.52,1238.52,1238.52,1318.74,1318.74,1318.74,1318.74,1405.35,1405.35,1405.35,1405.35,1405.35,1405.35,1405.35,1405.35,1405.35,1405.35,1405.35,1405.35,1405.35,1405.35,1405.35,1405.35,1405.35,1493.97,1493.97,1493.97,1553.03,1553.03,1553.03,1662.78,1662.78,1662.78,1662.78,1662.78,1662.78,1662.78,1711.64,1711.64,1711.64,1711.64,1711.64,1711.64,1711.64,1711.64,1711.64,1732.49,1732.49,1732.49,1732.49,1732.49,1732.49,1732.49,1732.49,1732.49,1732.49,1732.49,1732.49,1732.49,1732.49,1732.49,1732.49,1732.49,1732.49,1732.49,1732.49,1732.49,1732.49,1732.49,1732.49,1743.87,1743.87,1839.99,1839.99,1839.99,1839.99,1839.99,1839.99,1839.99,1839.99,1839.99,1938.28,1938.28,1938.28,1938.28,1980.0,1980.0,2022.53,2022.53,2022.53,2022.53,2022.53,2022.53,2022.53,2022.53,2022.53,2022.53,2036.26,2092.82,2092.82,2092.82,2178.86,2178.86,2209.7,2292.28,2292.28,2292.28,2292.28,2292.28,2292.28,2292.28,2292.28,2292.28,2376.16,2465.49,2547.24,2547.24,2547.24,2547.24,2547.24,2547.24,2547.24,2596.49,2626.09,2626.09,2626.09,2626.09,2626.09,2626.09,2626.09,2700.85,2700.85,2700.85,2700.85,2700.85,2805.99,2805.99,2805.99,2805.99,2805.99,2805.99,2818.02,2818.02,2818.02,2843.86,2843.86,2843.86,2947.69,2947.69,2947.69,2947.69,2947.69,2947.69,2947.69,2947.69,2947.69,2947.69,2947.69,3010.82,3010.82,3010.82,3085.81,3085.81,3085.81,3085.81,3085.81,3085.81,3085.81,3085.81,3085.81,3085.81,3085.81,3085.81,3085.81,3085.81,3120.88,3120.88,3120.88,3120.88,3120.88,3120.88,3120.88,3120.88,3120.88,3199.14,3273.74,3273.74,3273.74,3273.74,3273.74,3273.74,3273.74,3319.76,3319.76,3319.76,3319.76,3319.76,3319.76,3319.76,3319.76,3319.76,3319.76,3376.2,3377.8,3377.8,3421.4,3421.4,3421.4,3421.4,3482.79,3482.79,3482.79,3482.79,3482.79,3482.79,3482.79,3482.79,3482.79,3482.79,3482.79,3482.79,3482.79,3482.79,3488.51,3488.51,3488.51,3488.51,3488.51,3488.51,3488.51,3543.04,3543.04,3543.04,3543.04,3569.12,3569.12,3569.12,3569.12,3577.39,3644.34,3644.34,3644.34,3644.34,3644.34,3644.34,3644.34,3644.34,3644.34,3644.34,3644.34,3644.34,3644.34,3644.34,3644.34,3644.34,3644.34,3658.82,3658.82,3658.82,3723.58,3723.58,3723.58,3723.58,3723.58,3723.58,3723.58,3723.58,3723.58,3723.58,3723.58,3723.58,3723.58,3808.26,3808.26,3808.26,3808.26,3899.03,3899.03,3899.03,3899.03,3914.64,3914.64,3914.64,3914.64,4012.5,4012.5,4012.5,4012.5,4012.5,4012.5,4012.5,4012.5,4012.5,4018.17,4018.17,4018.17,4018.17,4018.17,4061.53,4061.53,4061.53,4061.53,4061.53,4061.53,4061.53,4061.53,4061.53,4061.53,4061.53,4061.53,4061.53,4061.53,4061.53,4061.53,4137.41,4137.41,4137.41,4137.41,4137.41,4137.41,4206.46,4206.46,4214.22,4214.22,4214.22,4214.22,4214.22,4214.22,4214.22,4214.22,4214.22,4214.22,4214.22,4214.22,4214.22,4301.5,4301.5,4335.02,4335.02,4335.02,4335.02,4335.02,4335.02,4335.02,4370.68,4416.07,4416.07,4416.07,4416.07,4416.07,4513.69,4513.69,4513.69,4513.69,4513.69,4513.69,4520.8,4619.01,4619.01,4619.01,4619.01,4619.01,4619.01,4619.01,4619.01,4619.01,4619.01,4619.01,4662.89,4727.07,4727.07,4727.07,4727.07,4745.53,4827.11,4827.11,4827.11,4827.11,4906.42,4906.42,4906.42,4906.42,4906.42,4906.42,4906.42,4906.42,4916.91,4916.91,4916.91,4916.91,4968.55,4968.55,4968.55,4968.55,4968.55,4968.55,4968.55,4968.55,5078.65,5078.65,5078.65,5078.65,5078.65,5080.43,5080.43,5080.43,5080.43,5080.43,5080.43,5166.68,5166.68,5166.68,5166.68,5166.68,5166.68,5166.68,5166.68,5284.83,5284.83,5284.83,5284.83,5284.83,5284.83,5284.83,5284.83,5284.83,5370.58,5370.58,5439.5,5439.5,5518.45,5518.45,5583.72,5583.72,5583.72,5583.72,5618.17,5645.44,5645.44,5645.44,5645.44,5645.44,5645.44,5645.44,5645.44,5645.44,5645.44,5709.13,5709.13,5709.13,5709.13,5709.13,5709.13,5709.13,5709.13,5709.13,5709.13,5793.25,5793.25,5793.25,5793.25,5841.75,5841.75,5841.75,5841.75,5841.75,5881.84,5881.84,5881.84,5881.84,5968.4,6107.93,6194.58,6194.58,6331.42,6331.42,6331.42,6331.42,6331.42,6331.42,6331.42,6331.42,6331.42,6331.42,6331.42,6331.42,6331.42,6331.42,6331.42,6331.42,6332.37,6332.37,6332.37,6409.69,6409.69,6409.69,6409.69,6409.69,6409.69,6454.62,6521.48,6521.48,6521.48,6521.48,6521.48,6567.18,6567.18,6567.18,6567.18,6567.18,6567.18,6567.18,6567.18,6567.18,6567.18,6567.18,6567.18,6567.18,6567.18,6567.18,6567.18,6567.18,6567.18,6567.18,6567.18,6567.18,6567.18,6567.18,6567.18,6567.18,6567.18,6648.15,6656.27,6656.27,6656.27,6727.95,6799.8,6799.8,6799.8,6799.8,6799.8,6799.8,6799.8,6847.26,6847.26,6847.26,6847.26,6847.26,6847.26,6847.26,6847.26,6847.26,6847.26,6926.16,6926.16,6926.16,6960.97,6960.97,6960.97,6960.97,6960.97,6960.97,6960.97,6960.97,7051.88,7051.88,7051.88,7051.88,7051.88,7051.88,7051.88,7106.65,7106.65,7106.65,7106.65,7106.65,7106.65,7106.65,7106.65,7126.92,7126.92,7126.92,7126.92,7126.92,7126.92,7181.86,7342.68,7342.68,7342.68,7342.68,7366.6,7428.12,7474.27,7474.27,7474.27,7474.27,7474.27,7474.27,7474.27,7474.27,7474.27,7474.27,7474.27,7474.27,7557.42,7557.42,7557.42,7557.42,7557.42,7557.42,7557.42,7557.42,7557.42,7557.42,7557.42,7557.42,7557.42,7557.42,7557.42,7557.42,7557.42,7557.42,7574.65,7574.65,7574.65,7656.89,7656.89,7656.89,7656.89,7656.89,7656.89,7656.89,7745.76,7745.76,7745.76,7745.76,7745.76,7745.76,7863.28,7863.28,7918.83,7918.83,7918.83,7918.83,7918.83,7918.83,7918.83,7918.83,7918.83,7918.83,7918.83,7918.83,7918.83,7918.83,7918.83,7918.83,7997.7,7997.7,8066.87,8101.48,8101.48,8101.48,8101.48,8101.48,8101.48,8113.39,8113.39,8113.39,8113.39,8113.39,8113.39,8113.39,8113.39,8113.39,8113.39,8187.32,8187.32,8187.32,8248.97,8248.97,8248.97,8248.97,8264.11,8264.11,8264.11,8264.11,8264.11,8264.11,8264.11,8264.11,8264.11,8371.26,8388.74,8388.74,8388.74,8388.74,8388.74,8388.74,8388.74,8388.74,8388.74,8388.74,8388.74,8388.74,8388.74,8388.74,8388.74,8388.74,8388.74,8388.74,8388.74,8464.48,8464.48,8464.48,8464.48,8464.48,8464.48,8464.48,8464.48,8464.48,8464.48,8464.48,8522.33,8522.33,8522.33,8622.8,8622.8,8622.8,8622.8,8622.8,8622.8,8622.8,8622.8,8622.8,8622.8,8622.8,8622.8,8622.8,8622.8,8622.8,8649.4,8649.4,8649.4,8649.4,8649.4,8649.4,8649.4,8649.4,8649.4,8649.4,8649.4,8696.21,8696.21,8696.21,8696.21,8696.21,8696.21,8696.21,8696.21,8696.21,8696.21,8696.21,8696.21,8696.21,8696.21,8696.21,8696.21,8696.21,8696.21,8696.21,8727.81,8727.81,8850.24,8945.07,8945.07,8945.07,8945.07,8992.78,8992.78,8992.78,9015.91,9015.91,9015.91,9015.91,9015.91,9015.91,9099.62,9099.62,9099.62,9099.62,9099.62,9099.62,9099.62,9188.62,9188.62,9188.62,9188.62,9188.62,9188.62,9265.19,9265.19,9265.19,9265.19,9265.19,9265.19,9265.19,9265.19,9265.19,9265.19,9265.19,9265.19,9265.19,9265.19,9265.19,9265.19,9265.19,9265.19,9265.19,9265.19,9265.19,9265.19,9265.19,9265.19,9268.24,9268.24,9268.24,9268.24,9268.24,9268.24,9268.24,9290.75,9329.96,9393.64,9455.12,9455.12,9455.12,9455.12,9455.12,9455.12,9484.04,9484.04,9484.04,9484.04,9578.5,9578.5,9578.5,9578.5,9578.5,9578.5,9578.5,9578.5,9578.5,9636.52,9636.52,9636.52,9636.52,9636.52,9636.52,9636.52,9636.52,9636.52,9636.52,9636.52,9636.52,9636.52,9636.52,9636.52,9636.52,9636.52,9636.52,9636.52,9636.52,9636.52,9636.52,9647.88,9675.84,9675.84,9768.79,9768.79,9820.67,9820.67,9820.67,9820.67,9820.67,9820.67,9820.67,9823.14,9823.14,9823.14,9823.14,9823.14,9823.14,9823.14,9911.24,9911.24,9969.26,9969.26,9969.26,9969.26,10008.29,10008.29,10008.29,10067.22,10128.51,10128.51,10128.51,10128.51,10128.51,10128.51,10128.51,10128.51,10128.51,10128.51,10128.51,10128.51,10128.51,10128.51,10128.51,10136.7,10152.88,10152.88,10152.88,10215.75,10215.75,10315.0,10411.62],\"yaxis\":\"y\",\"type\":\"scattergl\"}],                        {\"template\":{\"data\":{\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"choropleth\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"choropleth\"}],\"contour\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"contour\"}],\"contourcarpet\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"contourcarpet\"}],\"heatmap\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"heatmap\"}],\"heatmapgl\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"heatmapgl\"}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"histogram2d\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"histogram2d\"}],\"histogram2dcontour\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"histogram2dcontour\"}],\"mesh3d\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"mesh3d\"}],\"parcoords\":[{\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"parcoords\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}],\"scatter\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatter\"}],\"scatter3d\":[{\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatter3d\"}],\"scattercarpet\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattercarpet\"}],\"scattergeo\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattergeo\"}],\"scattergl\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattergl\"}],\"scattermapbox\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattermapbox\"}],\"scatterpolar\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterpolar\"}],\"scatterpolargl\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterpolargl\"}],\"scatterternary\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterternary\"}],\"surface\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"surface\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}]},\"layout\":{\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"autotypenumbers\":\"strict\",\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]],\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]},\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"geo\":{\"bgcolor\":\"white\",\"lakecolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"showlakes\":true,\"showland\":true,\"subunitcolor\":\"white\"},\"hoverlabel\":{\"align\":\"left\"},\"hovermode\":\"closest\",\"mapbox\":{\"style\":\"light\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"bgcolor\":\"#E5ECF6\",\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"gridwidth\":2,\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\"},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"gridwidth\":2,\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\"},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"gridwidth\":2,\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\"}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"ternary\":{\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"bgcolor\":\"#E5ECF6\",\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"title\":{\"x\":0.05},\"xaxis\":{\"automargin\":true,\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"zerolinewidth\":2},\"yaxis\":{\"automargin\":true,\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"zerolinewidth\":2}}},\"xaxis\":{\"anchor\":\"y\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"purchased_date\"}},\"yaxis\":{\"anchor\":\"x\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"Cumulative Dollars Spent\"}},\"legend\":{\"title\":{\"text\":\"purchased_by\"},\"tracegroupgap\":0},\"margin\":{\"t\":60}},                        {\"responsive\": true}                    )                };                                <p>For more of the plotting and charting, check it out live on streamlit!</p>","tags":["time-series","darts","streamlit","python","intermediate"]},{"location":"blog/vgac-tagging/","title":"Custom Data Tagging with Python OpenCV","text":"","tags":["projects","vgac","opencv","python","web-dev"]},{"location":"blog/vgac-tagging/#original","title":"Original","text":"<p>The screen capture above is the first tool used for tagging images for the Videogame Affordances Corpus (VGAC). It was based on the python gui package TKinter, with a simple button and keyboard interface (<code>q-w-e</code>, <code>a-s-d</code>, <code>z-x-c</code>). It works by splitting a given image into a grid, labelling any grid tiles that are the same or very similar to 'known' tiles (i.e. tagged before). Then the user can toggle each affordance for the unkown tiles, effectively labelling the whole image. It also featured rudimentary sprite matching to find game characters in the images. This turned out to be a difficult template matching task to generalize, so I tried out matching in several color and edge detection spaces (rgb, gray, sobel and laplace transformed images). 'Research-grade' Code can be found at this gitlab repo.</p>","tags":["projects","vgac","opencv","python","web-dev"]},{"location":"blog/vgac-tagging/#first-web-version","title":"First Web Version","text":"<p>Eventually that project ran into problems with Mac OS compatibility, so the tagging tool turned to flask a python web microframework used by many developers who like using python and want to deploy code to the web. This got things moving forward, adding a database and the ability to have multiple people tagging images. This implementation can be found in the vgac_tagging folder at [https://pom-itb-gitlab01.campus.pomona.edu/faim-lab/vgac_tagging] While this version is a bit dated, the closest derivative can be found at [http://pom-itb-cs1.campus.pomona.edu/tagging/expert]</p>","tags":["projects","vgac","opencv","python","web-dev"]},{"location":"blog/vgac-tagging/#expert-tagger","title":"Expert Tagger","text":"<p>Since Flask is intended to not be asynchronous and some of our services need time to process images, the current implementation of the main tagging tool and api are built with klein, a twisted flask (ha). The current setup can be found at [https://pom-itb-gitlab01.campus.pomona.edu/faim-lab/vgac-webservices], including all web apps, html and js files, and server config</p>","tags":["projects","vgac","opencv","python","web-dev"]},{"location":"blog/vgac-tagging/#notes","title":"Notes","text":"","tags":["projects","vgac","opencv","python","web-dev"]},{"location":"blog/vgac-tagging/#web-services","title":"Web Services","text":"<p>Web App is a general term for an internet enabled browser application, be it mobile or desktop.</p> <p>They don't need to be the most intricate or need too many parts. Simple apps we make consist of an html file and a Javascript file for the main functionality. </p> <p>For making it look pretty, they also utilize a css stylesheet. </p> <p>And for interacting with our videogame screenshot data they can utilize the DBAPI and / or make an additional API to process data</p> <p>These are all the components of the VGAC Web Services in brief detail. For our purposes our <code>front-end</code> is mostly the html, js, css (files in the static folder), our <code>back-end</code> is the python api files, and the <code>server</code> is handled with Docker and Nginx</p>","tags":["projects","vgac","opencv","python","web-dev"]},{"location":"blog/vgac-tagging/#contribute-a-new-page","title":"Contribute a new page","text":"<p>In our site any file named MYPAGE in the /static/html/ folder of the production branch will generate a link at [http://pom-itb-cs1.campus.pomona.edu/html/MYPAGE.html] and [http://pom-itb-cs1.campus.pomona.edu/tagging/MYPAGE]</p> <p>The intention here is that your file name is simple and relevant to the tagging topic (this may be expanded as we add more types of services / apps).</p> <p>For example, the first app we added to the site was the <code>expert.html</code> page, which is at [http://pom-itb-cs1.campus.pomona.edu/tagging/expert]</p> <p>Reference the Gitlab Workflow page for a refresher on checking out a feature branch and adding a <code>To Do</code> Issue to the boards.</p> <p>When you're happy with your $MYPAGE or at least want to see the skeleton of it live, make sure it is in the /static/html/ folder of your cloned repository</p> Text Only<pre><code>vgac_web\n|   README.org\n|   .gitlab-ci.yml\n|   docker-compose.yml\n|\n|___OTHER_FOLDERS\n|   |\n|\n|___static\n|   |___html   \n|   |   |   expert.html\n|   |   |   MYPAGE.html\n|   |\n|   |___css   \n|   |   |   style.css\n|   |\n|   |___js   \n|   |   |   expert.js\n|   |   |   OTHER_SCRIPTS.js\n|   |\n|   |___images \n|   |   |   examples.png\n</code></pre> <p>Then push your changes and merge them into the Master branch. </p> <p>This triggers a pipeline to deploy your page to the staging server, so it should be available at [http://pom-itb-cs1.campus.pomona.edu/staging/html/MYPAGE.html] (note the /staging/ prefix)</p> <p>Once you've confirmed it seems to be working you can open a merge request from Master to Production (It is set up so you cannot push changes directly to production)</p> <p>Merging Master into Production triggers the same pipeline but for the live apps, so your page should be easily accessible at [http://pom-itb-cs1.campus.pomona.edu/html/MYPAGE.html]!</p>","tags":["projects","vgac","opencv","python","web-dev"]},{"location":"blog/vgac-tagging/#web-app-components","title":"Web App Components","text":"","tags":["projects","vgac","opencv","python","web-dev"]},{"location":"blog/vgac-tagging/#html","title":"HTML","text":"<p>The structure and content of your web page, things like titles, paragraphs, images.</p> <p>For a brief intro to html and css, see this intro</p>","tags":["projects","vgac","opencv","python","web-dev"]},{"location":"blog/vgac-tagging/#css","title":"CSS","text":"<p>A Cascading Style Sheet: defines formatting (colors, sizes, resizing, font) for different sections of the html</p> <p>Generally intended to be reused on many html pages so they all have the same look and feel, but feel free to make and test your own ideas!</p> <p>Some helpful starter links for resizing / formatting: </p> <ul> <li>https://hackernoon.com/the-ultimate-css-battle-grid-vs-flexbox-d40da0449faf</li> <li>https://webdesign.tutsplus.com/articles/flexbox-vs-css-grid-which-should-you-use--cms-30184</li> </ul> <p>In our site any css file $MYSTYLE in the static folder like so <code>/static/css/$MYSTYLE.css</code> can be reached at the pom-itb-CS1 url /css/$MYSTYLE.css</p> <p>This means that in your html, you can use a line like the following to link your stylesheet and use its contents</p> Text Only<pre><code>&lt;link href=\"/css/$MYSTLE.css\" rel=\"stylesheet\"&gt;\n</code></pre>","tags":["projects","vgac","opencv","python","web-dev"]},{"location":"blog/vgac-tagging/#javascript","title":"Javascript","text":"<p>Code that enables interaction, changing content, and many data fetching operations (api calls)</p> <p>Very popular these days, front end web design (how sites and apps look and behave) in many companies relies on front end JS frameworks such as Vue, React, and angular to make sites like Netflix and Facebook</p> <p>Like css and html files, any $NEWSCRIPT.js file you add to /static/js/$NEWSCRIPT.js can be reached at pom-itb-cs1.campus.pomona.edu/js/$NEWSCRIPT.js</p> <p>Similar to css, this means you can link the js code into your html file with a line like the following</p> Text Only<pre><code>&lt;script src=\"/js/$NEWSCRIPT.js\"&gt;&lt;/script&gt;\n</code></pre> <p>If you haven't worked with Javascript at all the plethora of functions may be confusing, see here for some crash course basics on the language</p> <p>You don't need to be an expert to work with Javascript, but referencing the basics is important if you're not used to the language.</p>","tags":["projects","vgac","opencv","python","web-dev"]},{"location":"blog/vgac-tagging/#jquery","title":"Jquery","text":"<p>One of the most commonly used pieces of code on the web, jquery is a library built with Javascript that gives you easier access to html elements and data, the ability to add more html from the js code, and add event loops to get data and update the page</p> <p>AJAX is useful for fetching data without refreshing or changing pages</p> <p>If you want to use it you should follow the jqfundamentals pages for implementing. For linking it, in your html (either in Header or the line before your own script tag) add the line</p> Text Only<pre><code>&lt;script src=\"https://ajax.googleapis.com/ajax/libs/jquery/1.7.1/jquery.min.js\"&gt;&lt;/script&gt;\n</code></pre> <p>This adds the code for jquery (fetched from google's server) to our html page. Then, in our NEWSCRIPT.js file we can add a line like the following to cast JQuery to <code>$</code></p> Text Only<pre><code>var $ = window.jQuery;\n</code></pre>","tags":["projects","vgac","opencv","python","web-dev"]},{"location":"blog/vgac-tagging/#fetch-and-promise","title":"Fetch and Promise","text":"<p>If you don't want to bother with JQuery, the Fetch API with Promises has been gaining popularity and is used by many JS frameworks.</p> <p>Promises are critical to Asynchronous operations, which help parallelize work and get the user their webpage faster. The catch to async operations is that they might succeed, but they also might fail. See this guide for some basic examples, the next link is a follow up to this.</p> <p>For relatively in-depth intro to using fetch (with modern JS coding style and practices), see google codelabs</p>","tags":["projects","vgac","opencv","python","web-dev"]},{"location":"blog/vgac-tagging/#api","title":"API","text":"<p>An Application Programming Interface: the way Javascript in the browser asks for data. In other words, how the front end App connects to the back end server.</p> <p>An API <code>Call</code> involves an internet client (browser, phone, terminal) making a <code>Request</code> to a URL (ex. http://pom-itb-cs1.campus.pomona.edu/api/test) and receiving a <code>Response</code> with data in it</p> <p>If you have cURL installed in your Terminal you can ping our database api and receive a test message back with the following line</p> Text Only<pre><code>curl http://pom-itb-cs1.campus.pomona.edu/api/test\n</code></pre> <p>What is An API in English</p> <p>In our case, and in many cases, the API lets us access data from the database in a form that is useful.</p> <p>For example [[http://pom-itb-cs1.campus.pomona.edu/api/screenshot]] will provide the image data and some other meta data about a random Videogame screenshot in our database</p> <p>Not all web pages need to access an API, as many pages are just text. </p> <p>One API (even one API endpoint) can be used to supply data for multiple pages. See most called apis by developers and companies for some ideas on what kinds of data an API can / should provide.</p>","tags":["projects","vgac","opencv","python","web-dev"]},{"location":"blog/vgac-tagging/#database","title":"Database","text":"<p>We use PostgreSQL</p> <p>Data is stored in tables and organized into rows. Our current database has tables for Screenshots, Tiles, Screenshot Affordance Labels and Tile Affordance Labels (with tables for Sprites to come soon probably)</p> <p>Get data from here using SQL commands (well postgres is slightly different, but both have a similar syntax).</p> <p>The most friendly introduction I've found is at https://www.postgresqltutorial.com/. Looking at examples of SELECT, FROM, and WHERE statements will get you pretty far.</p>","tags":["projects","vgac","opencv","python","web-dev"]},{"location":"blog/vgac-tagging/#reverse-proxy-server","title":"Reverse Proxy Server","text":"<p>Makes it easier to add and scale web apps. Accepts all internet requests to one service that decides how to handle it. </p> <p>This lets us use Docker to containerize our apps and let them communicate securely without being directly accessible by the outside world.</p> <p>See https://en.m.wikipedia.org/wiki/Reverse_proxy for more.</p>","tags":["projects","vgac","opencv","python","web-dev"]},{"location":"blog/vgac-tagging/#staging-server","title":"Staging server","text":"<p>Having errors in live user-facing code is always bad for business. </p>  <p>\"Everybody has a testing environment. Some people are lucky enough enough to have a totally separate environment to run production in.\" - tweet</p>  <p>A Staging server let's us test out pages and catch bugs without showing them to the whole world. </p> <p>We also get a very easy <code>reset</code> point if anything goes catastrophically wrong in production</p> <p>We don't have access to another domain or subdomain, so adding /staging/ at the beginning of the url will target the staging code (master branch). Another valid strategy involves using sub domains (ex. <code>staging.app.com</code>)</p>","tags":["projects","vgac","opencv","python","web-dev"]},{"location":"blog/weather-forecast-dashboard/","title":"Checking 48 Mountain Weather Locations at Once","text":"","tags":["streamlit","python","async","intermediate"]},{"location":"blog/weather-forecast-dashboard/#peak-weather-checking-new-hampshires-48-4000-footers","title":"Peak Weather: Checking New Hampshire's 48 4,000 Footers","text":"<p>Check it out live on streamlit cloud</p> <p>Built to give you a dashboard view of the next few hours' forecast for New Hampshires 48 4,000 ft mountains. Gonna rain on the Kinsmans? Is it snowing on Washington? Should I hike Owl's Head?</p> <p>Powered by Streamlit + Open Weather API. Specifically, Streamlit runs the web interactinos and OpenWeather provides the data.</p> <p>This post will go over a few aspects of the app:</p> <ul> <li>Data scraping the mountain metadata</li> <li>Connecting to Weather API feed</li> <li>Making it reasonably fast </li> </ul>","tags":["streamlit","python","async","intermediate"]},{"location":"blog/weather-forecast-dashboard/#data-scraping","title":"Data Scraping","text":"<p>I couldn't find an easy csv or api for the latitudes and longitudes of the 48 4,000 footers, so I turned to Wikipedia for the list.</p>","tags":["streamlit","python","async","intermediate"]},{"location":"blog/weather-forecast-dashboard/#try-pandas","title":"Try Pandas","text":"<p>The <code>read_html()</code> function in Pandas has been a sanity saver in my job for reading data from flat file specification documents.</p> <p>Unfortunately the data I'm looking for in Wikipedia is in <code>&lt;li&gt;...&lt;/li&gt;</code> tags, not a real html <code>&lt;table&gt;...&lt;/table&gt;</code></p>","tags":["streamlit","python","async","intermediate"]},{"location":"blog/weather-forecast-dashboard/#naive-copypaste","title":"Naive Copy+Paste","text":"<p>Next I tried just copying the list of names and heights to feed to a search API, yielding a csv like the following after some cleanup:</p> Text Only<pre><code>name,height_ft\nWashington,6288\nAdams,5774\nJefferson,5712\n</code></pre> <p>And this gives us csv access to the data like so:</p> Python<pre><code>import pandas as pd\nmountains = pd.read_csv('./data/mtns.txt')\nmountains.head(3)\n</code></pre>       name height_ft     0 Washington 6288   1 Adams 5774   2 Jefferson 5712","tags":["streamlit","python","async","intermediate"]},{"location":"blog/weather-forecast-dashboard/#a-links-to-the-rescue","title":"A-Links to the Rescue","text":"<p>Now with the list of peaks, I needed the corresponding latitude and longitudes.</p> <p>After searching for a straightforward source, I realized the Wikipedia pages linked from the main list page were the best...</p> <p>I grabbed the portion of the html with the list to a file with dev tools (chrome f12), but could have been done with BeautifulSoup</p>","tags":["streamlit","python","async","intermediate"]},{"location":"blog/weather-forecast-dashboard/#scrape-mountain-links","title":"Scrape Mountain Links","text":"Python<pre><code>from bs4 import BeautifulSoup\n# Chunk from 4,000 footers page containing list of mountains\n# https://en.wikipedia.org/wiki/Four-thousand_footers\nsoup = BeautifulSoup(open(\"./data/wiki.html\"), \"html.parser\")\n\n# Gather &lt;a&gt; tags, ignore citation\nlinks = [x for x in soup.find_all(\"a\") if x.get(\"title\")]\nlinks[:2]\n</code></pre> Text Only<pre><code>[&lt;a class=\"mw-redirect\" href=\"/wiki/Mount_Washington_(New_Hampshire)\" title=\"Mount Washington (New Hampshire)\"&gt;Washington&lt;/a&gt;,\n &lt;a href=\"/wiki/Mount_Adams_(New_Hampshire)\" title=\"Mount Adams (New Hampshire)\"&gt;Adams&lt;/a&gt;]\n</code></pre>","tags":["streamlit","python","async","intermediate"]},{"location":"blog/weather-forecast-dashboard/#get-lat-lon-for-one-mountain","title":"Get Lat Lon For One Mountain","text":"<p>With access to the <code>href</code> attributes of the <code>&lt;a&gt;</code> tags, I could then fetch all of those pages and scrape out the Lat and Lon from each.</p> <p>Most older guides will use Python's <code>requests</code> library for this kind of task, but that library does not have the ability to send asynchronous requests without multiprocessing (Translation: It's difficult to fetch a bunch of pages all at once).</p> <p>I've found success with <code>httpx</code> and <code>aiohttp</code> for making asynchronous requests in one Python process. So I went with <code>httpx</code> for fetching each page.</p> <p>Lets demonstrate fetching one of those pages and scraping the Latitude and Longitude. We won't worry too much about errors or missed data for this cleaning phase.</p> Python<pre><code>import httpx\n# English Wikipedia\nBASE_URL = \"https://en.wikipedia.org\"\n\ndef convert(raw_tude: str) -&gt; float:\n    \"\"\"Takes a wikipedia latitude or longitude string and converts it to float\n    Math Source: https://stackoverflow.com/questions/21298772/how-to-convert-latitude-longitude-to-decimal-in-python\n\n    Args:\n        raw_tude (str): Lat or Lon in one of the following forms:\n            degrees\u00b0minutes\u2032seconds\u2033N,\n            degrees\u00b0minutes\u2032N,\n            degrees-minutes-secondsN,\n            degrees-minutesN\n\n    Returns:\n        (float): Float converted lat or lon based on supplied DMS\n    \"\"\"\n    tude = raw_tude.replace(\"\u00b0\", \"-\").replace(\"\u2032\", \"-\").replace(\"\u2033\", \"\")\n    if tude[-2] == \"-\":\n        tude = tude[:-2] + tude[-1]\n    multiplier = 1 if tude[-1] in [\"N\", \"E\"] else -1\n    return multiplier * sum(\n        float(x) / 60 ** n for n, x in enumerate(tude[:-1].split(\"-\"))\n    )\n\na_link = links[0]\na_link\n</code></pre> Text Only<pre><code>&lt;a class=\"mw-redirect\" href=\"/wiki/Mount_Washington_(New_Hampshire)\" title=\"Mount Washington (New Hampshire)\"&gt;Washington&lt;/a&gt;\n</code></pre> Python<pre><code># bs4 lets us \"get\" html tag attributes as in python dicts\nname = a_link.get(\"title\")\nlink = a_link.get(\"href\")\n\n# httpx lets us fetch the raw html page\nraw_page = httpx.get(BASE_URL + link)\n# Which bs4 will help parse\nraw_soup = BeautifulSoup(raw_page, \"html.parser\")\n\n# find returns first instance of a tag with this class\nraw_lat = raw_soup.find(class_=\"latitude\").text.strip()\nlat = convert(raw_lat)\nraw_lon = raw_soup.find(class_=\"longitude\").text.strip()\nlon = convert(raw_lon)\n\nname, link, lat, lon\n</code></pre> Text Only<pre><code>('Mount Washington (New Hampshire)',\n '/wiki/Mount_Washington_(New_Hampshire)',\n 44.2705,\n -71.30324999999999)\n</code></pre>","tags":["streamlit","python","async","intermediate"]},{"location":"blog/weather-forecast-dashboard/#get-lat-lon-for-many-mountains","title":"Get Lat Lon For Many Mountains","text":"<p>Lets chuck the first 10 mountains into a for-loop and fetch the same pieces of data.</p> <p>First we'll define a function to encapsulate the synchronous fetch logic</p> <p>Then we'll see how long this takes with jupyter's <code>%%time</code> magic</p> Python<pre><code>def sync_get_coords(a_link: BeautifulSoup) -&gt; dict:\n    name = a_link.get(\"title\")\n    link = a_link.get(\"href\")\n    raw_page = httpx.get(BASE_URL + link)\n    raw_soup = BeautifulSoup(raw_page, \"html.parser\")\n    raw_lat = raw_soup.find(class_=\"latitude\").text.strip()\n    lat = convert(raw_lat)\n    raw_lon = raw_soup.find(class_=\"longitude\").text.strip()\n    lon = convert(raw_lon)\n    return {\"name\": name, \"link\": link, \"lat\": lat, \"lon\": lon}\n</code></pre> Python<pre><code>%%time\n\nfor a_link in links[:10]:\n    result = sync_get_coords(a_link)\n    print(result)\n</code></pre> Text Only<pre><code>{'name': 'Mount Washington (New Hampshire)', 'link': '/wiki/Mount_Washington_(New_Hampshire)', 'lat': 44.2705, 'lon': -71.30324999999999}\n{'name': 'Mount Adams (New Hampshire)', 'link': '/wiki/Mount_Adams_(New_Hampshire)', 'lat': 44.32055555555556, 'lon': -71.29138888888889}\n{'name': 'Mount Jefferson (New Hampshire)', 'link': '/wiki/Mount_Jefferson_(New_Hampshire)', 'lat': 44.30416666666667, 'lon': -71.31694444444445}\n{'name': 'Mount Monroe (New Hampshire)', 'link': '/wiki/Mount_Monroe_(New_Hampshire)', 'lat': 44.25555555555555, 'lon': -71.32249999999999}\n{'name': 'Mount Madison', 'link': '/wiki/Mount_Madison', 'lat': 44.32833333333333, 'lon': -71.27777777777777}\n{'name': 'Mount Lafayette', 'link': '/wiki/Mount_Lafayette', 'lat': 44.16083333333333, 'lon': -71.64444444444445}\n{'name': 'Mount Lincoln (New Hampshire)', 'link': '/wiki/Mount_Lincoln_(New_Hampshire)', 'lat': 44.14888888888889, 'lon': -71.64444444444445}\n{'name': 'South Twin Mountain (New Hampshire)', 'link': '/wiki/South_Twin_Mountain_(New_Hampshire)', 'lat': 44.1875, 'lon': -71.55533333333334}\n{'name': 'Carter Dome', 'link': '/wiki/Carter_Dome', 'lat': 44.26722222222222, 'lon': -71.17888888888889}\n{'name': 'Mount Moosilauke', 'link': '/wiki/Mount_Moosilauke', 'lat': 44.02444444444444, 'lon': -71.83083333333333}\nCPU times: user 2.44 s, sys: 62.4 ms, total: 2.5 s\nWall time: 3.9 s\n</code></pre> <p>Results will vary by machine, internet connection, Wikipedia server status, and butterly wing flaps.</p> <p>Mine were like this the first time around:</p> Text Only<pre><code>CPU times: user 2.25 s, sys: 65.1 ms, total: 2.31 s\nWall time: 5.47 s\n</code></pre>","tags":["streamlit","python","async","intermediate"]},{"location":"blog/weather-forecast-dashboard/#faster-fetching","title":"Faster Fetching","text":"<p>We're not using the asynchronous capabilities of <code>httpx</code> yet, so each of the 10 requests to Wikipedia needs to go over the wire and back in order for the next request to start.</p> <p>How about we speed things up a little (Jupyter <code>%%time</code> doesn't work on async cells):</p> Python<pre><code>import asyncio\nasync def get_coords(client: httpx.AsyncClient, a_link: BeautifulSoup) -&gt; dict:\n    \"\"\"Given http client and &lt;a&gt; link from wikipedia list,\n    Fetches the place's html page,\n    Attempts to parse and convert lat and lon to decimal from the page (first occurrence)\n    Returns entry with keys: \"name\", \"link\", \"lat\", \"lon\"\n\n    Args:\n        client (httpx.AsyncClient): To make requests. See httpx docs\n        a_link (BeautifulSoup): &lt;a&gt; ... &lt;/a&gt; chunk\n\n    Returns:\n        dict: coordinate entry for this wikipedia place\n    \"\"\"    \n    name = a_link.get(\"title\")\n    link = a_link.get(\"href\")\n    raw_page = await client.get(BASE_URL + link)\n    raw_soup = BeautifulSoup(raw_page, \"html.parser\")\n    raw_lat = raw_soup.find(class_=\"latitude\").text.strip()\n    lat = convert(raw_lat)\n\n    raw_lon = raw_soup.find(class_=\"longitude\").text.strip()\n    lon = convert(raw_lon)\n\n    return {\"name\": name, \"link\": link, \"lat\": lat, \"lon\": lon}\n\n\nasync def gather_coords(links: list) -&gt; list:\n    \"\"\"Given List of a links, asynchronously fetch all of them and return results\"\"\"\n    async with httpx.AsyncClient() as client:\n        tasks = [asyncio.ensure_future(get_coords(client, link)) for link in links]\n        coords = await asyncio.gather(*tasks)\n        return coords\n</code></pre> Python<pre><code>from timeit import default_timer as timer\nstart = timer()\n# Async get all lat lon as list of dictionaries\ncoords = await gather_coords(links[:10])\nend = timer()\nprint(*coords[:10], f\"{end - start :.2f} seconds\", sep='\\n')\n</code></pre> Text Only<pre><code>{'name': 'Mount Washington (New Hampshire)', 'link': '/wiki/Mount_Washington_(New_Hampshire)', 'lat': 44.2705, 'lon': -71.30324999999999}\n{'name': 'Mount Adams (New Hampshire)', 'link': '/wiki/Mount_Adams_(New_Hampshire)', 'lat': 44.32055555555556, 'lon': -71.29138888888889}\n{'name': 'Mount Jefferson (New Hampshire)', 'link': '/wiki/Mount_Jefferson_(New_Hampshire)', 'lat': 44.30416666666667, 'lon': -71.31694444444445}\n{'name': 'Mount Monroe (New Hampshire)', 'link': '/wiki/Mount_Monroe_(New_Hampshire)', 'lat': 44.25555555555555, 'lon': -71.32249999999999}\n{'name': 'Mount Madison', 'link': '/wiki/Mount_Madison', 'lat': 44.32833333333333, 'lon': -71.27777777777777}\n{'name': 'Mount Lafayette', 'link': '/wiki/Mount_Lafayette', 'lat': 44.16083333333333, 'lon': -71.64444444444445}\n{'name': 'Mount Lincoln (New Hampshire)', 'link': '/wiki/Mount_Lincoln_(New_Hampshire)', 'lat': 44.14888888888889, 'lon': -71.64444444444445}\n{'name': 'South Twin Mountain (New Hampshire)', 'link': '/wiki/South_Twin_Mountain_(New_Hampshire)', 'lat': 44.1875, 'lon': -71.55533333333334}\n{'name': 'Carter Dome', 'link': '/wiki/Carter_Dome', 'lat': 44.26722222222222, 'lon': -71.17888888888889}\n{'name': 'Mount Moosilauke', 'link': '/wiki/Mount_Moosilauke', 'lat': 44.02444444444444, 'lon': -71.83083333333333}\n2.01 seconds\n</code></pre> Python<pre><code># Data from original run\n2.16 / 5.47\n</code></pre> Text Only<pre><code>0.3948811700182816\n</code></pre> <p>40% of the time spent scraping data, sounds good to me!</p>","tags":["streamlit","python","async","intermediate"]},{"location":"blog/weather-forecast-dashboard/#data-cleaning","title":"Data Cleaning","text":"<p>If you thought the \"finds first occurrence\" strategy for scraping latitude and longitude was going to cause errors, cheers to you.</p> <p>Turns out just a few mountains have multiple peaks that count as 4,000 footers, so these mountains have 2 sets of latitudes and longitudes.</p> <p>I fetched these by hand and said LGTM with my csv of: - Mountain Names - Heights - Latitudes - Longitudes</p>","tags":["streamlit","python","async","intermediate"]},{"location":"blog/weather-forecast-dashboard/#weather-scraping","title":"Weather Scraping","text":"<p>I figured there's probably a free open API for accessing weather data, and a quick google found two that caught my eye:</p> <ul> <li>OpenWeatherMap</li> <li>Weather.gov</li> </ul> <p>It's a free API, but this was the selling point for OpenWeatherMap for this Proof-of-Concept project:</p> <p>The <code>One Call API</code> provides the following weather data for any geographical coordinates:</p> <ul> <li>Current weather</li> <li>Minute forecast for 1 hour</li> <li>Hourly forecast for 48 hours</li> <li>Daily forecast for 7 days</li> <li>National weather alerts</li> <li>Historical weather data for the previous 5 days</li> </ul>","tags":["streamlit","python","async","intermediate"]},{"location":"blog/weather-forecast-dashboard/#api-signup-and-prep","title":"API Signup and Prep","text":"<p>Getting a free account and key was straightforward involving just an email address verification link.</p> <p>Then off to the races with the following documentation (there's more on their site in better formatting):</p> Bash<pre><code># One Call URL\nhttps://api.openweathermap.org/data/2.5/onecall?lat={lat}&amp;lon={lon}&amp;exclude={part}&amp;appid={API key}\n</code></pre> <p>Parameters</p> <p><code>lat</code>, <code>lon</code>: required  Geographical coordinates (latitude, longitude)</p> <p><code>appid</code>: required  Your unique API key (you can always find it on your account page under the \"API key\" tab)</p> Python<pre><code>from pydantic import BaseSettings\n\n\nclass Settings(BaseSettings):\n    \"\"\"Handles fetching configuration from environment variables and secrets.\n    Type-hinting for config as a bonus\"\"\"\n\n    open_weather_api_key: str\n\n\nsettings = Settings()\n\n\nclass WeatherUnit:\n    STANDARD = \"standard\"\n    KELVIN = \"standard\"\n    METRIC = \"metric\"\n    IMPERIAL = \"imperial\"\n\n\ndef get_one_call_endpoint(\n    lat: float,\n    lon: float,\n    units: WeatherUnit = WeatherUnit.IMPERIAL,\n    exclude=\"\",\n    lang=\"en\",\n):\n    if exclude != \"\":\n        exclude = f\"&amp;exclude={exclude}\"\n    return f\"https://api.openweathermap.org/data/2.5/onecall?lat={lat}&amp;lon={lon}&amp;units={units}{exclude}&amp;lang={lang}&amp;appid={settings.open_weather_api_key}\"\n\n\ndef get_one_call_data(lat, lon):\n    endpoint = get_one_call_endpoint(lat, lon)\n    response = httpx.get(endpoint)\n    return response.json()\n</code></pre>","tags":["streamlit","python","async","intermediate"]},{"location":"blog/weather-forecast-dashboard/#test-one-location","title":"Test One Location","text":"<p>I included some of the API parameters as endpoint configuration options as I messed around with it.</p> <p>For this use case these defaults are sensible to me:</p> <ul> <li>American users -&gt; <code>units = Imperial</code></li> <li>English speaking users -&gt; <code>lang=\"en\"</code></li> <li>Exclude -&gt; don't care too much about some extra data coming over to the server</li> </ul> <p>Lets see what we get for a live mountain location!</p> Python<pre><code>mount_washington = coords[0]\nmount_washington\n</code></pre> Text Only<pre><code>{'name': 'Mount Washington (New Hampshire)',\n 'link': '/wiki/Mount_Washington_(New_Hampshire)',\n 'lat': 44.2705,\n 'lon': -71.30324999999999}\n</code></pre> Python<pre><code>#collapse-output\nget_one_call_data(mount_washington['lat'], mount_washington['lon'])\n</code></pre> Text Only<pre><code>{'lat': 44.2705,\n 'lon': -71.3032,\n 'timezone': 'America/New_York',\n 'timezone_offset': -18000,\n 'current': {'dt': 1644375343,\n  'sunrise': 1644321288,\n  'sunset': 1644357845,\n  'temp': 11.35,\n  'feels_like': -0.72,\n  'pressure': 1011,\n  'humidity': 84,\n  'dew_point': 7.88,\n  'uvi': 0,\n  'clouds': 99,\n  'visibility': 300,\n  'wind_speed': 8.5,\n  'wind_deg': 300,\n  'wind_gust': 15.79,\n  'weather': [{'id': 600,\n    'main': 'Snow',\n    'description': 'light snow',\n    'icon': '13n'}],\n  'snow': {'1h': 0.19}},\n 'minutely': [{'dt': 1644375360, 'precipitation': 0},\n  {'dt': 1644375420, 'precipitation': 0},\n  {'dt': 1644375480, 'precipitation': 0},\n  {'dt': 1644375540, 'precipitation': 0},\n  {'dt': 1644375600, 'precipitation': 0},\n  {'dt': 1644375660, 'precipitation': 0},\n  {'dt': 1644375720, 'precipitation': 0},\n  {'dt': 1644375780, 'precipitation': 0},\n  {'dt': 1644375840, 'precipitation': 0},\n  {'dt': 1644375900, 'precipitation': 0},\n  {'dt': 1644375960, 'precipitation': 0},\n  {'dt': 1644376020, 'precipitation': 0},\n  {'dt': 1644376080, 'precipitation': 0},\n  {'dt': 1644376140, 'precipitation': 0},\n  {'dt': 1644376200, 'precipitation': 0},\n  {'dt': 1644376260, 'precipitation': 0},\n  {'dt': 1644376320, 'precipitation': 0},\n  {'dt': 1644376380, 'precipitation': 0},\n  {'dt': 1644376440, 'precipitation': 0},\n  {'dt': 1644376500, 'precipitation': 0},\n  {'dt': 1644376560, 'precipitation': 0},\n  {'dt': 1644376620, 'precipitation': 0},\n  {'dt': 1644376680, 'precipitation': 0},\n  {'dt': 1644376740, 'precipitation': 0},\n  {'dt': 1644376800, 'precipitation': 0},\n  {'dt': 1644376860, 'precipitation': 0},\n  {'dt': 1644376920, 'precipitation': 0},\n  {'dt': 1644376980, 'precipitation': 0},\n  {'dt': 1644377040, 'precipitation': 0},\n  {'dt': 1644377100, 'precipitation': 0},\n  {'dt': 1644377160, 'precipitation': 0},\n  {'dt': 1644377220, 'precipitation': 0},\n  {'dt': 1644377280, 'precipitation': 0},\n  {'dt': 1644377340, 'precipitation': 0},\n  {'dt': 1644377400, 'precipitation': 0},\n  {'dt': 1644377460, 'precipitation': 0},\n  {'dt': 1644377520, 'precipitation': 0},\n  {'dt': 1644377580, 'precipitation': 0},\n  {'dt': 1644377640, 'precipitation': 0},\n  {'dt': 1644377700, 'precipitation': 0},\n  {'dt': 1644377760, 'precipitation': 0},\n  {'dt': 1644377820, 'precipitation': 0},\n  {'dt': 1644377880, 'precipitation': 0},\n  {'dt': 1644377940, 'precipitation': 0},\n  {'dt': 1644378000, 'precipitation': 0},\n  {'dt': 1644378060, 'precipitation': 0},\n  {'dt': 1644378120, 'precipitation': 0},\n  {'dt': 1644378180, 'precipitation': 0},\n  {'dt': 1644378240, 'precipitation': 0},\n  {'dt': 1644378300, 'precipitation': 0},\n  {'dt': 1644378360, 'precipitation': 0},\n  {'dt': 1644378420, 'precipitation': 0},\n  {'dt': 1644378480, 'precipitation': 0},\n  {'dt': 1644378540, 'precipitation': 0},\n  {'dt': 1644378600, 'precipitation': 0},\n  {'dt': 1644378660, 'precipitation': 0},\n  {'dt': 1644378720, 'precipitation': 0},\n  {'dt': 1644378780, 'precipitation': 0},\n  {'dt': 1644378840, 'precipitation': 0},\n  {'dt': 1644378900, 'precipitation': 0},\n  {'dt': 1644378960, 'precipitation': 0}],\n 'hourly': [{'dt': 1644372000,\n   'temp': 10.76,\n   'feels_like': -1.25,\n   'pressure': 1011,\n   'humidity': 87,\n   'dew_point': 7.99,\n   'uvi': 0,\n   'clouds': 99,\n   'visibility': 353,\n   'wind_speed': 8.25,\n   'wind_deg': 300,\n   'wind_gust': 15.05,\n   'weather': [{'id': 600,\n     'main': 'Snow',\n     'description': 'light snow',\n     'icon': '13n'}],\n   'pop': 0.33,\n   'snow': {'1h': 0.22}},\n  {'dt': 1644375600,\n   'temp': 11.35,\n   'feels_like': -0.72,\n   'pressure': 1011,\n   'humidity': 84,\n   'dew_point': 7.88,\n   'uvi': 0,\n   'clouds': 99,\n   'visibility': 300,\n   'wind_speed': 8.5,\n   'wind_deg': 300,\n   'wind_gust': 15.79,\n   'weather': [{'id': 600,\n     'main': 'Snow',\n     'description': 'light snow',\n     'icon': '13n'}],\n   'pop': 0.33,\n   'snow': {'1h': 0.19}},\n  {'dt': 1644379200,\n   'temp': 10.33,\n   'feels_like': -1.82,\n   'pressure': 1011,\n   'humidity': 86,\n   'dew_point': 7.34,\n   'uvi': 0,\n   'clouds': 99,\n   'visibility': 338,\n   'wind_speed': 8.32,\n   'wind_deg': 303,\n   'wind_gust': 15.97,\n   'weather': [{'id': 600,\n     'main': 'Snow',\n     'description': 'light snow',\n     'icon': '13n'}],\n   'pop': 0.33,\n   'snow': {'1h': 0.24}},\n  {'dt': 1644382800,\n   'temp': 9.09,\n   'feels_like': -3.19,\n   'pressure': 1011,\n   'humidity': 89,\n   'dew_point': 6.78,\n   'uvi': 0,\n   'clouds': 99,\n   'visibility': 302,\n   'wind_speed': 8.14,\n   'wind_deg': 300,\n   'wind_gust': 15.41,\n   'weather': [{'id': 600,\n     'main': 'Snow',\n     'description': 'light snow',\n     'icon': '13n'}],\n   'pop': 0.33,\n   'snow': {'1h': 0.17}},\n  {'dt': 1644386400,\n   'temp': 7.93,\n   'feels_like': -4.67,\n   'pressure': 1012,\n   'humidity': 90,\n   'dew_point': 5.86,\n   'uvi': 0,\n   'clouds': 98,\n   'visibility': 319,\n   'wind_speed': 8.43,\n   'wind_deg': 300,\n   'wind_gust': 16.02,\n   'weather': [{'id': 600,\n     'main': 'Snow',\n     'description': 'light snow',\n     'icon': '13n'}],\n   'pop': 0.33,\n   'snow': {'1h': 0.18}},\n  {'dt': 1644390000,\n   'temp': 6.48,\n   'feels_like': -6.12,\n   'pressure': 1012,\n   'humidity': 92,\n   'dew_point': 4.84,\n   'uvi': 0,\n   'clouds': 96,\n   'visibility': 417,\n   'wind_speed': 8.43,\n   'wind_deg': 303,\n   'wind_gust': 15.66,\n   'weather': [{'id': 600,\n     'main': 'Snow',\n     'description': 'light snow',\n     'icon': '13n'}],\n   'pop': 0.21,\n   'snow': {'1h': 0.16}},\n  {'dt': 1644393600,\n   'temp': 4.3,\n   'feels_like': -8.3,\n   'pressure': 1013,\n   'humidity': 95,\n   'dew_point': 11.14,\n   'uvi': 0,\n   'clouds': 94,\n   'visibility': 761,\n   'wind_speed': 7.81,\n   'wind_deg': 302,\n   'wind_gust': 14.2,\n   'weather': [{'id': 804,\n     'main': 'Clouds',\n     'description': 'overcast clouds',\n     'icon': '04n'}],\n   'pop': 0.09},\n  {'dt': 1644397200,\n   'temp': 4.03,\n   'feels_like': -8.57,\n   'pressure': 1013,\n   'humidity': 94,\n   'dew_point': 10.85,\n   'uvi': 0,\n   'clouds': 92,\n   'visibility': 975,\n   'wind_speed': 7.4,\n   'wind_deg': 302,\n   'wind_gust': 13.35,\n   'weather': [{'id': 804,\n     'main': 'Clouds',\n     'description': 'overcast clouds',\n     'icon': '04n'}],\n   'pop': 0.09},\n  {'dt': 1644400800,\n   'temp': 3.79,\n   'feels_like': -8.52,\n   'pressure': 1014,\n   'humidity': 95,\n   'dew_point': 10.58,\n   'uvi': 0,\n   'clouds': 94,\n   'visibility': 1495,\n   'wind_speed': 7.02,\n   'wind_deg': 296,\n   'wind_gust': 11.65,\n   'weather': [{'id': 804,\n     'main': 'Clouds',\n     'description': 'overcast clouds',\n     'icon': '04n'}],\n   'pop': 0.04},\n  {'dt': 1644404400,\n   'temp': 3.38,\n   'feels_like': -8.12,\n   'pressure': 1016,\n   'humidity': 95,\n   'dew_point': 10.27,\n   'uvi': 0,\n   'clouds': 95,\n   'visibility': 1822,\n   'wind_speed': 6.22,\n   'wind_deg': 295,\n   'wind_gust': 10.11,\n   'weather': [{'id': 804,\n     'main': 'Clouds',\n     'description': 'overcast clouds',\n     'icon': '04n'}],\n   'pop': 0.04},\n  {'dt': 1644408000,\n   'temp': 2.26,\n   'feels_like': -8.12,\n   'pressure': 1017,\n   'humidity': 96,\n   'dew_point': 9.19,\n   'uvi': 0,\n   'clouds': 96,\n   'visibility': 5758,\n   'wind_speed': 5.19,\n   'wind_deg': 298,\n   'wind_gust': 7.99,\n   'weather': [{'id': 804,\n     'main': 'Clouds',\n     'description': 'overcast clouds',\n     'icon': '04d'}],\n   'pop': 0.04},\n  {'dt': 1644411600,\n   'temp': 3.88,\n   'feels_like': -5.91,\n   'pressure': 1017,\n   'humidity': 94,\n   'dew_point': 10.62,\n   'uvi': 0.35,\n   'clouds': 89,\n   'visibility': 10000,\n   'wind_speed': 4.97,\n   'wind_deg': 305,\n   'wind_gust': 8.77,\n   'weather': [{'id': 804,\n     'main': 'Clouds',\n     'description': 'overcast clouds',\n     'icon': '04d'}],\n   'pop': 0},\n  {'dt': 1644415200,\n   'temp': 7.86,\n   'feels_like': -1.5,\n   'pressure': 1017,\n   'humidity': 84,\n   'dew_point': 12.45,\n   'uvi': 0.9,\n   'clouds': 71,\n   'visibility': 10000,\n   'wind_speed': 5.17,\n   'wind_deg': 301,\n   'wind_gust': 7.63,\n   'weather': [{'id': 803,\n     'main': 'Clouds',\n     'description': 'broken clouds',\n     'icon': '04d'}],\n   'pop': 0},\n  {'dt': 1644418800,\n   'temp': 11.75,\n   'feels_like': 3.81,\n   'pressure': 1016,\n   'humidity': 76,\n   'dew_point': 13.91,\n   'uvi': 1.59,\n   'clouds': 52,\n   'visibility': 10000,\n   'wind_speed': 4.61,\n   'wind_deg': 311,\n   'wind_gust': 6.82,\n   'weather': [{'id': 803,\n     'main': 'Clouds',\n     'description': 'broken clouds',\n     'icon': '04d'}],\n   'pop': 0},\n  {'dt': 1644422400,\n   'temp': 14.92,\n   'feels_like': 9.14,\n   'pressure': 1016,\n   'humidity': 71,\n   'dew_point': 15.62,\n   'uvi': 2.16,\n   'clouds': 56,\n   'visibility': 10000,\n   'wind_speed': 3.49,\n   'wind_deg': 304,\n   'wind_gust': 4.79,\n   'weather': [{'id': 803,\n     'main': 'Clouds',\n     'description': 'broken clouds',\n     'icon': '04d'}],\n   'pop': 0},\n  {'dt': 1644426000,\n   'temp': 17.1,\n   'feels_like': 17.1,\n   'pressure': 1015,\n   'humidity': 70,\n   'dew_point': 17.31,\n   'uvi': 2.29,\n   'clouds': 64,\n   'visibility': 10000,\n   'wind_speed': 2.44,\n   'wind_deg': 283,\n   'wind_gust': 3.87,\n   'weather': [{'id': 803,\n     'main': 'Clouds',\n     'description': 'broken clouds',\n     'icon': '04d'}],\n   'pop': 0},\n  {'dt': 1644429600,\n   'temp': 18.88,\n   'feels_like': 18.88,\n   'pressure': 1014,\n   'humidity': 68,\n   'dew_point': 18.59,\n   'uvi': 1.95,\n   'clouds': 67,\n   'visibility': 10000,\n   'wind_speed': 2.1,\n   'wind_deg': 270,\n   'wind_gust': 3.51,\n   'weather': [{'id': 803,\n     'main': 'Clouds',\n     'description': 'broken clouds',\n     'icon': '04d'}],\n   'pop': 0},\n  {'dt': 1644433200,\n   'temp': 19.72,\n   'feels_like': 19.72,\n   'pressure': 1014,\n   'humidity': 68,\n   'dew_point': 19.26,\n   'uvi': 1.2,\n   'clouds': 98,\n   'visibility': 10000,\n   'wind_speed': 2.01,\n   'wind_deg': 226,\n   'wind_gust': 3.6,\n   'weather': [{'id': 804,\n     'main': 'Clouds',\n     'description': 'overcast clouds',\n     'icon': '04d'}],\n   'pop': 0},\n  {'dt': 1644436800,\n   'temp': 19.83,\n   'feels_like': 19.83,\n   'pressure': 1013,\n   'humidity': 72,\n   'dew_point': 20.84,\n   'uvi': 0.58,\n   'clouds': 95,\n   'visibility': 10000,\n   'wind_speed': 2.77,\n   'wind_deg': 185,\n   'wind_gust': 4.38,\n   'weather': [{'id': 804,\n     'main': 'Clouds',\n     'description': 'overcast clouds',\n     'icon': '04d'}],\n   'pop': 0},\n  {'dt': 1644440400,\n   'temp': 18.39,\n   'feels_like': 13.32,\n   'pressure': 1013,\n   'humidity': 84,\n   'dew_point': 23.11,\n   'uvi': 0.17,\n   'clouds': 70,\n   'visibility': 10000,\n   'wind_speed': 3.36,\n   'wind_deg': 185,\n   'wind_gust': 5.3,\n   'weather': [{'id': 803,\n     'main': 'Clouds',\n     'description': 'broken clouds',\n     'icon': '04d'}],\n   'pop': 0},\n  {'dt': 1644444000,\n   'temp': 11.05,\n   'feels_like': 11.05,\n   'pressure': 1014,\n   'humidity': 95,\n   'dew_point': 18.41,\n   'uvi': 0,\n   'clouds': 55,\n   'visibility': 10000,\n   'wind_speed': 2.82,\n   'wind_deg': 191,\n   'wind_gust': 2.77,\n   'weather': [{'id': 803,\n     'main': 'Clouds',\n     'description': 'broken clouds',\n     'icon': '04d'}],\n   'pop': 0},\n  {'dt': 1644447600,\n   'temp': 8.55,\n   'feels_like': 8.55,\n   'pressure': 1015,\n   'humidity': 96,\n   'dew_point': 16.05,\n   'uvi': 0,\n   'clouds': 48,\n   'visibility': 10000,\n   'wind_speed': 2.89,\n   'wind_deg': 185,\n   'wind_gust': 3.04,\n   'weather': [{'id': 802,\n     'main': 'Clouds',\n     'description': 'scattered clouds',\n     'icon': '03n'}],\n   'pop': 0},\n  {'dt': 1644451200,\n   'temp': 9.16,\n   'feels_like': 1.51,\n   'pressure': 1015,\n   'humidity': 96,\n   'dew_point': 16.72,\n   'uvi': 0,\n   'clouds': 56,\n   'visibility': 10000,\n   'wind_speed': 4.12,\n   'wind_deg': 177,\n   'wind_gust': 6.71,\n   'weather': [{'id': 803,\n     'main': 'Clouds',\n     'description': 'broken clouds',\n     'icon': '04n'}],\n   'pop': 0},\n  {'dt': 1644454800,\n   'temp': 9.46,\n   'feels_like': 1.56,\n   'pressure': 1015,\n   'humidity': 96,\n   'dew_point': 16.95,\n   'uvi': 0,\n   'clouds': 100,\n   'visibility': 9935,\n   'wind_speed': 4.32,\n   'wind_deg': 179,\n   'wind_gust': 7.67,\n   'weather': [{'id': 804,\n     'main': 'Clouds',\n     'description': 'overcast clouds',\n     'icon': '04n'}],\n   'pop': 0},\n  {'dt': 1644458400,\n   'temp': 9.12,\n   'feels_like': 1.67,\n   'pressure': 1015,\n   'humidity': 94,\n   'dew_point': 16.07,\n   'uvi': 0,\n   'clouds': 100,\n   'visibility': 9789,\n   'wind_speed': 3.98,\n   'wind_deg': 182,\n   'wind_gust': 6.71,\n   'weather': [{'id': 804,\n     'main': 'Clouds',\n     'description': 'overcast clouds',\n     'icon': '04n'}],\n   'pop': 0},\n  {'dt': 1644462000,\n   'temp': 9.39,\n   'feels_like': 2.62,\n   'pressure': 1014,\n   'humidity': 92,\n   'dew_point': 16.05,\n   'uvi': 0,\n   'clouds': 100,\n   'visibility': 10000,\n   'wind_speed': 3.6,\n   'wind_deg': 199,\n   'wind_gust': 4.16,\n   'weather': [{'id': 804,\n     'main': 'Clouds',\n     'description': 'overcast clouds',\n     'icon': '04n'}],\n   'pop': 0},\n  {'dt': 1644465600,\n   'temp': 10.08,\n   'feels_like': 3.63,\n   'pressure': 1013,\n   'humidity': 92,\n   'dew_point': 16.77,\n   'uvi': 0,\n   'clouds': 100,\n   'visibility': 10000,\n   'wind_speed': 3.47,\n   'wind_deg': 209,\n   'wind_gust': 4.21,\n   'weather': [{'id': 804,\n     'main': 'Clouds',\n     'description': 'overcast clouds',\n     'icon': '04n'}],\n   'pop': 0},\n  {'dt': 1644469200,\n   'temp': 10.22,\n   'feels_like': 3.69,\n   'pressure': 1013,\n   'humidity': 92,\n   'dew_point': 16.92,\n   'uvi': 0,\n   'clouds': 99,\n   'visibility': 10000,\n   'wind_speed': 3.53,\n   'wind_deg': 205,\n   'wind_gust': 4.29,\n   'weather': [{'id': 804,\n     'main': 'Clouds',\n     'description': 'overcast clouds',\n     'icon': '04n'}],\n   'pop': 0},\n  {'dt': 1644472800,\n   'temp': 9.5,\n   'feels_like': 2.26,\n   'pressure': 1012,\n   'humidity': 92,\n   'dew_point': 16.18,\n   'uvi': 0,\n   'clouds': 87,\n   'visibility': 10000,\n   'wind_speed': 3.89,\n   'wind_deg': 213,\n   'wind_gust': 3.74,\n   'weather': [{'id': 804,\n     'main': 'Clouds',\n     'description': 'overcast clouds',\n     'icon': '04n'}],\n   'pop': 0},\n  {'dt': 1644476400,\n   'temp': 9.05,\n   'feels_like': 2.03,\n   'pressure': 1012,\n   'humidity': 93,\n   'dew_point': 15.87,\n   'uvi': 0,\n   'clouds': 22,\n   'visibility': 10000,\n   'wind_speed': 3.71,\n   'wind_deg': 220,\n   'wind_gust': 3.71,\n   'weather': [{'id': 801,\n     'main': 'Clouds',\n     'description': 'few clouds',\n     'icon': '02n'}],\n   'pop': 0},\n  {'dt': 1644480000,\n   'temp': 8.8,\n   'feels_like': 2.21,\n   'pressure': 1011,\n   'humidity': 92,\n   'dew_point': 15.4,\n   'uvi': 0,\n   'clouds': 27,\n   'visibility': 10000,\n   'wind_speed': 3.44,\n   'wind_deg': 230,\n   'wind_gust': 3.56,\n   'weather': [{'id': 802,\n     'main': 'Clouds',\n     'description': 'scattered clouds',\n     'icon': '03n'}],\n   'pop': 0},\n  {'dt': 1644483600,\n   'temp': 10.08,\n   'feels_like': 3.76,\n   'pressure': 1010,\n   'humidity': 91,\n   'dew_point': 16.47,\n   'uvi': 0,\n   'clouds': 51,\n   'visibility': 10000,\n   'wind_speed': 3.4,\n   'wind_deg': 230,\n   'wind_gust': 3.51,\n   'weather': [{'id': 803,\n     'main': 'Clouds',\n     'description': 'broken clouds',\n     'icon': '04n'}],\n   'pop': 0},\n  {'dt': 1644487200,\n   'temp': 11.68,\n   'feels_like': 5.83,\n   'pressure': 1010,\n   'humidity': 90,\n   'dew_point': 17.89,\n   'uvi': 0,\n   'clouds': 64,\n   'visibility': 10000,\n   'wind_speed': 3.27,\n   'wind_deg': 223,\n   'wind_gust': 3.18,\n   'weather': [{'id': 803,\n     'main': 'Clouds',\n     'description': 'broken clouds',\n     'icon': '04n'}],\n   'pop': 0},\n  {'dt': 1644490800,\n   'temp': 12.47,\n   'feels_like': 6.19,\n   'pressure': 1009,\n   'humidity': 90,\n   'dew_point': 18.73,\n   'uvi': 0,\n   'clouds': 71,\n   'visibility': 10000,\n   'wind_speed': 3.58,\n   'wind_deg': 223,\n   'wind_gust': 3.8,\n   'weather': [{'id': 803,\n     'main': 'Clouds',\n     'description': 'broken clouds',\n     'icon': '04n'}],\n   'pop': 0},\n  {'dt': 1644494400,\n   'temp': 13.1,\n   'feels_like': 7.18,\n   'pressure': 1009,\n   'humidity': 94,\n   'dew_point': 20.43,\n   'uvi': 0,\n   'clouds': 76,\n   'visibility': 856,\n   'wind_speed': 3.42,\n   'wind_deg': 224,\n   'wind_gust': 3.69,\n   'weather': [{'id': 803,\n     'main': 'Clouds',\n     'description': 'broken clouds',\n     'icon': '04d'}],\n   'pop': 0.09},\n  {'dt': 1644498000,\n   'temp': 16.45,\n   'feels_like': 11.53,\n   'pressure': 1008,\n   'humidity': 96,\n   'dew_point': 24.35,\n   'uvi': 0.23,\n   'clouds': 100,\n   'visibility': 619,\n   'wind_speed': 3.11,\n   'wind_deg': 222,\n   'wind_gust': 5.57,\n   'weather': [{'id': 600,\n     'main': 'Snow',\n     'description': 'light snow',\n     'icon': '13d'}],\n   'pop': 0.55,\n   'snow': {'1h': 0.17}},\n  {'dt': 1644501600,\n   'temp': 18.79,\n   'feels_like': 13.89,\n   'pressure': 1008,\n   'humidity': 95,\n   'dew_point': 26.6,\n   'uvi': 0.58,\n   'clouds': 100,\n   'visibility': 6348,\n   'wind_speed': 3.29,\n   'wind_deg': 214,\n   'wind_gust': 5.3,\n   'weather': [{'id': 804,\n     'main': 'Clouds',\n     'description': 'overcast clouds',\n     'icon': '04d'}],\n   'pop': 0.42},\n  {'dt': 1644505200,\n   'temp': 20.39,\n   'feels_like': 14.59,\n   'pressure': 1007,\n   'humidity': 95,\n   'dew_point': 28.13,\n   'uvi': 1.01,\n   'clouds': 100,\n   'visibility': 10000,\n   'wind_speed': 4.05,\n   'wind_deg': 199,\n   'wind_gust': 6.71,\n   'weather': [{'id': 804,\n     'main': 'Clouds',\n     'description': 'overcast clouds',\n     'icon': '04d'}],\n   'pop': 0.38},\n  {'dt': 1644508800,\n   'temp': 21.81,\n   'feels_like': 15.03,\n   'pressure': 1006,\n   'humidity': 94,\n   'dew_point': 29.3,\n   'uvi': 1.04,\n   'clouds': 94,\n   'visibility': 4548,\n   'wind_speed': 5.08,\n   'wind_deg': 196,\n   'wind_gust': 9.06,\n   'weather': [{'id': 804,\n     'main': 'Clouds',\n     'description': 'overcast clouds',\n     'icon': '04d'}],\n   'pop': 0.25},\n  {'dt': 1644512400,\n   'temp': 22.01,\n   'feels_like': 14.76,\n   'pressure': 1005,\n   'humidity': 92,\n   'dew_point': 28.96,\n   'uvi': 1.1,\n   'clouds': 91,\n   'visibility': 2107,\n   'wind_speed': 5.57,\n   'wind_deg': 197,\n   'wind_gust': 12.86,\n   'weather': [{'id': 804,\n     'main': 'Clouds',\n     'description': 'overcast clouds',\n     'icon': '04d'}],\n   'pop': 0.23},\n  {'dt': 1644516000,\n   'temp': 21.06,\n   'feels_like': 13.69,\n   'pressure': 1004,\n   'humidity': 99,\n   'dew_point': 29.71,\n   'uvi': 0.94,\n   'clouds': 93,\n   'visibility': 53,\n   'wind_speed': 5.5,\n   'wind_deg': 194,\n   'wind_gust': 10.96,\n   'weather': [{'id': 600,\n     'main': 'Snow',\n     'description': 'light snow',\n     'icon': '13d'}],\n   'pop': 0.35,\n   'snow': {'1h': 0.31}},\n  {'dt': 1644519600,\n   'temp': 20.07,\n   'feels_like': 12.65,\n   'pressure': 1004,\n   'humidity': 99,\n   'dew_point': 28.72,\n   'uvi': 0.27,\n   'clouds': 100,\n   'visibility': 76,\n   'wind_speed': 5.37,\n   'wind_deg': 216,\n   'wind_gust': 13.85,\n   'weather': [{'id': 600,\n     'main': 'Snow',\n     'description': 'light snow',\n     'icon': '13d'}],\n   'pop': 0.68,\n   'snow': {'1h': 0.36}},\n  {'dt': 1644523200,\n   'temp': 19.6,\n   'feels_like': 12.4,\n   'pressure': 1004,\n   'humidity': 95,\n   'dew_point': 27.32,\n   'uvi': 0.14,\n   'clouds': 100,\n   'visibility': 255,\n   'wind_speed': 5.1,\n   'wind_deg': 240,\n   'wind_gust': 14,\n   'weather': [{'id': 600,\n     'main': 'Snow',\n     'description': 'light snow',\n     'icon': '13d'}],\n   'pop': 0.6,\n   'snow': {'1h': 0.21}},\n  {'dt': 1644526800,\n   'temp': 18.72,\n   'feels_like': 11.3,\n   'pressure': 1005,\n   'humidity': 94,\n   'dew_point': 26.04,\n   'uvi': 0.04,\n   'clouds': 100,\n   'visibility': 781,\n   'wind_speed': 5.14,\n   'wind_deg': 254,\n   'wind_gust': 15.99,\n   'weather': [{'id': 600,\n     'main': 'Snow',\n     'description': 'light snow',\n     'icon': '13d'}],\n   'pop': 0.7,\n   'snow': {'1h': 0.14}},\n  {'dt': 1644530400,\n   'temp': 16.41,\n   'feels_like': 9.25,\n   'pressure': 1005,\n   'humidity': 96,\n   'dew_point': 24.3,\n   'uvi': 0,\n   'clouds': 100,\n   'visibility': 645,\n   'wind_speed': 4.61,\n   'wind_deg': 261,\n   'wind_gust': 9.51,\n   'weather': [{'id': 600,\n     'main': 'Snow',\n     'description': 'light snow',\n     'icon': '13d'}],\n   'pop': 0.69,\n   'snow': {'1h': 0.16}},\n  {'dt': 1644534000,\n   'temp': 14.34,\n   'feels_like': 5.86,\n   'pressure': 1007,\n   'humidity': 95,\n   'dew_point': 21.97,\n   'uvi': 0,\n   'clouds': 99,\n   'visibility': 689,\n   'wind_speed': 5.39,\n   'wind_deg': 260,\n   'wind_gust': 12.37,\n   'weather': [{'id': 600,\n     'main': 'Snow',\n     'description': 'light snow',\n     'icon': '13n'}],\n   'pop': 0.66,\n   'snow': {'1h': 0.11}},\n  {'dt': 1644537600,\n   'temp': 14.02,\n   'feels_like': 4.87,\n   'pressure': 1007,\n   'humidity': 94,\n   'dew_point': 21.47,\n   'uvi': 0,\n   'clouds': 99,\n   'visibility': 282,\n   'wind_speed': 5.95,\n   'wind_deg': 257,\n   'wind_gust': 16.06,\n   'weather': [{'id': 600,\n     'main': 'Snow',\n     'description': 'light snow',\n     'icon': '13n'}],\n   'pop': 0.58,\n   'snow': {'1h': 0.26}},\n  {'dt': 1644541200,\n   'temp': 13.08,\n   'feels_like': 3.52,\n   'pressure': 1008,\n   'humidity': 94,\n   'dew_point': 20.55,\n   'uvi': 0,\n   'clouds': 100,\n   'visibility': 248,\n   'wind_speed': 6.17,\n   'wind_deg': 261,\n   'wind_gust': 16.87,\n   'weather': [{'id': 600,\n     'main': 'Snow',\n     'description': 'light snow',\n     'icon': '13n'}],\n   'pop': 0.42,\n   'snow': {'1h': 0.28}}],\n 'daily': [{'dt': 1644336000,\n   'sunrise': 1644321288,\n   'sunset': 1644357845,\n   'moonrise': 1644334440,\n   'moonset': 1644298020,\n   'moon_phase': 0.25,\n   'temp': {'day': 18.59,\n    'min': 10.33,\n    'max': 19.72,\n    'night': 10.33,\n    'eve': 15.06,\n    'morn': 14.94},\n   'feels_like': {'day': 12.94, 'night': -1.82, 'eve': 5.97, 'morn': 14.94},\n   'pressure': 1010,\n   'humidity': 97,\n   'dew_point': 26.85,\n   'wind_speed': 8.5,\n   'wind_deg': 300,\n   'wind_gust': 15.97,\n   'weather': [{'id': 601,\n     'main': 'Snow',\n     'description': 'snow',\n     'icon': '13d'}],\n   'clouds': 100,\n   'pop': 0.99,\n   'snow': 10.77,\n   'uvi': 1.2},\n  {'dt': 1644422400,\n   'sunrise': 1644407609,\n   'sunset': 1644444329,\n   'moonrise': 1644422460,\n   'moonset': 1644388320,\n   'moon_phase': 0.28,\n   'temp': {'day': 14.92,\n    'min': 2.26,\n    'max': 19.83,\n    'night': 10.08,\n    'eve': 11.05,\n    'morn': 3.79},\n   'feels_like': {'day': 9.14, 'night': 3.63, 'eve': 11.05, 'morn': -8.52},\n   'pressure': 1016,\n   'humidity': 71,\n   'dew_point': 15.62,\n   'wind_speed': 8.43,\n   'wind_deg': 300,\n   'wind_gust': 16.02,\n   'weather': [{'id': 600,\n     'main': 'Snow',\n     'description': 'light snow',\n     'icon': '13d'}],\n   'clouds': 56,\n   'pop': 0.33,\n   'snow': 0.51,\n   'uvi': 2.29},\n  {'dt': 1644508800,\n   'sunrise': 1644493928,\n   'sunset': 1644530813,\n   'moonrise': 1644510840,\n   'moonset': 1644478500,\n   'moon_phase': 0.31,\n   'temp': {'day': 21.81,\n    'min': 8.8,\n    'max': 22.01,\n    'night': 9.46,\n    'eve': 16.41,\n    'morn': 11.68},\n   'feels_like': {'day': 15.03, 'night': 0.05, 'eve': 9.25, 'morn': 5.83},\n   'pressure': 1006,\n   'humidity': 94,\n   'dew_point': 29.3,\n   'wind_speed': 6.2,\n   'wind_deg': 272,\n   'wind_gust': 17,\n   'weather': [{'id': 600,\n     'main': 'Snow',\n     'description': 'light snow',\n     'icon': '13d'}],\n   'clouds': 94,\n   'pop': 0.7,\n   'snow': 2.27,\n   'uvi': 1.1},\n  {'dt': 1644595200,\n   'sunrise': 1644580246,\n   'sunset': 1644617298,\n   'moonrise': 1644599520,\n   'moonset': 1644568620,\n   'moon_phase': 0.34,\n   'temp': {'day': 15.17,\n    'min': 2.52,\n    'max': 16.65,\n    'night': 11.16,\n    'eve': 14.95,\n    'morn': 5.09},\n   'feels_like': {'day': 9.57, 'night': 2.14, 'eve': 7.18, 'morn': -3.42},\n   'pressure': 1014,\n   'humidity': 76,\n   'dew_point': 17.38,\n   'wind_speed': 5.55,\n   'wind_deg': 191,\n   'wind_gust': 11.25,\n   'weather': [{'id': 802,\n     'main': 'Clouds',\n     'description': 'scattered clouds',\n     'icon': '03d'}],\n   'clouds': 47,\n   'pop': 0.34,\n   'uvi': 1.75},\n  {'dt': 1644681600,\n   'sunrise': 1644666563,\n   'sunset': 1644703782,\n   'moonrise': 1644688800,\n   'moonset': 1644658320,\n   'moon_phase': 0.37,\n   'temp': {'day': 21.24,\n    'min': -10.28,\n    'max': 21.61,\n    'night': -10.28,\n    'eve': 7.41,\n    'morn': 19.63},\n   'feels_like': {'day': 12.07, 'night': -22.88, 'eve': -5.19, 'morn': 10.44},\n   'pressure': 1010,\n   'humidity': 98,\n   'dew_point': 29.59,\n   'wind_speed': 10.74,\n   'wind_deg': 267,\n   'wind_gust': 36.6,\n   'weather': [{'id': 600,\n     'main': 'Snow',\n     'description': 'light snow',\n     'icon': '13d'}],\n   'clouds': 100,\n   'pop': 0.66,\n   'snow': 2.45,\n   'uvi': 0.74},\n  {'dt': 1644768000,\n   'sunrise': 1644752878,\n   'sunset': 1644790266,\n   'moonrise': 1644778560,\n   'moonset': 1644747720,\n   'moon_phase': 0.4,\n   'temp': {'day': -4.45,\n    'min': -12.17,\n    'max': 0.68,\n    'night': -6.97,\n    'eve': 0.23,\n    'morn': -11.9},\n   'feels_like': {'day': -4.45, 'night': -6.97, 'eve': 0.23, 'morn': -24.5},\n   'pressure': 1029,\n   'humidity': 65,\n   'dew_point': -6.56,\n   'wind_speed': 6.91,\n   'wind_deg': 319,\n   'wind_gust': 9.95,\n   'weather': [{'id': 804,\n     'main': 'Clouds',\n     'description': 'overcast clouds',\n     'icon': '04d'}],\n   'clouds': 100,\n   'pop': 0,\n   'uvi': 1},\n  {'dt': 1644854400,\n   'sunrise': 1644839191,\n   'sunset': 1644876749,\n   'moonrise': 1644868680,\n   'moonset': 1644836640,\n   'moon_phase': 0.44,\n   'temp': {'day': -5.12,\n    'min': -13.97,\n    'max': -2,\n    'night': -9.49,\n    'eve': -3.84,\n    'morn': -10.5},\n   'feels_like': {'day': -16.47,\n    'night': -19.82,\n    'eve': -16.44,\n    'morn': -19.3},\n   'pressure': 1020,\n   'humidity': 69,\n   'dew_point': -6.23,\n   'wind_speed': 6.2,\n   'wind_deg': 308,\n   'wind_gust': 8.72,\n   'weather': [{'id': 801,\n     'main': 'Clouds',\n     'description': 'few clouds',\n     'icon': '02d'}],\n   'clouds': 18,\n   'pop': 0,\n   'uvi': 1},\n  {'dt': 1644940800,\n   'sunrise': 1644925504,\n   'sunset': 1644963233,\n   'moonrise': 1644959040,\n   'moonset': 1644925140,\n   'moon_phase': 0.47,\n   'temp': {'day': -2.83,\n    'min': -8.66,\n    'max': 1.02,\n    'night': -2.72,\n    'eve': 0.75,\n    'morn': -5.62},\n   'feels_like': {'day': -15.43,\n    'night': -11.79,\n    'eve': -7.58,\n    'morn': -16.47},\n   'pressure': 1030,\n   'humidity': 79,\n   'dew_point': -0.74,\n   'wind_speed': 9.37,\n   'wind_deg': 303,\n   'wind_gust': 26.06,\n   'weather': [{'id': 600,\n     'main': 'Snow',\n     'description': 'light snow',\n     'icon': '13d'}],\n   'clouds': 100,\n   'pop': 0.26,\n   'snow': 0.62,\n   'uvi': 1}]}\n</code></pre>","tags":["streamlit","python","async","intermediate"]},{"location":"blog/weather-forecast-dashboard/#fetch-for-many-locations","title":"Fetch for Many Locations","text":"<p>Using the same scaffolding as the Wikipedia asynchronous scrape, the helper code for the main streamlit app also relies on <code>httpx</code> to fetch 48 responses quickly.</p> Python<pre><code>async def async_get_one_call_data(client: httpx.AsyncClient, lat: float, lon: float) -&gt; dict:\n    \"\"\"Given http client and valid lat lon, retrieves open weather \"One call\" API data\n\n    Args:\n        client (httpx.AsyncClient): To make requests. See httpx docs\n        lat (float): lat of the desired location\n        lon (float): lon of the desired location\n\n    Returns:\n        dict: json response from Open Weather One Call\n    \"\"\"\n    endpoint = get_one_call_endpoint(lat, lon)\n    response = await client.get(endpoint)\n    return response.json()\n\n\nasync def gather_one_call_weather_data(lat_lon_pairs: list) -&gt; list:\n    \"\"\"Given list of tuples of lat, lon pairs, will asynchronously fetch the one call open weather api data for those pairs\n\n    Args:\n        lat_lon_pairs (list): Destinations to get data for\n\n    Returns:\n        list: List of dictionaries which are json responses from open weather\n    \"\"\"\n    async with httpx.AsyncClient() as client:\n        tasks = [\n            asyncio.ensure_future(async_get_one_call_data(client, lat, lon))\n            for lat, lon in lat_lon_pairs\n        ]\n        one_call_weather_data = await asyncio.gather(*tasks)\n        return one_call_weather_data\n</code></pre>","tags":["streamlit","python","async","intermediate"]},{"location":"blog/weather-forecast-dashboard/#web-app-component","title":"Web App Component","text":"<p>Goals from the start: - Usable UI for comparing / viewing weather on 48 locations (mobile-friendly for hikers) - Not sluggish to load data or click through page after page to get different mountains / times - Good uptime</p> <p>Other technical considerations: - Obeying API limits     - API key security - Streamlit resource limits     - Cloud host or self host</p>","tags":["streamlit","python","async","intermediate"]},{"location":"blog/weather-forecast-dashboard/#caching-data","title":"Caching Data","text":"<p>There are 2 main points of loading data in the app:</p> <ul> <li>Load the list of mountains, heights, lats, lons</li> <li>Fetch live data from OpenWeatherMap for all locations</li> </ul> <p>With Streamlit, decorating a function with <code>@st.cache()</code> will save the computed result so that it can be loaded faster by the next user!</p>","tags":["streamlit","python","async","intermediate"]},{"location":"blog/weather-forecast-dashboard/#caching-mountain-data","title":"Caching Mountain Data","text":"<p>The first list is static, and purely for convenience of fetching columns I load it in with <code>pandas</code>. (In hindsight I could have at least reset the index after sorting).</p> <p>Leaving the default arguments lets this dataset get cached indefinitely (until the app gets shut down / restarted)</p> <p>note: <code>st.cache</code> decorators commented out in notebook</p> Python<pre><code>import pandas as pd\n# import streamlit as st\n\n#@st.cache()\ndef load_metadata() -&gt; pd.DataFrame:\n    \"\"\"Function to read mountain lat, lon, and other metadata and cache results\n\n    Returns:\n        pd.DataFrame: df containing information for 48 mountains\n    \"\"\"\n    df = pd.read_csv(\"./data/mountains.csv\")\n    df = df.sort_values(\"name\")\n    return df\n\nload_metadata().head()\n</code></pre>       .dataframe tbody tr th:only-of-type {         vertical-align: middle;     }      .dataframe tbody tr th {         vertical-align: top;     }      .dataframe thead th {         text-align: right;     }      name link lat lon height_ft     29 Bondcliff https://en.wikipedia.org/wiki/Bondcliff 44.153056 -71.531111 4265   35 Cannon Mountain https://en.wikipedia.org/wiki/Cannon_Mountain_... 44.156389 -71.698333 4100   8 Carter Dome https://en.wikipedia.org/wiki/Carter_Dome 44.267222 -71.178889 4832   33 East Peak Mount Osceola https://en.wikipedia.org/wiki/East_Peak_Mount_... 44.006111 -71.520556 4340   43 Galehead Mountain https://en.wikipedia.org/wiki/Galehead_Mountain 44.185278 -71.573611 4024","tags":["streamlit","python","async","intermediate"]},{"location":"blog/weather-forecast-dashboard/#caching-weather-data","title":"Caching Weather Data","text":"<p>With this dataset I don't want to cache things indefinitely. In fact, we want it to update as often as the API limits will allow us to query it!</p> <p>Setting a <code>ttl</code> or \"Time To Live\" value in <code>st.cache(ttl=...)</code> will cause the cache to bust if the precomputed result is longer than the provided time.</p> <p>We'll set the <code>ttl</code> to 60 minutes to respect OpenWeatherMaps.</p> <p>This means that if 100 users all open the app within 59 minutes of one another then only 1 request to <code>load_data()</code> would actually go to OpenWeatherMaps. The other 99 requests would use the cached result.</p> <p>When any user opens it 61 minutes after the first user, the cache will be busted and another request to OpenWeatherMaps will refresh all of the 48 mountains' weather data in the app.</p> Python<pre><code>pass\n# @st.cache(ttl=60 * 60)\ndef load_data(lat_lon_pairs: list) -&gt; list:\n    \"\"\"Function to fetch Open Weather data and cache results\n\n    Args:\n        lat_lon_pairs (list): Destinations to get data for\n\n    Returns:\n        list: List of dictionaries which are json responses from open weather\n    \"\"\"\n    data = asyncio.run(gather_one_call_weather_data(lat_lon_pairs))\n    return data\n</code></pre>","tags":["streamlit","python","async","intermediate"]},{"location":"blog/weather-forecast-dashboard/#bonuses","title":"Bonuses","text":"","tags":["streamlit","python","async","intermediate"]},{"location":"blog/weather-forecast-dashboard/#display-future-forecast","title":"Display future forecast","text":"<p>Hikers don't need to know just the weather right now. They also need to know the next few hours' forecast.</p> <p>The OpenWeatherMaps data provides temperature and weather event forecasts hourly.</p> <p>So how about a row across the screen with 5 hours of data in 5 even columns.</p> <p>Feels good on desktop, but a horrendous amount of scrolling past locations you don't care about on mobile.</p> <p><code>st.expander()</code> provides a way to tuck sections away in a drop down hide/expand section.</p> <p>Then using <code>st.columns()</code> we can get an iterator over <code>x</code> amount of columns. Zipping this with the hourly results starting from the next hour gives a nice way to match up layout to data. It also gives some flexibility for how many columns to include.</p> Python<pre><code>response = load_data()[0]\ncurrent_temperature = round(response[\"current\"][\"temp\"], 1)\n\nwith st.expander(\"Expand for future forecast:\"):\n    for col, entry in zip(st.columns(5), response[\"hourly\"][1:]):\n        col.write(f\"{clean_time(entry['dt'])}\")\n\n        temperature = round(entry[\"temp\"], 1)\n        col.metric(\n            \"Temp (F)\", temperature, round(temperature - current_temperature, 1)\n        )\n        current_temperature = temperature\n</code></pre>","tags":["streamlit","python","async","intermediate"]},{"location":"blog/weather-forecast-dashboard/#jump-link-table","title":"Jump Link Table","text":"<p>Using the app on mobile even with expander sections was too much scrolling.</p> <p>I thought a Markdown table of links would be more straightforward, but I wound up doing a bunch of string mangling to get it running.</p> <p>Having anchors on most commands such as <code>st.title()</code> is great for in-page navigation</p> Python<pre><code>def get_mtn_anchor(mountain: str) -&gt; str:\n    anchor = mountain.lower().replace(\" \", \"-\")\n    return f\"[{mountain}](#{anchor})\"\n\nmountains = load_metadata()\n\ntable = []\n\ntable.append(\"| Mountains |  |  |\")\ntable.append(\"|---|---|---|\")\nfor left, middle, right in zip(\n    mountains.name[::3], mountains.name[1::3], mountains.name[2::3]\n):\n    table.append(\n        f\"| {get_mtn_anchor(left)} | {get_mtn_anchor(middle)} | {get_mtn_anchor(right)} |\"\n    )\n# st.markdown(\"\\n\".join(table))\n\"\\n\".join(table)\n</code></pre> Text Only<pre><code>\"| Mountains |  |  |\\n|---|---|---|\\n| [Bondcliff](#bondcliff) | [Cannon Mountain](#cannon-mountain) | [Carter Dome](#carter-dome) |\\n| [East Peak Mount Osceola](#east-peak-mount-osceola) | [Galehead Mountain](#galehead-mountain) | [Middle Carter Mountain](#middle-carter-mountain) |\\n| [Middle Tripyramid](#middle-tripyramid) | [Mount Adams](#mount-adams) | [Mount Bond](#mount-bond) |\\n| [Mount Cabot](#mount-cabot) | [Mount Carrigain](#mount-carrigain) | [Mount Eisenhower](#mount-eisenhower) |\\n| [Mount Field](#mount-field) | [Mount Flume](#mount-flume) | [Mount Garfield](#mount-garfield) |\\n| [Mount Hale](#mount-hale) | [Mount Hancock](#mount-hancock) | [Mount Hancock](#mount-hancock) |\\n| [Mount Isolation](#mount-isolation) | [Mount Jackson](#mount-jackson) | [Mount Jefferson](#mount-jefferson) |\\n| [Mount Lafayette](#mount-lafayette) | [Mount Liberty](#mount-liberty) | [Mount Lincoln](#mount-lincoln) |\\n| [Mount Madison](#mount-madison) | [Mount Monroe](#mount-monroe) | [Mount Moosilauke](#mount-moosilauke) |\\n| [Mount Moriah](#mount-moriah) | [Mount Osceola](#mount-osceola) | [Mount Passaconaway](#mount-passaconaway) |\\n| [Mount Pierce](#mount-pierce) | [Mount Tecumseh](#mount-tecumseh) | [Mount Tom](#mount-tom) |\\n| [Mount Washington](#mount-washington) | [Mount Waumbek](#mount-waumbek) | [Mount Whiteface](#mount-whiteface) |\\n| [Mount Willey](#mount-willey) | [Mount Zealand](#mount-zealand) | [North Kinsman Mountain](#north-kinsman-mountain) |\\n| [North Tripyramid](#north-tripyramid) | [North Twin Mountain](#north-twin-mountain) | [Owl's Head (Franconia)](#owl's-head-(franconia)) |\\n| [South Carter Mountain](#south-carter-mountain) | [South Kinsman Mountain](#south-kinsman-mountain) | [South Twin Mountain](#south-twin-mountain) |\\n| [West Bond](#west-bond) | [Wildcat D Mountain](#wildcat-d-mountain) | [Wildcat Mountain](#wildcat-mountain) |\"\n</code></pre>","tags":["streamlit","python","async","intermediate"]},{"location":"guides/","title":"Guides","text":"<p>Links on the left.</p> <p>These pages combine topics from one or more Resources into useful How-To style guides.</p> <p>Don't be afraid to gloss over certain prerequisites and focus on getting things working before fully understanding the individual parts.</p>"},{"location":"guides/geoparquet_intro/","title":"GeoParquet Intro","text":"<p>See it Live: https://geoparquet.streamlit.app/</p> <p>Repo: https://github.com/gerardrbentley/explore-geoparquet</p>"},{"location":"guides/geoparquet_intro/#idea","title":"Idea","text":"<p>Start with World Bank country data source. Use GeoPandas to load the source GeoJSON into memory. Use GeoPandas to write parquet and/or feather files.</p> <p>Use GeoPandas to load parquet data in Streamlit application. Display dataframe Display spatial data with GeoPandas plotting + Folium</p>"},{"location":"guides/geoparquet_intro/#start","title":"Start","text":"Bash<pre><code>mkdir geoparquet\ncd geoparquet \ntouch streamlit_app.py requirements.txt convert_to_parquet.py\necho 'venv' &gt; .gitignore\ngit init\npython -m venv venv\n. ./venv/bin/activate\npython -m pip install geopandas pyarrow streamlit streamlit-folium\n# dev\npython -m pip install notebook\n# for deploy\npython -m pip install -r requirements.txt\n</code></pre>"},{"location":"guides/geoparquet_intro/#conversion","title":"Conversion","text":"<p>Couldn't load zip off the bat on my local. Unzipped spatial folder and that directory worked fine.</p> Python<pre><code>import geopandas\nimport pyarrow\n\ngdf = geopandas.read_file(SOURCE_SPATIAL_PATH)\ngdf.to_parquet(PARQUET_PATH)\ngdf.to_feather(FEATHER_PATH)\n</code></pre> <p>Timed loading from directory and parquet / feather. Parquet and Feather were similarly faster, with feather being slightly smaller fil size.</p>"},{"location":"guides/geoparquet_intro/#data-display","title":"Data Display","text":"<p>Streamlit doesn't happily display geospatial data in dataframes out of the box. So displaying the data table means we won't include geometry.</p> Python<pre><code>import streamlit as st\nimport geopandas\nimport folium\nimport pyarrow\nfrom streamlit_folium import st_folium\n\n\n@st.experimental_singleton\ndef load_country_data(file_path: str) -&gt; geopandas.GeoDataFrame:\n    gdf = geopandas.read_parquet(file_path)\n    return gdf\n\ncountries = load_country_data(\"countries.parquet\")\ncols = countries.columns\ndata_cols = cols[~cols.isin([\"geometry\"])]\n\nst.dataframe(countries[data_cols])\n</code></pre> <p>That's all that's needed to the non-geospatial columns.</p>"},{"location":"guides/geoparquet_intro/#map-display","title":"Map Display","text":"<p>Streamlit folium helps to connect GeoPandas with leaflet and streamilt.</p> Python<pre><code>m = folium.Map(location=[0, 0], zoom_start=2)\n\ncolumn_choice = st.selectbox(\"Column to Display\", cols, cols.get_loc(\"POP_RANK\"))\ntooltip_columns = st.multiselect(\"Columns for Tooltip\", cols, default_display_cols)\n\ncountries.explore(column_choice, cmap=\"Blues\", m=m, tooltip=tooltip_columns)  \n\nst_folium(m, returned_objects=[])\n</code></pre> <p>Passing in a list of <code>default_display_cols</code> makes the map a little less noisy when hovering.</p>"},{"location":"guides/golang_postgres_stack/","title":"Project Golang Postgres Stack","text":"<p>This app looks up locations from a static table.</p> <p>For simplicity, it's all treated as text.</p> <p>Code available on Github branch <code>read-postgres</code></p> Bash<pre><code># Open terminal to desired project directory\ncd ~/projects\n# Make directory for the project\nmkdir places\n# Enter the directory\ncd places\n# Make docker-compose config file\ntouch docker-compose.yml\n\n# Make directory for the go backend server\nmkdir backend\n# Enter the go\ncd backend\n# Initialize go project\ngo mod init github.com/gerardrbentley/places\n# Install postgres driver and mock interface\ngo get -u github.com/jackc/pgx/v5/pgxpool\ngo get -u github.com/spacetab-io/pgxpoolmock\n# Make file for entrypoint and basic integration testing\ntouch main.go main_test.go\n# Make package for API handlers\nmkdir handler\n# Make file for web handler code\ntouch handler/place.go handler/place_test.go\n# Make package for data services\nmkdir service\n# Make files for service code\ntouch service/place.go service/place_test.go\n# Make dockerfile for building\ntouch Dockerfile\n# Run tests\ngo test ./...\n\n# Run stack. (cd from `backend` to `places`)\ncd ..\ndocker-compose up --build\n# Tear down stack and database\ndocker-compose down --volumes --remove-orphans\n# Enter psql (with running stack)\ndocker-compose exec database psql -U places_user -d places\n# Run database management script\ndocker-compose exec database psql -U places_user -d places -f /tmp/sample_data/load_places_data.sql\n\n\n# With database in background\ndocker-compose up -d database\n# Run Just Go server\nDB_CONNECTION=postgres://places_user:places_password@localhost:5432/places go run main.go\n# Manually request endpoint (from another terminal)\ncurl -v \"http://localhost:5000/place?name=First\"\n# Build and run executable\ngo build main.go\nDB_CONNECTION=postgres://places_user:places_password@localhost:5432/places ./main\n</code></pre>"},{"location":"guides/golang_postgres_stack/#main-entrypoint","title":"Main Entrypoint","text":"<p>Adds setup for services and database connection over basic web service.</p> main.go<pre><code>package main\n\nimport (\n    \"context\"\n    \"fmt\"\n    \"log\"\n    \"net/http\"\n    \"os\"\n\n    . \"github.com/gerardrbentley/places/handler\"\n    . \"github.com/gerardrbentley/places/service\"\n    \"github.com/jackc/pgx/v5/pgxpool\"\n)\n\nfunc setupHandlers(placeService PlaceService) *http.ServeMux {\n    h := http.NewServeMux()\n    h.Handle(\"/place\", PlaceHandler(placeService))\n\n    return h\n}\n\nfunc main() {\n    log.Println(\"Starting Up....\")\n\n    log.Println(\"Connecting to postgres...\")\n    dbpool, err := pgxpool.New(context.Background(), os.Getenv(\"DB_CONNECTION\"))\n    if err != nil {\n        fmt.Fprintf(os.Stderr, \"Unable to create connection pool: %v\\n\", err)\n        os.Exit(1)\n    }\n    defer dbpool.Close()\n    placeService := NewPgPlaceService(dbpool)\n\n    h := setupHandlers(placeService)\n    log.Fatal(http.ListenAndServe(\":5000\", h))\n}\n</code></pre>"},{"location":"guides/golang_postgres_stack/#handler","title":"Handler","text":"<p>Handler can accept a service on initialization for use in serving requests.</p> <p>This layer can be responsible for serialization, but we'll re-use the struct from the service.</p> handler/place.go<pre><code>package handler\n\nimport (\n    \"encoding/json\"\n    \"log\"\n    \"net/http\"\n\n    . \"github.com/gerardrbentley/places/service\"\n)\n\ntype PlaceError struct {\n    Error string `json:\"error\"`\n}\n\nfunc PlaceHandler(service PlaceService) http.Handler {\n    return http.HandlerFunc(func(w http.ResponseWriter, r *http.Request) {\n        w.Header().Set(\"Content-Type\", \"application/json\")\n\n        searchName := r.URL.Query().Get(\"name\")\n        if searchName == \"\" {\n            w.WriteHeader(http.StatusBadRequest)\n            p := PlaceError{Error: \"no name query\"}\n            if err := json.NewEncoder(w).Encode(p); err != nil {\n                log.Println(err.Error())\n                w.WriteHeader(http.StatusInternalServerError)\n            }\n            return\n        }\n\n        records, err := service.LookupByName(searchName)\n        if err != nil {\n            log.Println(err.Error())\n            w.WriteHeader(http.StatusNotFound)\n            return\n        }\n\n        if err := json.NewEncoder(w).Encode(records); err != nil {\n            log.Println(err.Error())\n            w.WriteHeader(http.StatusInternalServerError)\n        }\n    })\n}\n</code></pre>"},{"location":"guides/golang_postgres_stack/#service","title":"Service","text":"<p>By accepting an interface for our database Query tool we can freely mock out the database returns.</p> <p>The database access could be abstracted to a \"repository\" layer or a \"db\" package.</p> service/place.go<pre><code>package service\n\nimport (\n    \"context\"\n    \"errors\"\n    \"log\"\n\n    \"github.com/jackc/pgx/v5\"\n)\n\ntype PlaceRecord struct {\n    FoodResourceType  string `db:\"food_resource_type\" json:\"-\"`\n    Agency            string `db:\"agency\" json:\"name\"`\n    Location          string `db:\"location\" json:\"location\"`\n    OperationalStatus string `db:\"operational_status\" json:\"-\"`\n    OperationalNotes  string `db:\"operational_notes\" json:\"notes\"`\n    WhoTheyServe      string `db:\"who_they_serve\" json:\"-\"`\n    Address           string `db:\"address\" json:\"address\"`\n    Latitude          string `db:\"latitude\" json:\"latitude\"`\n    Longitude         string `db:\"longitude\" json:\"longitude\"`\n    PhoneNumber       string `db:\"phone_number\" json:\"phone_number\"`\n    Website           string `db:\"website\" json:\"website\"`\n    DaysOrHours       string `db:\"days_or_hours\" json:\"days_or_hours\"`\n    DateUpdated       string `db:\"date_updated\" json:\"-\"`\n}\n\ntype PlaceService interface {\n    LookupByName(searchName string) ([]PlaceRecord, error)\n}\n\ntype DbPool interface {\n    Query(ctx context.Context, sql string, args ...any) (pgx.Rows, error)\n}\n\ntype PgPlaceService struct {\n    dbpool DbPool\n}\n\nfunc NewPgPlaceService(dbpool DbPool) *PgPlaceService {\n    return &amp;PgPlaceService{dbpool: dbpool}\n}\n\nfunc (s *PgPlaceService) LookupByName(searchName string) ([]PlaceRecord, error) {\n    rows, err := s.dbpool.Query(context.Background(),\n        `select \"food_resource_type\",\n            \"agency\",\n            \"location\",\n            \"operational_status\",\n            \"operational_notes\",\n            \"who_they_serve\",\n            \"address\",\n            \"latitude\",\n            \"longitude\",\n            \"phone_number\",\n            \"website\",\n            \"days_or_hours\",\n            \"date_updated\" \n        from place where tsv @@ plainto_tsquery($1);`,\n        searchName)\n    if err != nil {\n        log.Printf(\"Query failed: %v\\n\", err)\n        return []PlaceRecord{}, errors.New(\"Database Error\")\n    }\n    records, err := pgx.CollectRows(rows, pgx.RowToStructByName[PlaceRecord])\n    if err == pgx.ErrNoRows || len(records) == 0 {\n        return []PlaceRecord{}, errors.New(\"Not Found\")\n    } else if err != nil {\n        log.Printf(\"Parsing Record failed: %v\\n\", err)\n        return []PlaceRecord{}, errors.New(\"Database Record Corrupted\")\n    }\n    return records, nil\n}\n\ntype InMemoryPlaceService struct {\n    LookupByNameFunc func(searchName string) ([]PlaceRecord, error)\n}\n\nfunc (s *InMemoryPlaceService) LookupByName(searchName string) ([]PlaceRecord, error) {\n    return s.LookupByNameFunc(searchName)\n}\n\ntype InMemoryDbPool struct {\n    QueryFunc func(ctx context.Context, sql string, args ...any) (pgx.Rows, error)\n}\n\nfunc (p InMemoryDbPool) Query(ctx context.Context, sql string, args ...any) (pgx.Rows, error) {\n    return p.QueryFunc(ctx, sql, args)\n}\n</code></pre>"},{"location":"guides/golang_postgres_stack/#happy-path-integration-test","title":"Happy Path Integration Test","text":"<p>This utilizes an in-memory service to mock the responses.</p> <p>A full e2e test would ideally use the running postgres container.</p> main_test.go<pre><code>package main\n\nimport (\n    \"encoding/json\"\n    \"fmt\"\n    \"log\"\n    \"net/http\"\n    \"net/http/httptest\"\n    \"testing\"\n\n    . \"github.com/gerardrbentley/places/service\"\n)\n\nfunc TestLookupPlaceRoute(t *testing.T) {\n    s := InMemoryPlaceService{\n        LookupByNameFunc: func(searchName string) ([]PlaceRecord, error) {\n            return []PlaceRecord{{Agency: searchName}}, nil\n        },\n    }\n    h := setupHandlers(&amp;s)\n\n    w := httptest.NewRecorder()\n    mockName := \"seattle\"\n    req, _ := http.NewRequest(\"GET\", fmt.Sprintf(\"/place?name=%s\", mockName), nil)\n    h.ServeHTTP(w, req)\n\n    if w.Code != http.StatusOK {\n        t.Errorf(\"Not OK %v\", w.Code)\n    }\n    results := []PlaceRecord{}\n    if err := json.NewDecoder(w.Body).Decode(&amp;results); err != nil {\n        log.Fatalln(err)\n    }\n    result := results[0]\n    if result.Agency != mockName {\n        t.Errorf(\"Not same Name: %v\", result.Agency)\n    }\n}\n</code></pre>"},{"location":"guides/golang_postgres_stack/#happy-path-handler-test","title":"Happy Path Handler Test","text":"handler/place_test.go<pre><code>package handler\n\nimport (\n    \"context\"\n    \"encoding/json\"\n    \"fmt\"\n    \"log\"\n    \"net/http\"\n    \"net/http/httptest\"\n    \"testing\"\n\n    . \"github.com/gerardrbentley/places/service\"\n)\n\nfunc TestPlaceHandler(t *testing.T) {\n    ctx := context.Background()\n    mockName := \"seattle\"\n\n    req, _ := http.NewRequestWithContext(\n        ctx,\n        http.MethodPatch,\n        fmt.Sprintf(\"/place?name=%s\", mockName),\n        nil,\n    )\n    w := httptest.NewRecorder()\n\n    s := InMemoryPlaceService{\n        LookupByNameFunc: func(searchName string) ([]PlaceRecord, error) {\n            return []PlaceRecord{{Agency: searchName}}, nil\n        },\n    }\n    r := http.NewServeMux()\n    r.Handle(\"/place\", PlaceHandler(&amp;s))\n    r.ServeHTTP(w, req)\n\n    if w.Code != http.StatusOK {\n        t.Errorf(\"Not OK %v\", w.Code)\n    }\n    var results []PlaceRecord\n    if err := json.NewDecoder(w.Body).Decode(&amp;results); err != nil {\n        log.Fatalln(err)\n    }\n    result := results[0]\n    if result.Agency != mockName {\n        t.Errorf(\"Not same name: %v\", result.Agency)\n    }\n}\n</code></pre>"},{"location":"guides/golang_postgres_stack/#happy-path-service-test","title":"Happy Path Service Test","text":"<p>Since the postgres rows is an interface, we can hack in a mock.</p> service/place_test.go<pre><code>package service\n\nimport (\n    \"context\"\n    \"reflect\"\n    \"testing\"\n    \"unsafe\"\n\n    \"github.com/jackc/pgx/v5\"\n    \"github.com/jackc/pgx/v5/pgconn\"\n)\n\ntype MockRows struct {\n    values   [][]any\n    visitIdx int\n}\n\nfunc (r *MockRows) Close() {}\n\nfunc (r *MockRows) Err() error {\n    return nil\n}\n\nfunc (r *MockRows) CommandTag() pgconn.CommandTag {\n    return pgconn.NewCommandTag(\"mock\")\n}\n\nfunc (r *MockRows) FieldDescriptions() []pgconn.FieldDescription {\n    return []pgconn.FieldDescription{}\n}\n\nfunc (r *MockRows) Next() bool {\n    return r.visitIdx &lt; len(r.values)\n}\n\ntype mockScanner struct {\n    ptrToStruct any\n}\n\nfunc (r *MockRows) Scan(dest ...any) error {\n    v := reflect.ValueOf(dest[0]).Elem().FieldByName(\"ptrToStruct\")\n    v = reflect.NewAt(v.Type(), unsafe.Pointer(v.UnsafeAddr())).Elem()\n\n    for i, value := range r.values[r.visitIdx] {\n        n := v.Elem().Elem().Field(i)\n        n.Set(reflect.ValueOf(value))\n    }\n\n    r.visitIdx = r.visitIdx + 1\n\n    return nil\n}\n\nfunc (r *MockRows) Values() ([]any, error) {\n    return r.values[r.visitIdx], nil\n}\n\nfunc (r *MockRows) RawValues() [][]byte {\n    return nil\n}\n\nfunc (r *MockRows) Conn() *pgx.Conn {\n    return nil\n}\n\nfunc TestLookupByName(t *testing.T) {\n    mockName := \"seattle\"\n    mockPool := InMemoryDbPool{\n        QueryFunc: func(ctx context.Context, sql string, args ...any) (pgx.Rows, error) {\n            pgxrows := MockRows{\n                values: [][]any{\n                    {\n                        \"meal\",\n                        mockName,\n                    },\n                    {\n                        \"food bank\",\n                        \"Fremont \" + mockName,\n                    },\n                },\n            }\n            return &amp;pgxrows, nil\n        },\n    }\n    s := PgPlaceService{dbpool: mockPool}\n    records, _ := s.LookupByName(mockName)\n    record := records[0]\n    if record.Agency != mockName {\n        t.Errorf(\"Not expected: %v\", record.Agency)\n    }\n}\n</code></pre>"},{"location":"guides/golang_postgres_stack/#dockerfile","title":"Dockerfile","text":"<p>Running a database container with your webserver means you need networking.</p> <p>Docker-compose is a straightforward way to handle this networking.</p> <p>A go application can be built and ran in a Docker container, optionally distroless (comment last <code>FROM</code> and <code>COPY</code> steps to maintain shell access).</p> backend/Dockerfile<pre><code>FROM golang:1.19.3-buster AS build\n\nWORKDIR /src/github.com/gerardrbentley/places/\n\nCOPY go.mod .\nCOPY go.sum .\n\nRUN go mod download\n\nCOPY . /src/github.com/gerardrbentley/places/\nRUN CGO_ENABLED=0 GO111MODULE=on GOOS=linux go build -o /bin/app &amp;&amp; \\\n    chmod 111 /bin/app\n\nFROM gcr.io/distroless/base as final\nCOPY --from=build /bin/app /bin/app\nCMD [\"/bin/app\"]\n</code></pre>"},{"location":"guides/golang_postgres_stack/#docker-compose-file","title":"Docker Compose File","text":"<p>This allows one command to spin up and down all or individual database and webserver.</p> docker-compose.yml<pre><code>services:\n  backend:\n    build: ./backend\n    environment:\n      - DB_CONNECTION=postgres://places_user:places_password@database:5432/places\n    ports:\n      - \"5000:5000\"\n    restart: always\n  database:\n    image: postgres:15.1\n    command: [\"postgres\", \"-c\", \"log_statement=all\", \"-c\", \"log_destination=stderr\"]\n    environment:\n      PGDATA: /var/lib/postgresql/data/pgdata/\n      POSTGRES_HOST: database\n      POSTGRES_PORT: 5432\n      POSTGRES_DB: places\n      POSTGRES_USER: places_user\n      POSTGRES_PASSWORD: places_password\n    ports:\n      - \"5432:5432\"\n    restart: always\n    volumes:\n      - ./sample_data:/tmp/sample_data\n      - postgres_data:/var/lib/postgresql/data/pgdata\n\nvolumes:\n  postgres_data:\n</code></pre>"},{"location":"guides/golang_postgres_stack/#database-management-script","title":"Database Management Script","text":"<p>Data source from http://www.seattle.gov/humanservices/: <code>sample_data/Emergency_Food_and_Meals_Seattle_and_King_County.csv</code></p> <p>To load the table into postgres and add a text search index we can use a sql script such as the following:</p> sample_data/load_places_data.sql<pre><code>drop table if exists place;\n\ncreate table place (\n    \"food_resource_type\" text,\n    \"agency\" text,\n    \"location\" text,\n    \"operational_status\" text,\n    \"operational_notes\" text,\n    \"who_they_serve\" text,\n    \"address\" text,\n    \"latitude\" text,\n    \"longitude\" text,\n    \"phone_number\" text,\n    \"website\" text,\n    \"days_or_hours\" text,\n    \"date_updated\" text,\n    \"tsv\"         tsvector\n);\n\ncreate trigger tsvectorupdate before insert or update\non place for each row execute procedure\ntsvector_update_trigger(\n    tsv, 'pg_catalog.english', \"agency\", \"location\", \"operational_notes\"\n);\n\ncreate index index_pages_on_tsv on place using gin (tsv);\n\n\\copy place(\"food_resource_type\", \"agency\", \"location\", \"operational_status\", \"operational_notes\", \"who_they_serve\", \"address\", \"latitude\", \"longitude\", \"phone_number\", \"website\", \"days_or_hours\", \"date_updated\") from '/tmp/sample_data/Emergency_Food_and_Meals_Seattle_and_King_County.csv' with null as E'\\'\\'' delimiter ',' CSV HEADER\n</code></pre> <p>And that's a wrap.</p>"},{"location":"guides/golang_postgres_streamlit_stack/","title":"Project Golang Postgres Streamlit Stack","text":"<p>This app looks up Seattle Emergency Food and Meal info from a static table.</p> <p>Code available on Github branch <code>streamlit-reader</code></p>"},{"location":"guides/golang_postgres_streamlit_stack/#preface","title":"Preface","text":"<p>Starts from where Project Golang Postgres Stack leaves off.</p> Bash<pre><code># Starting in project directory\ncd places\n# Make folder for streamlit app\nmkdir frontend\n# Make files for app and building\ntouch frontend/streamlit_app.py frontend/requirements.txt frontend/Dockerfile\n# Enter frontend to setup requirements\ncd frontend\n# establish virtual environment\npython -m venv venv\n. ./venv/bin/activate\n# install necessary packages\npython -m pip install streamlit httpx\n</code></pre>"},{"location":"guides/golang_postgres_streamlit_stack/#frontend-consumer","title":"Frontend Consumer","text":"<p>Goals:</p> <ul> <li>web frontend to interact with backend API</li> <li>text box for entering search terms</li> <li>data display as a table</li> <li>map out locations that have latitude and longitude information</li> </ul> frontend/streamlit_app.py<pre><code>import streamlit as st\nimport httpx\nimport os\nimport json\n\nBACKEND_HOST = os.getenv(\"BACKEND_HOST\", \"http://127.0.0.1:5000\")\n\nst.header(\"Search Seattle Emergency Food Locations\")\n\nquery = st.text_input(\"Search term\")\nsearch_url = f\"{BACKEND_HOST}/place\"\n\nresponse = httpx.get(search_url, params={\"name\": query})\ntry:\n    records = response.json()\n    st.dataframe(records)\nexcept (json.decoder.JSONDecodeError, st.errors.StreamlitAPIException):\n    st.warning(f\"Error from Web Request Code: {response.status_code}\")\n    st.stop()\n\nmap_data = []\nfor record in records:\n    try:\n        record[\"latitude\"] = float(record[\"latitude\"])\n        record[\"longitude\"] = float(record[\"longitude\"])\n        map_data.append(record)\n    except ValueError:\n        pass\n\nst.map(map_data)\n</code></pre>"},{"location":"guides/golang_postgres_streamlit_stack/#requirements","title":"Requirements","text":"<p>Python <code>requirements.txt</code> file works fine for specifying package versions needed.</p> frontend/requirements.txt<pre><code>httpx==0.23.3\nstreamlit==1.17.0\n</code></pre>"},{"location":"guides/golang_postgres_streamlit_stack/#dockerfile","title":"Dockerfile","text":"<p>To build the streamlit app in docker we need a dockerfile</p> frontend/Dockerfile<pre><code># pull official base image\nFROM python:3.10-buster\n\n# Don't buffer logs or write pyc\nENV PYTHONUNBUFFERED 1\nENV PYTHONDONTWRITEBYTECODE 1\n\n# Set Virtual env as active python environment\nENV VIRTUAL_ENV=/opt/venv\nRUN python3 -m venv $VIRTUAL_ENV\nENV PATH=\"$VIRTUAL_ENV/bin:$PATH\"\n\n# Install all requirements\nCOPY requirements.txt /tmp/\nRUN pip install --upgrade pip &amp;&amp; pip install --no-cache-dir -r /tmp/requirements.txt\n\n# Run as non-root user\nRUN useradd --create-home appuser\nWORKDIR /home/appuser\nUSER appuser\n\nCOPY . .\n\nENTRYPOINT [ \"python\", \"-m\", \"streamlit\", \"run\", \"streamlit_app.py\"]\n</code></pre>"},{"location":"guides/golang_postgres_streamlit_stack/#compose-update","title":"Compose Update","text":"<p>Adding a new <code>service</code> entry is all that's needed to get this spun up with <code>docker-compose up --build</code></p> docker-compose.yml<pre><code>services:\n  frontend:\n    ports:\n      - \"8501:8501\"\n    build: ./frontend\n    environment:\n      - BACKEND_HOST=http://backend:5000\n</code></pre> <p>BONUS: mount the frontend folder as a volume for hot-reloading on code changes.</p>"},{"location":"guides/golang_webserver/","title":"Project Golang Webserver","text":"Bash<pre><code># Open terminal to desired project directory\ncd ~/projects\n# Make directory for the project\nmkdir days-until-api\n# Enter the directory\ncd days-until-api\n# Initialize go project\ngo mod init github.com/gerardrbentley/days-until-api\n# Make file for entrypoint and basic integration testing\ntouch main.go main_test.go\n# Make package for API handlers\nmkdir handler\n# Make file for web app code\ntouch handler/daysuntil.go handler/daysuntil_test.go\n# Run tests\ngo test ./...\n# Run Server\ngo run main.go\n# Manually request endpoint (from another terminal)\ncurl \"localhost:5000/daysuntil?date=2024-01-03\"\n# Build and run executable\ngo build main.go\n./main\n</code></pre>"},{"location":"guides/golang_webserver/#main-entrypoint","title":"Main Entrypoint","text":"main.go<pre><code>package main\n\nimport (\n    \"log\"\n    \"net/http\"\n\n    . \"github.com/gerardrbentley/days-until-api/handler\"\n)\n\nfunc setupHandlers() *http.ServeMux {\n    h := http.NewServeMux()\n    h.Handle(\"/daysuntil\", DaysUntilHandler())\n\n    return h\n}\n\nfunc main() {\n    log.Println(\"Starting Up....\")\n    h := setupHandlers()\n    log.Fatal(http.ListenAndServe(\":5000\", h))\n}\n</code></pre>"},{"location":"guides/golang_webserver/#handler","title":"Handler","text":"handler/daysuntil.go<pre><code>package handler\n\nimport (\n    \"encoding/json\"\n    \"log\"\n    \"net/http\"\n    \"time\"\n)\n\ntype DaysUntilError struct {\n    Error string `json:\"error\"`\n}\n\ntype DaysUntilResponse struct {\n    Days int `json:\"days\"`\n}\n\nfunc DaysUntilHandler() http.Handler {\n    return http.HandlerFunc(func(w http.ResponseWriter, r *http.Request) {\n        w.Header().Set(\"Content-Type\", \"application/json\")\n\n        rawDate := r.URL.Query().Get(\"date\")\n        layout := \"2006-01-02\"\n        requestedDate, err := time.Parse(layout, rawDate)\n\n        if err != nil {\n            log.Println(err.Error())\n            w.WriteHeader(http.StatusBadRequest)\n            p := DaysUntilError{Error: \"not a valid date in format YYYY-MM-DD\"}\n            if err := json.NewEncoder(w).Encode(p); err != nil {\n                log.Println(err.Error())\n                w.WriteHeader(http.StatusInternalServerError)\n            }\n            return\n        }\n\n        now := time.Now()\n        today := time.Date(now.Year(), now.Month(), now.Day(), 0, 0, 0, 0, time.UTC)\n        difference := requestedDate.Sub(today)\n        numDays := int(difference.Hours() / 24)\n\n        p := DaysUntilResponse{Days: numDays}\n        if err := json.NewEncoder(w).Encode(p); err != nil {\n            log.Println(err.Error())\n            w.WriteHeader(http.StatusInternalServerError)\n        }\n    })\n}\n</code></pre>"},{"location":"guides/golang_webserver/#happy-path-integration-test","title":"Happy Path Integration Test","text":"main_test.go<pre><code>package main\n\nimport (\n    \"encoding/json\"\n    \"fmt\"\n    \"log\"\n    \"net/http\"\n    \"net/http/httptest\"\n    \"testing\"\n    \"time\"\n\n    . \"github.com/gerardrbentley/days-until-api/handler\"\n)\n\nfunc TestDaysUntilRoute(t *testing.T) {\n    h := setupHandlers()\n\n    w := httptest.NewRecorder()\n    twoDaysFuture := time.Now().Add(time.Hour * 24 * 2).Format(\"2006-01-02\")\n    req, _ := http.NewRequest(\"GET\", fmt.Sprintf(\"/daysuntil?date=%s\", twoDaysFuture), nil)\n    h.ServeHTTP(w, req)\n\n    if w.Code != http.StatusOK {\n        t.Errorf(\"Not OK %v\", w.Code)\n    }\n    result := DaysUntilResponse{}\n    if err := json.NewDecoder(w.Body).Decode(&amp;result); err != nil {\n        log.Fatalln(err)\n    }\n    if result.Days != 2 {\n        t.Errorf(\"Not 2 days: %v\", result.Days)\n    }\n}\n</code></pre>"},{"location":"guides/golang_webserver/#happy-path-handler-test","title":"Happy Path Handler Test","text":"handler/daysuntil_test.go<pre><code>package handler\n\nimport (\n    \"context\"\n    \"encoding/json\"\n    \"fmt\"\n    \"log\"\n    \"net/http\"\n    \"net/http/httptest\"\n    \"testing\"\n    \"time\"\n)\n\nfunc TestDaysUntilHandler(t *testing.T) {\n    ctx := context.Background()\n    twoDaysFuture := time.Now().Add(time.Hour * 24 * 2).Format(\"2006-01-02\")\n\n    req, _ := http.NewRequestWithContext(\n        ctx,\n        http.MethodPatch,\n        fmt.Sprintf(\"/daysuntil?date=%s\", twoDaysFuture),\n        nil,\n    )\n    w := httptest.NewRecorder()\n\n    r := http.NewServeMux()\n    r.Handle(\"/daysuntil\", DaysUntilHandler())\n    r.ServeHTTP(w, req)\n\n    if w.Code != http.StatusOK {\n        t.Errorf(\"Not OK %v\", w.Code)\n    }\n    result := DaysUntilResponse{}\n    if err := json.NewDecoder(w.Body).Decode(&amp;result); err != nil {\n        log.Fatalln(err)\n    }\n    if result.Days != 2 {\n        t.Errorf(\"Not 2 days: %v\", result.Days)\n    }\n}\n</code></pre>"},{"location":"guides/golang_webserver/#gin-framework","title":"Gin Framework","text":"<p>Alternative to using standard library http servemux</p> Bash<pre><code>go get -u github.com/gin-gonic/gin\n</code></pre> main.go<pre><code>package main\n\nimport \"github.com/gin-gonic/gin\"\n\nfunc ginHandler(c *gin.Context) {\n    rawDate := c.Query(\"date\")\n\n        c.JSON(http.StatusOK, gin.H{\"same_date\": rawDate})\n}\n\nfunc setupRouter() *gin.Engine {\n    r := gin.Default()\n    r.GET(\"/daysuntil\", ginHandler)\n    return r\n}\n\nfunc main() {\n    r := setupRouter()\n    r.Run(\":5000\")\n}\n</code></pre>"},{"location":"resources/","title":"Resources","text":"<p>Links on the left side.</p> <p>These pages are for my own notes and links to helpful resources on a variety of topics.</p> <p>Many of these Resource pages will have a related Setup page for the tool.</p>"},{"location":"resources/ci_cd/","title":"CI/CD","text":"<p>CI/CD is a method of Continuosly Integrating new code with old code and Continuously Deploying new code to different infrastructures / environments.</p> <p>Goal: Allow smooth code updates</p>","tags":["CI/CD"]},{"location":"resources/circleci/","title":"CircleCI","text":"<p>CircleCI is a platform / product for Continuous Integration and Continuous Delivery.</p> <p>It integrates with Github (and Github Enterprise) and Bitbucket. (i.e. Not compatible with Gitlab, which has its own workflow).</p>","tags":["CircleCI","CI/CD"]},{"location":"resources/circleci/#setup","title":"Setup","text":"<p>2 Steps:</p> <ol> <li>Presence of <code>.circleci/config.yml</code> file in Github repo </li> <li>\"Set Up Project\" for that repo on app.circleci.com</li> </ol>","tags":["CircleCI","CI/CD"]},{"location":"resources/circleci/#configyml-file","title":"Config.yml File","text":"<p>Varies depending on complexity of application</p> <p>Configuration Reference</p> <p>Generally a good structure to list out 'jobs' (tasks) at the top and workflows (jobs to perform) below</p> Text Only<pre><code>version: 2.1\njobs:\n    build-and-push-docker-image:\n        docker:\n            - image: cimg/base:stable-18.04\n        steps:\n            - checkout\n            - setup_remote_docker\n            - run:\n                name: Build Docker Image\n                command: |\n                    ls -lah ./\n                    echo '^^^ ls current working directory ^^^'\n                    docker build -t docker-repository/image-name .\n            - run:\n                name: Push To Dockerhub\n                command: |\n                    docker build -t docker-repository/image-name .\n                    echo $DOCKERHUB_PASS | docker login -u $DOCKERHUB_USERNAME --password-stdin\n                    docker push docker-repository/image-name\nworkflows:\n    update-docker-image:\n        jobs:\n            - build-and-push-docker-image\n</code></pre>","tags":["CircleCI","CI/CD"]},{"location":"resources/circleci/#isolating-git-branches","title":"Isolating Git Branches","text":"<p>Use Cases:  - Deploying to a production environment, staging environment, and testing environment based on git branch - Not wasting resources trying to test / deploy branches that are for development / breaking changes</p> <p>In <code>config.yml</code>, add <code>filters:</code> property to jobs that should only run when code is committed to specific branches</p> Text Only<pre><code>workflows:\n    update-docker-image:\n        jobs:\n            - build-and-push-docker-image:\n                filters:\n                    branches:\n                        only: main\n    update-production-docker-image:\n        jobs:\n            - build-and-push-docker-image:\n                filters:\n                    branches:\n                        only: production\n</code></pre>","tags":["CircleCI","CI/CD"]},{"location":"resources/circleci/#prevent-typo-hell","title":"Prevent Typo Hell","text":"<p>Requires CircleCI Local CLI</p> <p>Use <code>circleci config validate</code> from the project root directory to validate config file before pushing to version control.</p> <p>NOTE Install Local CLI (sudo to install to /usr/local/bin): <code>curl -fLSs https://raw.githubusercontent.com/CircleCI-Public/circleci-cli/master/install.sh | sudo bash</code></p>","tags":["CircleCI","CI/CD"]},{"location":"resources/curl/","title":"curl","text":"<p>Curl is a CLI utility for making web (and other) requests.</p>","tags":["curl"]},{"location":"resources/curl/#level-up","title":"Level Up","text":"<p>Combine <code>curl</code> with <code>jq</code> to pretty print JSON from responses.</p> Bash<pre><code>curl -v \"http://localhost:5001/places?name=First\" | jq '.'\n</code></pre>","tags":["curl"]},{"location":"resources/curl/#install-macos","title":"Install MacOS","text":"Bash<pre><code>brew install curl \nbrew install jq\n</code></pre>","tags":["curl"]},{"location":"resources/frontend/","title":"Frontend","text":"<p>Term associated with writing the code that produces the (appealing) visual aspects of an application.</p> <p>Generally encompasses websites, mobile sites / apps, and desktop apps. </p>","tags":["frontend","full-stack","web"]},{"location":"resources/frontend/#tools-web","title":"Tools: Web","text":"<p>HTML, CSS, and Javascript are the foundation of Frontend Dev.</p> <p>HTML: Structure of a webpage</p> <p>CSS: Styling of the page</p> <p>Javascript: Functionality of the page</p>","tags":["frontend","full-stack","web"]},{"location":"resources/frontend/#frontend-js-tools","title":"Frontend JS Tools","text":"<p>Vue, React, Angular: Javascript libraries that make it significantly easier to manipulate elements of a webpage and seperate logic of different components / widgets into small files.</p>","tags":["frontend","full-stack","web"]},{"location":"resources/frontend/#frontend-python-tools","title":"Frontend Python Tools","text":"<p>Templating: Django and Flask use Template languages to render python variables / backend data dynamically (Flask's is called Jinja)</p> <p>This is converted to HTML in the 'render' process. Usually Javascript is used to make the page more functional than having clickable links.</p> <p>LEFT FIELD Brython is an adaptation of Python code for the Browser</p>","tags":["frontend","full-stack","web"]},{"location":"resources/gatsby/","title":"Gatsby","text":"<p>Gatsby (Homepage) is a Site Generator for React. </p> <p>Open source Javascript framework for building web apps (frontend). </p>","tags":["React","Frontend","SSR","Web"]},{"location":"resources/gatsby/#quickstart","title":"Quickstart","text":"<p>Source</p> <p>Pre-requisite: npm</p> <p>Install cli tool Bash<pre><code>npm install -g gatsby-cli\n</code></pre></p> <p>Create Site Bash<pre><code>gatsby new site-folder-name https://github.com/gatsbyjs/gatsby-starter-hello-world\n</code></pre></p> <p>Run Dev server Bash<pre><code>cd site-folder-name\ngatsby develop --host 0.0.0.0 --port 8000\n</code></pre></p> <p>Visit Site Home: [http://localhost:8000]</p> <p>Visit GraphQL query analyzer: [http://localhost:8000/___graphql]</p>","tags":["React","Frontend","SSR","Web"]},{"location":"resources/gatsby/#data-for-pages-with-graphql","title":"Data For Pages with GraphQL","text":"<p>Using GraphQL isn't required with Gatsby, but it's built in and official plugins parse data into a GraphQL schema.</p>","tags":["React","Frontend","SSR","Web"]},{"location":"resources/gatsby/#pages","title":"Pages","text":"<p>Pages can use variables (via <code>pageContext</code>) in queries to narrow down / specify results.</p> <p>This functionality extends to templates used with <code>createPage</code>.</p> <p>Example (Blog Posts with Path): - <code>gatsby-node.js</code> calls <code>createPage</code> with a <code>pathSlug</code> variable in the context:</p> JavaScript<pre><code>const posts = data.allMarkdownRemark.edges\n\nposts.forEach(({node}, index) =&gt; {\n    const path = node.frontmatter.path\n    createPage({\n        path,\n        component: blogPostTemplate,\n        context: {\n            pathSlug: path,\n            prev: index === 0 ? posts[posts.length - 1].node.frontmatter : posts[index - 1].node.frontmatter,\n            next: index === (posts.length - 1) ? posts[0].node.frontmatter : posts[index + 1].node.frontmatter\n        }\n    })\n}\n</code></pre> <ul> <li><code>blogPostTemplate.js</code> can make a graphql query using <code>pathSlug</code> to specify which post we're building a page for</li> </ul> JavaScript<pre><code>export const query = graphql`\n    query($pathSlug: String!) {\n        markdownRemark(frontmatter: { path: {eq: $pathSlug}}) {\n            html\n            frontmatter {\n                title\n            }\n        }\n    }\n`\n</code></pre>","tags":["React","Frontend","SSR","Web"]},{"location":"resources/gatsby/#components","title":"Components","text":"<p>Components / Layouts cannot use variables in queries.</p> <p>Must use Gatsby's <code>StaticQuery</code>, where static can be read as 'no variables' or 'constant.'</p> <p>Hook method <code>useStaticQuery</code> Example (Layout fetching site information): - <code>Layout.js</code> contains a Functional compenent, in which it calls <code>useStaticQuery</code> with a graphql query:</p> JavaScript<pre><code>const Layout: React.FunctionComponent&lt;LayoutProps&gt; = ({ next, prev, children }) =&gt; {\n    const data = useStaticQuery(\n        graphql`\n            query {\n                site {\n                    siteMetadata {\n                        title\n                        description\n                        author\n                        year\n                    }\n                }\n            }\n        `\n    )\n    console.log({data})\n    const { title, description, author, year } = data.site.siteMetadata\n    #... use `title`, `description`, `author`, `year` in jsx \n</code></pre> <p>Reminder: Access Grapphql schema at <code>localhost:8000/___graphql</code> (With your site's IP address if WSL2 or remote) </p>","tags":["React","Frontend","SSR","Web"]},{"location":"resources/gatsby/#testing","title":"Testing","text":"<p>Using Jest is pretty well supported with Gatsby and React;</p>","tags":["React","Frontend","SSR","Web"]},{"location":"resources/gatsby/#setup","title":"Setup","text":"<ol> <li>Install testing dependencies for react / jest</li> </ol> Bash<pre><code>npm install --save-dev jest babel-jest react-test-renderer babel-preset-gatsby identity-obj-proxy\n</code></pre> <ol> <li><code>jest.config.js</code> file</li> </ol> JavaScript<pre><code>module.exports = {\n    transform: {\n        \"^.+\\\\.[jt]sx?$\": `&lt;rootDir&gt;/jest-preprocess.js`,\n    },\n    moduleNameMapper: {\n        \".+\\\\.(css|styl|less|sass|scss)$\": `identity-obj-proxy`,\n        \".+\\\\.(jpg|jpeg|png|gif|eot|otf|webp|svg|ttf|woff|woff2|mp4|webm|wav|mp3|m4a|aac|oga)$\": `&lt;rootDir&gt;/__mocks__/file-mock.js`,\n    },\n    testPathIgnorePatterns: [`node_modules`, `\\\\.cache`, `&lt;rootDir&gt;.*/public`],\n    transformIgnorePatterns: [`node_modules/(?!(gatsby)/)`],\n    globals: {\n        __PATH_PREFIX__: ``,\n    },\n    testURL: `http://localhost`,\n    setupFiles: [`&lt;rootDir&gt;/loadershim.js`],\n    collectCoverage: true,\n    collectCoverageFrom: [\"src/**/*.{js,jsx,ts,tsx}\"]\n}\n</code></pre> <ol> <li><code>jest-preprocess.js</code></li> </ol> JavaScript<pre><code>const babelOptions = {\n  presets: [\"babel-preset-gatsby\"],\n}\n\nmodule.exports = require(\"babel-jest\").createTransformer(babelOptions)\n</code></pre> <ol> <li><code>loadershim.js</code></li> </ol> JavaScript<pre><code>global.___loader = {\n  enqueue: jest.fn(),\n}\n</code></pre>","tags":["React","Frontend","SSR","Web"]},{"location":"resources/gatsby/#resetting-test-cache","title":"Resetting Test Cache","text":"<p>Jest and React make unit tests by rendering the code and saving a snapshot of that render to compare the tested code to.</p> <p>Whenever you change code that changes the data on the page (changing html), run the following to update snapshots:</p> Bash<pre><code>jest -u\n# Or if test is in your `package.json` scripts\nnpm test -- -u\n</code></pre>","tags":["React","Frontend","SSR","Web"]},{"location":"resources/gatsby/#testing-components-with-staticquery","title":"Testing Components with <code>StaticQuery</code>","text":"<p>Components with <code>StaticQuery</code> / <code>useStaticQuery</code> receives data via context rather than props.</p> <p>Create Components in a way that there is a 'Pure' version; Given the same inputs it will yield the same output.</p> <p>The Unpure version will make the graphql <code>StaticQuery</code> and pass that data to the Pure component</p> <p>Example (Layout that requires site metadata):</p> <ol> <li>Components <code>src/components/layout.tsx</code></li> </ol> <p>NOTE: export both the Pure component and the Main version</p> Text Only<pre><code>import * as React from 'react'\nimport { Link, useStaticQuery, graphql } from 'gatsby'\nimport { MarkdownRemark } from '../types'\n\nexport interface LayoutProps {\n    next?: MarkdownRemark\n    prev?: MarkdownRemark\n}\n\nexport interface PureLayoutProps {\n    data: {\n        site: {\n            siteMetadata: {\n                title: string\n                description: string\n            }\n        }\n    }\n    next?: MarkdownRemark\n    prev?: MarkdownRemark\n}\n\nexport const PureLayout: React.FunctionComponent&lt;PureLayoutProps&gt; = ({ data, next, prev, children }) =&gt; {\n    const { title, description } = data.site.siteMetadata\n    let header = (\n        &lt;div&gt;\n            &lt;div&gt;\n                &lt;div&gt;\n                    &lt;Link to='/'&gt;Index Page&lt;/Link&gt;\n                &lt;/div&gt;\n                &lt;div&gt;\n                    {prev &amp;&amp; &lt;Link to={prev.frontmatter.path}&gt;Prev ({prev.frontmatter.title})&lt;/Link&gt;}\n                &lt;/div&gt;\n            &lt;/div&gt;\n            &lt;div&gt;\n                &lt;h2&gt;{title}&lt;/h2&gt;\n                &lt;p&gt;{description}&lt;/p&gt;\n            &lt;/div&gt;\n            &lt;div&gt;\n                &lt;div&gt;\n                    &lt;a href=\"/#\"&gt;Other Link&lt;/a&gt;\n                &lt;/div&gt;\n                &lt;div&gt;\n                    {next &amp;&amp; &lt;Link to={next.frontmatter.path}&gt;Next ({next.frontmatter.title})&lt;/Link&gt;}\n                &lt;/div&gt;\n            &lt;/div&gt;\n        &lt;/div&gt;\n    )\n\n    return (\n        &lt;div&gt;\n            &lt;header&gt;\n                {header}\n            &lt;/header&gt;\n            &lt;hr&gt;&lt;/hr&gt;\n            &lt;main&gt;\n                {children}\n            &lt;/main&gt;\n        &lt;/div&gt;\n    )\n}\n\nexport const Layout: React.FunctionComponent&lt;LayoutProps&gt; = (props) =&gt; {\n    const data = useStaticQuery(\n        graphql`\n            query {\n                site {\n                    siteMetadata {\n                        title\n                        description\n                    }\n                }\n            }\n        `\n    )\n\n    return &lt;PureLayout {...props} data={data}&gt;&lt;/PureLayout&gt;\n}\n\nexport default Layout\n</code></pre> <ol> <li>Tests <code>__tests__/layout.ts</code></li> </ol> TypeScript<pre><code>import React from 'react'\nimport renderer from 'react-test-renderer'\n\nimport { PureLayout as Layout } from '../src/components/Layout'\n\ndescribe(\"Layout\", () =&gt; {\n  it(\"renders correctly\", () =&gt; {\n    const data = {\n      site: {\n        siteMetadata: {\n          title: \"Brain Blog\",\n          description: \"Notes from random topics, connected; Loosely modeled after Zettelkasten / Roam\",\n        }\n      }\n    }\n    const tree = renderer.create(&lt;Layout data={ data }&gt; &lt;/Layout&gt;).toJSON()\n    expect(tree).toMatchSnapshot()\n  })\n})\n</code></pre>","tags":["React","Frontend","SSR","Web"]},{"location":"resources/gatsby/#typescript","title":"Typescript","text":"","tags":["React","Frontend","SSR","Web"]},{"location":"resources/git/","title":"Git","text":"","tags":["git","beginner"]},{"location":"resources/git/#cloning-and-branching","title":"Cloning and Branching:","text":"","tags":["git","beginner"]},{"location":"resources/git/#fixing-your-local-repo","title":"Fixing your local Repo:","text":"","tags":["git","beginner"]},{"location":"resources/git/#reset-my-local-repo-to-whatever-is-online","title":"Reset my local Repo to whatever is online","text":"<p>If people have made many changes and you haven't caught up, it's sometimes easiest to just reset your local folder to whatever is on the server.</p> <p>For more nuance, see this stack overflow post</p> Text Only<pre><code>git fetch origin\ngit reset --hard origin/master\n</code></pre>","tags":["git","beginner"]},{"location":"resources/git/#git-push-origin-master-fails","title":"git push origin master FAILS","text":"<p>If someone else changed <code>Master</code> and you haven't received those, you may get an error (probably in red and yellow) that resembles the following:</p> Text Only<pre><code>error: failed to push some refs to 'git@pom-itb-gitlab01.campus.pomona.edu:web-lab/web-resources.git'\nhint: Updates were rejected because the remote contains work that you do\nhint: not have locally. This is usually caused by another repository pushing\nhint: to the same ref. You may want to first integrate the remote changes\nhint: (e.g., 'git pull ...') before pushing again.\n</code></pre>","tags":["git","beginner"]},{"location":"resources/git/#solution-1","title":"SOLUTION 1:","text":"<p>Try a simple git pull. If you didn't change the same files that others did then you should receive their changes painlessly.</p> Bash<pre><code>git pull origin master\n</code></pre> <p>This may get you into the land of Vim, a text editor that lives entirely in the terminal, it's ok to be confused, it happens to most people trying to exit vim.</p> <p>To escape vim and save the merge, type the follow key cominations: <code>escape</code> then <code>shift+;</code> (a colon) then <code>w</code> then <code>q</code> then <code>enter</code></p>","tags":["git","beginner"]},{"location":"resources/git/#solution-2","title":"SOLUTION 2:","text":"<p>If the git pull origin master command fails, you may get an error like the following:</p> Bash<pre><code>remote: Total 3 (delta 2), reused 1 (delta 0)\nUnpacking objects: 100% (3/3), done.\nFrom gitlab.com:username/web-resources\n * branch            master     -&gt; FETCH_HEAD\n   f62b1f8..118f52e  master     -&gt; origin/master\nAuto-merging index.html\nCONFLICT (content): Merge conflict in index.html\nAutomatic merge failed; fix conflicts and then commit the result.\n</code></pre> <p>This means that you tried to edit the same file that someone has already updated on the master branch of the server.</p> <p>Opening up your code editor and opening the conflicting files you're trying to merge should give you several options: - Keep only the work that the others added and discard your changes (probably not what you want) - Keep only the work that you added and discard others' changes (also probably not the goal) - Keep both (may need to add more lines, but often we want to collaborate, not clobber) - Keep some of each (some work is mutually exclusive and needs to pick one or the other)</p>","tags":["git","beginner"]},{"location":"resources/git/#broken-wrong-origin-link","title":"Broken / Wrong Origin link:","text":"","tags":["git","beginner"]},{"location":"resources/git/#hard-reset-all-files-fetch-others-work","title":"Hard reset all files (fetch other's work)","text":"","tags":["git","beginner"]},{"location":"resources/git/#fixing-the-remote-repo","title":"Fixing the remote Repo:","text":"","tags":["git","beginner"]},{"location":"resources/git/#pushed-wrong-files-to-gitlab","title":"Pushed wrong files to Gitlab.","text":"<p>Sometimes you have temporary files / images locally that you don't need to push to everyone via Gitlab. Sometimes those images / large files end up in the repo but others don't want to download them when they clone it, so we'll remove them</p>","tags":["git","beginner"]},{"location":"resources/git/#solution","title":"SOLUTION:","text":"<p>Files and folders listed in .gitignore will not be tracked by git (i.e. you can use <code>git add .</code> and it won't wrongfully add gitignored files). Learn more on gitignore syntax (https://www.atlassian.com/git/tutorials/saving-changes/gitignore)[here].</p> <p>After updating .gitignore to list the files you want to remove from remote gitlab,</p> Bash<pre><code>git rm -r --cached .\ngit add .\ngit commit -m \"Removed large files from repo...\"\ngit push origin CURRENT-BRANCH-NAME\n</code></pre> <p>This assumes there are files you need to <code>re-ignore</code> in folders throughout the project. If it's a single folder (<code>./VERY_FULL_FOLDER/</code>) or file (<code>./path/to/one_file_to_remove.py</code>) then you should replace the <code>.</code> in the first command to that path (i.e. <code>./VERY_FULL_FOLDER</code>) and only the files in that folder will be searched.</p>","tags":["git","beginner"]},{"location":"resources/gitlab/","title":"Gitlab","text":"<p>Gitlab is a host for git based projects and organizations!</p> <p>It is a self-hostable alternative to Github</p>","tags":["Gitlab"]},{"location":"resources/golang/","title":"Golang","text":"<p>Golang is a statically typed, compiled language originally developed at Google. It is popular for its simplicity and performance qualities.</p>"},{"location":"resources/graphql/","title":"GraphQL","text":"<p>GraphQL serves data in the form that the query requests it.</p>","tags":["GraphQL","Backend","API","Database"]},{"location":"resources/heroku/","title":"Heroku","text":"<p>Heroku is a Cloud platform that is easily configurable, owned by Salesforce.</p> <p>It no longer has a great free tier</p>","tags":["Heroku"]},{"location":"resources/heroku/#cli-tool","title":"CLI Tool","text":"<p><code>curl https://cli-assets.heroku.com/install.sh | sh</code></p>","tags":["Heroku"]},{"location":"resources/javascript/","title":"Javascript","text":"<p>Javascript runs in the Browser and can control elements on an HTML page, along with fetching data from other website's API's.</p> <p>Makes it a popular tool for Frontend Dev work.</p> <p>It can also run in a Backend server via Node</p>","tags":["Javascript","Backend","Frontend","Web"]},{"location":"resources/linux/","title":"Linux","text":"<p>Linux is a broad term that usually refers to Unix based operating systems used in both desktop and server flavours.</p> <p>Popular distrobutions include:</p> <ul> <li>Ubuntu</li> <li>Debian</li> <li>RHEL</li> <li>Arch</li> </ul>","tags":["Linux"]},{"location":"resources/local_network/","title":"Local Network","text":"<p>Local Network usually refers to the \"intranet\" of devices connected to the same internet connection as your current computer.</p> <p>Note: \"Your Current Computer\" can refer to any client in a Client-Server relationship.</p>","tags":["networks"]},{"location":"resources/local_network/#finding-local-ip-address","title":"Finding Local IP Address","text":"","tags":["networks"]},{"location":"resources/local_network/#macos","title":"MacOS","text":"Bash<pre><code>ipconfig getifaddr en0\nipconfig getifaddr en1\n</code></pre>","tags":["networks"]},{"location":"resources/local_server/","title":"Local Server","text":"","tags":["networks"]},{"location":"resources/local_server/#python","title":"Python","text":"Bash<pre><code>python -m http.server --directory ./site/ 8080\n</code></pre>","tags":["networks"]},{"location":"resources/machine_learning/","title":"Machine Learning","text":"<p>Machine Learning is the term used to describe any computer system that utilizes some \"fitting\" process to make decisions based on input data</p>","tags":["Machine Learning"]},{"location":"resources/markdown/","title":"Markdown","text":"<p>Markdown is a way of writing plain text with ~style~. This Blog is made by converting Markdown files (<code>.md</code>) to HTML content (with a bit of React sparkle)</p> <p>Organization and emphasis increase readability, and the format is widely accepted.</p> <p>It's the Rich Text Markup of choice for Github README files. Examples here include 'Github Flavored Markdown' where noted.</p> <p>Similar in principal, but much less daunting than Org Mode</p>","tags":["Markdown"]},{"location":"resources/markdown/#headers","title":"Headers","text":"","tags":["Markdown"]},{"location":"resources/markdown/#output","title":"Output:","text":"","tags":["Markdown"]},{"location":"resources/markdown/#header-1","title":"Header 1","text":"","tags":["Markdown"]},{"location":"resources/markdown/#header-2","title":"Header 2","text":"","tags":["Markdown"]},{"location":"resources/markdown/#header-3","title":"Header 3","text":"","tags":["Markdown"]},{"location":"resources/markdown/#header-6","title":"Header 6","text":"","tags":["Markdown"]},{"location":"resources/markdown/#input","title":"Input:","text":"Markdown<pre><code># Header 1\n## Header 2\n### Header 3\n###### Header 6\n</code></pre>","tags":["Markdown"]},{"location":"resources/markdown/#line-break","title":"Line Break","text":"","tags":["Markdown"]},{"location":"resources/markdown/#output_1","title":"Output:","text":"","tags":["Markdown"]},{"location":"resources/markdown/#input_1","title":"Input","text":"Markdown<pre><code>---\n</code></pre>","tags":["Markdown"]},{"location":"resources/markdown/#code","title":"Code","text":"","tags":["Markdown"]},{"location":"resources/markdown/#fenced-github-flavored-markdown","title":"Fenced (Github Flavored Markdown)","text":"","tags":["Markdown"]},{"location":"resources/markdown/#output_2","title":"Output:","text":"Python<pre><code>def do_the_things(stuff: [str]):\n    for x in stuff:\n        print(x)\n</code></pre>","tags":["Markdown"]},{"location":"resources/markdown/#input_2","title":"Input:","text":"Text Only<pre><code>```python\ndef do_the_things(stuff: [str]):\n    for x in stuff:\n        print(x)\n```\n</code></pre>","tags":["Markdown"]},{"location":"resources/markdown/#inline-generic-markdown","title":"Inline (Generic Markdown)","text":"","tags":["Markdown"]},{"location":"resources/markdown/#output_3","title":"Output:","text":"<p><code>do_the_things(['one', 'two', 'three'])</code></p>","tags":["Markdown"]},{"location":"resources/markdown/#input_3","title":"Input:","text":"Markdown<pre><code>`do_the_things(['one', 'two', 'three'])`\n</code></pre>","tags":["Markdown"]},{"location":"resources/markdown/#bold-and-italics","title":"Bold and Italics","text":"","tags":["Markdown"]},{"location":"resources/markdown/#output_4","title":"Output:","text":"<p>Italic Text</p> <p>Bold Text</p> <p>~~Striked-Through Text~~</p> <p>Italic And Bold Text</p> <p>Bold And Italic Text</p>","tags":["Markdown"]},{"location":"resources/markdown/#input_4","title":"Input:","text":"Markdown<pre><code>*Italic Text* or _Italic Text_\n\n**Bold Text** or __Bold Text__\n\n~~Striked-Through Text~~\n\n_Italic **And Bold** Text_\n\n__Bold *And Italic* Text__\n</code></pre>","tags":["Markdown"]},{"location":"resources/markdown/#quotes","title":"Quotes","text":"","tags":["Markdown"]},{"location":"resources/markdown/#output_5","title":"Output:","text":"<p>As Abraham Lincoln said:</p>  <p>Don't believe what you read on the internet. That stuff can be dangerous.</p>","tags":["Markdown"]},{"location":"resources/markdown/#input_5","title":"Input:","text":"Markdown<pre><code>As Abraham Lincoln said:\n\n&gt; Don't believe what you read on the internet.\n&gt; That stuff can be dangerous.\n</code></pre>","tags":["Markdown"]},{"location":"resources/markdown/#lists","title":"Lists","text":"","tags":["Markdown"]},{"location":"resources/markdown/#output_6","title":"Output:","text":"<p>Ordered</p> <ol> <li>First Order</li> <li>Second Ordered</li> <li>Still Ordered</li> </ol> <p>Unordered</p> <ul> <li>Unordered Item</li> <li>Another Item</li> <li>A third Item</li> </ul>","tags":["Markdown"]},{"location":"resources/markdown/#input_6","title":"Input:","text":"<p>NOTE: Can use just \"1.\" or incrementing numbers, or one \"1.\" then \"-\" Markdown<pre><code>1. First Order\n1. Second Ordered\n1. Still Ordered\n\nUnordered\n\n- Unordered Item\n- Another Item\n- A third Item\n</code></pre></p>","tags":["Markdown"]},{"location":"resources/markdown/#links","title":"Links","text":"","tags":["Markdown"]},{"location":"resources/markdown/#output_7","title":"Output:","text":"<p>Displayed Text As Link to /markdown</p>","tags":["Markdown"]},{"location":"resources/markdown/#input_7","title":"Input:","text":"Markdown<pre><code>[Displayed Text As Link to /markdown](/markdown)\n</code></pre>","tags":["Markdown"]},{"location":"resources/markdown/#tables-github-flavored-only","title":"Tables (Github Flavored Only)","text":"","tags":["Markdown"]},{"location":"resources/markdown/#output_8","title":"Output:","text":"Column Name Other Column ID     Text omgwtfbbq 1   Number 1234 2","tags":["Markdown"]},{"location":"resources/markdown/#input_8","title":"Input:","text":"Markdown<pre><code>| Column Name | Other Column | ID |\n| ------------|--------------|--- |\n| Text        | omgwtfbbq    | 1 |\n| Number      | 1234         | 2 |\n</code></pre>","tags":["Markdown"]},{"location":"resources/node/","title":"Node JS","text":"<p>Node (Homepage) is JS powered outside of the browser.</p> <p>Since Javascript doesn't have a native runtime environment in popular OS's, Node is necessary as a compiler and interpreter.</p>","tags":["Node","Backend","Full Stack","Web"]},{"location":"resources/open_source/","title":"Open Source","text":"<p>Open Source Software refers to applications / libraries / frameworks whose design and code is publicly available, which allows users to modify and share updated versions.</p> <p>Opposite of \"proprietary\" software, which a company sells to you and does not allow you to modify or usually even inspect.</p> <p>Generally trustworthy because of transparency, but don't click on random links / downloads / scripts.</p>","tags":["Open Source","General"]},{"location":"resources/open_source/#contributing","title":"Contributing","text":"<p>Open Source projects rely on contributors (from 1 to many) to write, bugtest, fix, and document code.</p>","tags":["Open Source","General"]},{"location":"resources/open_source/#paid-open-source","title":"Paid Open Source","text":"<p>Some Projects also rely on funding from users / sponsors. </p> <p>This includes - One Time Donors  - Patronage: donating on a schedule - Hosted / Paid Services: Providing support for the product or providing computing resources that are already running the product.</p>","tags":["Open Source","General"]},{"location":"resources/postgres/","title":"Postgres","text":""},{"location":"resources/postgres/#psql","title":"psql","text":"<p><code>psql</code> is the query interface CLI to postgres.</p> <p>Commands such as:</p> PostgreSQL Console (Psql)<pre><code># Show all tables\n\\dt\n# Describe a table called note\n\\d note\n# Show all functions\n\\df\n# Show the data in a table called note\nSELECT * FROM note;\n# Quit psql\n\\q\n</code></pre>"},{"location":"resources/python/","title":"Python","text":"<p>Python is a dynamic and strongly typed interpreted language (that compiles to byte code) that is overseen by the Python Software Foundation. It is popular for its pseudo-code-like syntax and powerful ecosystem of web, scripting, and machine learning libraries.</p>"},{"location":"resources/react/","title":"React JS","text":"<p>React is a Javascript Frontend framework</p>","tags":["React","Frontend","JS","Web"]},{"location":"resources/react/#hooks","title":"Hooks","text":"<p>Goal: Primitive for sharing stateful logic between components and when re-using components.</p> <p>Caveat: Do this without adding wrappers / services to the component hierarchy</p>","tags":["React","Frontend","JS","Web"]},{"location":"resources/react/#rules","title":"Rules","text":"<ul> <li> <p>Don't call Hooks inside loops, conditions, or nested functions**:</p> <p>May affect the order in which they are called</p> </li> <li> <p>Only call Hooks from React functions**:</p> <p>Call From React Function Components</p> <p>Call from custom Hooks</p> </li> </ul>","tags":["React","Frontend","JS","Web"]},{"location":"resources/react/#usestate-hook","title":"useState Hook","text":"<p><code>const [object, setObject] = useState(object)</code> </p> <p>Calling this in a functional component creates a \"state variable\" for use in the component. This happens when the component is first rendered, then it is accessed on re-render</p> <p>Can reference the \"stateful\" object with the first name in the destructured array (<code>object</code> in this case).</p> <p>Can update the object with the second name in the array (<code>setObject(object + 1)</code>).</p> <p>Same functionality as making a state variable in a class and u sing <code>this.state.object</code> and <code>this.setState({ object: this.state.object + 1})</code></p>","tags":["React","Frontend","JS","Web"]},{"location":"resources/reverse_proxy/","title":"Reverse Proxy","text":"<p>A Reverse Proxy (such as Nginx or Traefik) is a web server that receives and routes web traffic from the external world to protected services.</p>"},{"location":"resources/scp/","title":"SCP","text":""},{"location":"resources/scp/#transfer-with-scp","title":"Transfer with SCP","text":"<p>SCP is the Secure Copy command. It uses your SSH connection to transfer files.</p> Bash<pre><code>scp sshUser1:/files/file.txt user2@host2.com:/files\n</code></pre>"},{"location":"resources/scp/#compress-and-uncompress","title":"Compress and Uncompress","text":"Bash<pre><code># Compress Directory Recursively \nzip -r backup.zip the_folder_name/\n</code></pre>"},{"location":"resources/time_series/","title":"Time Series","text":"<p>For code libraries, blog posts, papers, and talks regarding Time Series data.</p>","tags":["timeseries","resources"]},{"location":"resources/time_series/#papers-and-books","title":"Papers and Books","text":"<ul> <li>Forecasting: Principles and Practice (3rd ed)</li> <li>Rob J Hyndman and George Athanasopoulos</li> <li>Forecasting: theory and practice</li> <li>Review</li> <li>Practical Time Series Analysis</li> <li>Aileen Nielsen</li> </ul>","tags":["timeseries","resources"]},{"location":"resources/time_series/#python-libraries","title":"Python Libraries","text":"<ul> <li>Pandas timeseries functionality</li> <li>Darts</li> <li>sktime</li> </ul>","tags":["timeseries","resources"]},{"location":"resources/time_series/#talks","title":"Talks","text":"<ul> <li>Aileen Nielsen Pydata</li> <li>Time Series Analysis with Python Intermediate (2016)</li> <li>Irregular Time Series (2016)</li> <li>Modern Time Series Analysis (2019)<ul> <li>notebooks</li> <li>referenced background notebooks</li> </ul> </li> </ul>","tags":["timeseries","resources"]},{"location":"resources/time_series/#darts-specific","title":"Darts Specific","text":"<ul> <li>quickstart</li> <li>github notebooks</li> <li>Unit8 Pydata Darts for Time Series Forecasting</li> <li>Unit8 Video Intro to Darts</li> </ul>","tags":["timeseries","resources"]},{"location":"resources/windows/","title":"Windows","text":"","tags":["Windows","OS","General"]},{"location":"resources/windows/#wsl-2","title":"WSL 2","text":"<p>Windows Subsystem For Linux:</p> <p>Allows you to run a Linux distro in Windows; Useful for developing with Linux tools</p>","tags":["Windows","OS","General"]},{"location":"resources/windows/#ip-address-of-virutal-kernel","title":"IP Address of Virutal Kernel","text":"<p>Running a server within WSL2, should host on <code>0.0.0.0</code>, which allows port forwarding to Windows. You'll need to target the Linux</p> Bash<pre><code>ip addr show eth0\n# direct\nip addr show eth0 | grep -oP '(?&lt;=inet\\s)\\d+(\\.\\d+){3}'\n</code></pre> <p>Example: Running a Gatsby blog development server: - <code>gatsby develop --host 0.0.0.0 --port 8000</code> - Browser: <code>http://100.200.300.47:8000</code></p> <p>(Where 100.200.300.47 is the result of running <code>ip addr show</code>)</p>","tags":["Windows","OS","General"]},{"location":"resources/windows/#graphical-applications","title":"Graphical Applications","text":"<p>Will probably be fully supported soon. There is some way to get a graphical application like Emacs to go through XServer, but is wonky and breaks on network changes.</p> <p>Involves vcxsrv, and this github issue</p>","tags":["Windows","OS","General"]},{"location":"resources/windows/#docker-with-wsl2","title":"Docker with WSL2","text":"<p>Pretty seamless using Docker Desktop on host Windows machine.  </p>","tags":["Windows","OS","General"]},{"location":"resources/windows/#windows-terminal","title":"Windows Terminal","text":"<p>Launch terminal tabs / windows for Powershell, Linux, Remote, etc.</p>","tags":["Windows","OS","General"]},{"location":"resources/yaml/","title":"YAML","text":"<p>YAML is a file format used for configuration files and other deployment needs.</p>"},{"location":"setups/","title":"Setups","text":"<p>Links on the left side.</p> <p>These pages are for noting down the way to configure my tools and environments.</p> <p>You'll eventually get a new computer or have to explain how to set something up to a peer; might as well write down the steps!</p>"},{"location":"setups/docker/","title":"Docker","text":"<p>Use Docker Desktop on Mac OS and Windows to install Docker if you're able. I don't tend to use the GUI features, but this will get it set up in the terminal as well.</p>"},{"location":"setups/domain/","title":"Domain Name","text":"<p>Most custom domain (website) names goes for around $12 USD per year, but students can get 3(!) free domains with The Github dev pack (highly recommended if you're eligible).  I use Google Domains for this domain (gerardbentley.com) mostly because I trust they won't have any hidden fees, won't leak my credit card info, and will be reliable. Feel free to use GoDaddy or Namecheap or any other domain name service you prefer. Custom domains are limited to one owner, so don't be disappointed if someone else got what you wanted first (capitalism!).</p>"},{"location":"setups/git/","title":"Git","text":"<p>Git is commonly installed on unix based systems, check if you have it by entering the following in a terminal:</p> Bash<pre><code>git --version\n</code></pre> <p>Follow the official guide if this spits out an error:</p> <p>https://git-scm.com/book/en/v2/Getting-Started-Installing-Git</p>"},{"location":"setups/golang/","title":"Golang","text":"<p>Follow the official guide: https://go.dev/doc/install</p>"},{"location":"setups/ide/","title":"IDE","text":"<p>Recommend following the install instructions from VS Code: https://code.visualstudio.com/download</p>"},{"location":"setups/macos/","title":"macOS","text":"<p>Developer Setup</p> <p>NOTE: at some point you will be prompted to install the \u201cCommand Line Developer Tools\u201d for Xcode. This is a large download. Be prepared.</p>"},{"location":"setups/macos/#python","title":"Python","text":"<p>Mac still ships with python 2, but one day maybe it won\u2019t\u2026</p> <p>I prefer using conda to manage python anyway, so my install notes works just fine for me.</p> Bash<pre><code># Note, this uses macOS Apple M1 installer. Use apple menu -&gt; About This Mac to check if \u201cChip\u201d is Apple M1\n\n# Download\ncurl https://repo.anaconda.com/miniconda/Miniconda3-latest-MacOSX-arm64.sh -o conda_installer.sh\n\n# Install with human prompts, agree to license and conda init\nbash conda_installer.sh\n\n# Cleanup your computer\nrm conda_installer.sh\n\n\n# (if needed for script) Silent Install\nbash conda_installer.sh -b\nsource miniconda3/bin/activate &amp;&amp; conda init zsh \n</code></pre> <p>In a new Terminal, update python version to desired 3.9 or 3.10 or what have you</p> Bash<pre><code>conda install python=3.10\n</code></pre> <p>Add packages you might want for development and testing. (This is where I had to install Xcode developer tools in order to build python packages)</p> Bash<pre><code>pip install black flake8 isort pep8-naming pytest-cov\n</code></pre> <p>Optionally install tools from the Project Jupyter ecosystem:</p> Bash<pre><code># Just jupyter notebook server\npip install notebook \n# Jupyterlab capability\npip install jupyterlab\n</code></pre>"},{"location":"setups/macos/#vs-code","title":"VS Code","text":"<ul> <li>Download the Mac installer (it should choose the correct version for your system)</li> <li>Unzip it from your browser downloads or Finder -&gt; Downloads</li> <li>Move the \u201cVisual Studio Code\u201d Application to Applications if desired</li> <li>Delete the Installer zip if desired</li> </ul> <p>NOTE: I couldn\u2019t find the correct link to download with curl</p>"},{"location":"setups/macos/#extensions","title":"Extensions","text":"<ul> <li>Dracula (theme)</li> <li>Python Specific</li> <li>Python (Microsoft official)</li> <li>Python Docstring Generator (Nils Werner)</li> <li>Even Better TOML (tamasfe)</li> <li>General VS Code</li> <li>indent-rainbow (oderwat): Visualize deeply indented blocks more easily</li> <li>GitLens (Eric Amodio): Quickly check git history of files, branches, lines, etc.</li> <li>Various File Types</li> <li>Markdown All in One (Yu Zhang)</li> <li>Markdown navigation (AlanWalk)</li> <li>markdownlint (David Anson)</li> <li>Paste Image (mushan)</li> <li>Markdown Preview Github Styling (Matt Bierner)</li> <li>Markdown Emoji (Matt Bierner)</li> <li>Docker (Microsoft)</li> <li>YAML (Red Hat)</li> <li>XML (Red Hat)</li> <li>SQL Formatter (adpyke)</li> </ul>"},{"location":"setups/macos/#settings","title":"Settings","text":"<p>I tend to put these in the VS Code settings (<code>cmd + shift + p</code> then type \"settings JSON\" for one way to get there).</p> <p>A lot of these are Python experience and specific to using <code>flake8</code>, <code>black</code>, and <code>isort</code>. These are the first several up to and including the \"autoDocstring\" entry.</p> <p>That said, there's some that are useful for general editing, searching large code bases, and working with the integrated terminal. Shoutout to Harald Kirschner's tips (and presentation).</p> JSON<pre><code>{\n    \"python.condaPath\": \"~/miniconda3/bin/conda\",\n    \"python.defaultInterpreterPath\": \"~/miniconda3/bin/python\",\n    \"python.formatting.provider\": \"black\",\n    \"python.linting.flake8Enabled\": true,\n    \"python.linting.enabled\": true,\n    \"python.testing.pytestEnabled\": true,\n    \"python.sortImports.path\": \"isort\",\n    \"python.sortImports.args\": [\n        \"--profile black\"\n    ],\n    \"python.terminal.executeInFileDir\": true,\n    \"jupyter.askForKernelRestart\": false,\n    \"terminal.integrated.inheritEnv\": false,\n    \"autoDocstring.startOnNewLine\": true,\n    \"workbench.colorTheme\": \"Dracula Soft\",\n    \"telemetry.telemetryLevel\": \"off\",\n    \"security.workspace.trust.untrustedFiles\": \"open\",\n    \"files.autoSave\": \"onFocusChange\",\n    \"terminal.integrated.allowChords\": false,\n    \"search.mode\": \"reuseEditor\",\n    \"search.searchEditor.doubleClickBehaviour\": \"openLocationToSide\",\n    \"files.defaultLanguage\": \"${activeEditorLanguage}\",\n    \"workbench.editor.pinnedTabSizing\": \"shrink\",\n    \"editor.minimap.enabled\": false,\n    \"diffEditor.ignoreTrimWhitespace\": false,\n}\n</code></pre>"},{"location":"setups/macos/#keybindings","title":"Keybindings","text":"<p>Some things are worth changing for productivity and matching other tools.</p> <ul> <li>Jump to beginning of text in the line: <code>ctrl + a</code></li> <li>Jump to top / bottom of file: <code>shift + cmd + ,</code> and <code>shift + cmd + /</code></li> <li>From Emacs muscle memory trying to get something similar</li> </ul> JSON<pre><code>[\n    {\n        \"key\": \"ctrl+a\",\n        \"command\": \"cursorHome\"\n    },\n    {\n        \"key\": \"shift+cmd+,\",\n        \"command\": \"cursorTop\",\n        \"when\": \"textInputFocus\"\n    },\n    {\n        \"key\": \"shift+cmd+/\",\n        \"command\": \"cursorBottom\",\n        \"when\": \"textInputFocus\"\n    },\n    {\n        \"key\": \"shift+cmd+,\",\n        \"command\": \"-editor.action.inPlaceReplace.up\",\n        \"when\": \"editorTextFocus &amp;&amp; !editorReadonly\"\n    }\n]\n</code></pre>"},{"location":"setups/macos/#docker","title":"Docker","text":"<p>For running applications and mocking linux filesystems.</p> <p>Docker recommends installing Rosetta 2 when running on Apple Silicon. This will install it to translate from Intel to M1:</p> Bash<pre><code>softwareupdate --install-rosetta\n</code></pre> <ul> <li>Download the <code>.dmg</code> </li> <li>Open and <code>.dmg</code> then drag the Docker Application into Applications</li> <li>Search or open Docker from spotlight</li> <li>Accept terms and conditions</li> <li>Try to run the hello world application</li> </ul> Bash<pre><code>docker run -rm hello-world\n</code></pre>"},{"location":"setups/macos/#github","title":"Github","text":"<p>I use SSH keys to connect to github / gitlab / etc. for convenience and making the connection work out of the box from tools like VS Code.</p>"},{"location":"setups/macos/#set-default-git-user","title":"Set Default Git User","text":"<p>Set your preferences so tools like VS Code will use these on your commits by default.</p> Bash<pre><code>git config --global user.name \"Your Name Here\" \ngit config --global user.email \"your_email@your_domain.com\"\n</code></pre>"},{"location":"setups/macos/#create-ssh-key","title":"Create SSH Key","text":"<p>It's best to create new keys for new machines as opposed to copying an old key from one machine to another. Use a password if other people have access to your machine, otherwise feel free to leave blank.</p> Bash<pre><code>ssh-keygen -t ed25519 -f ~/.ssh/github_key\n</code></pre>"},{"location":"setups/macos/#add-github-to-ssh-config","title":"Add Github to SSH Config","text":"<p>To make sure this key gets used when we try to authenticate to Github, we'll add the following entry to <code>~/.ssh/config</code>:</p> Text Only<pre><code>Host github.com\n    PreferredAuthentications publickey\n    IdentityFile ~/.ssh/github_key\n</code></pre> <p>To do this with a terminal editor use one of the following:</p> Bash<pre><code>nano ~/.ssh/config\nvim ~/.ssh/config\ned ~/.ssh/config\n</code></pre> <p>To do this with a GUI editor, you have to make sure the file exists first:</p> Bash<pre><code>touch ~/.ssh/config\nopen ~/.ssh/config\n</code></pre>"},{"location":"setups/macos/#add-public-key-to-github","title":"Add Public Key to Github","text":"<p>Copy your public key to the clipboard with the following:</p> Bash<pre><code>pbcopy &lt; ~/.ssh/github_key.pub\n</code></pre> <p>Alternatively you can paste it out and copy by hand:</p> Bash<pre><code>cat ~/.ssh/github_key.pub\n</code></pre> <p>NOTE: be sure you are copying your <code>.pub</code> Public key, and not the Private key</p> <ul> <li>Navigate to your github Account -&gt; Settings -&gt; SSH and GPG keys (under \"Access\", or this link)</li> <li>Select <code>New SSH Key</code></li> </ul>  <ul> <li>Enter a nickname for your machine</li> <li>Paste the Public key into the main text area</li> </ul>"},{"location":"setups/macos/#test-authentication","title":"Test Authentication","text":"<p>This command should spit out a message like <code>Hi {username}! You've successfully authenticated</code> if successful.</p> Bash<pre><code>ssh -T git@github.com\n</code></pre>"},{"location":"setups/macos/#homebrew","title":"Homebrew","text":"<p>See main site: \u201cInstalls the stuff you need that Apple didn\u2019t\u201d</p> Bash<pre><code># Grab and install homebrew from latest GitHub release\n/bin/bash -c \"$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)\"\n# Add to PATH\necho 'eval \"$(/opt/homebrew/bin/brew shellenv)\"' &gt;&gt; /Users/gar/.zprofile\neval \"$(/opt/homebrew/bin/brew shellenv)\"\n</code></pre>"},{"location":"setups/macos/#amethyst","title":"Amethyst","text":"<p>Window Tiling Manager for macOS 10.12+ by ianyh. For splitting windows left and right, full screen, quarters, etc.</p> Bash<pre><code>brew install --cask amethyst\n</code></pre> <p>Open application and grant Accessibility Privacy permissions in Security &amp; Privacy System Preferences</p>"},{"location":"setups/nginx_proxy_manager/","title":"Nginx Proxy Manager","text":"<p>Follow the official guide: https://nginxproxymanager.com/guide/#quick-setup</p>"},{"location":"setups/postgres/","title":"Postgres","text":"<p>Install Binary from source: https://www.postgresql.org/download/</p> <p>Or on Mac use <code>brew install postgres</code></p> <p>Or just use Docker:</p> Bash<pre><code>docker run --name some-postgres -e POSTGRES_PASSWORD=mysecretpassword -d postgres\n</code></pre> <p>Or use Docker within a docker-compose stack:</p> YAML<pre><code>services:\n  db:\n    image: postgres\n    restart: always\n    environment:\n      POSTGRES_PASSWORD: example\n</code></pre>"},{"location":"setups/python/","title":"Python","text":""},{"location":"setups/python/#install-conda","title":"Install Conda","text":"<ul> <li>Use Miniconda, the little sibling of Anaconda. It manages downloading and using different python versions and has some unique \"virtual environment\" features that are especially nice in data science work.<ul> <li>For the most up to date information (i.e. if the following doesn't work), see the official guide and click the link for your OS</li> <li>Browser / GUI method:<ul> <li>Download the installer for Python 3.9 for your computer's OS (or 3.10 if available).<ul> <li>note most Windows computers are 64-bit these days, if you're unsure just double check</li> </ul> </li> <li>Run the file that was downloaded (probably in your <code>~/Downloads</code> folder)<ul> <li>Ask it to add python to your account's <code>PATH</code> variable. </li> <li>Installing just for yourself should be fine for most users</li> </ul> </li> </ul> </li> <li>Command line method:<ul> <li>note right click links on website to get specific base version of Python (latest is fine in most cases)</li> <li>Windows Powershell Bash<pre><code># Download the installer exe\ncurl -uri https://repo.anaconda.com/miniconda/Miniconda3-latest-Windows-x86_64.exe -outfile MinicondaInstaller.exe\n# Install\n&amp; .\\MinicondaInstaller.exe /AddToPath=0 /InstallationType=JustMe /RegisterPython=0 /S /D=%UserProfile%\\Miniconda3\n</code></pre></li> <li>Linux bash (MacOS needs different download link) Bash<pre><code># Download\ncurl -url https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh --output MinicondaInstaller.sh\n# Install\nbash MinicondaInstaller.sh\n</code></pre></li> </ul> </li> </ul> </li> </ul>"},{"location":"setups/python/#setup-conda","title":"Setup Conda","text":"<p>note This glosses over what virtual environments actually are entirely. This is intended to keep the document short. In essence they are folders that each hold a specific version of python and specific versions of python packages.</p> <p>2 Goals: - Python can make new virtual environments for different projects - Allows your code editor to utilize tools like <code>black</code> and <code>flake8</code> to automatically format your code, show you what variables and functions you can use, and catch simple errors and tpyos before you run your code</p>"},{"location":"setups/python/#conda-init","title":"Conda Init","text":"<p>After Installing Miniconda - On windows open Anaconda Powershell Prompt on Windows, terminal for linux     - Enter <code>conda init powershell</code> then close and move on to the next step         - note if Windows fails to do the initialization and instead makes a pop-up about \"Folder Security\", click the pop up then take action to allow the changes.         You'll need to re-run the <code>conda init</code> command. - open up a new terminal     - note if you installed via command line method, you may need to close your current terminal and reopen     - If you see <code>(base)</code> on the left of your current line try running the following command Bash<pre><code>python --version\n</code></pre>     - If you don't see <code>(base)</code>, try running the following command.     Then open a new terminal window and try something like the <code>python --version</code> command again. Bash<pre><code>conda init\n</code></pre></p>"},{"location":"setups/python/#python-base-environment","title":"Python Base Environment","text":"<ul> <li>Install some packages in the <code>(base)</code> conda virtual environment that are useful for code formatting and testing:</li> </ul> Bash<pre><code># Windows Powershell\n# Download the requirements file\n(base)$&gt; curl -uri https://gist.githubusercontent.com/gerardrbentley/b4fd6bdeb9167462cf990160ec246512/raw/e7194b303bb59e518e5d28c65e916cf3ebf1032a/base.requirements.txt -outfile base.requirements.txt\n# Install\n(base)$&gt; python -m pip install -r base.requirements.txt\n\n# Unix\n# Download the requirements file\n(base)$&gt; curl -url https://gist.githubusercontent.com/gerardrbentley/b4fd6bdeb9167462cf990160ec246512/raw/e7194b303bb59e518e5d28c65e916cf3ebf1032a/base.requirements.txt --output base.requirements.txt\n# Install\n(base)$&gt; python -m pip install -r base.requirements.txt\n</code></pre> Text Only<pre><code># formatter\nblack\n# % testing coverage\ncoverage\n# linter + static checker\nflake8\n# interact with jupyter notebooks\nipykernel\n# sort imports in a consistent fashion\nisort\n# let flake8 nag you about object naming\npep8-naming\n# test your code\npytest\n# make pytest and coverage play nicely\npytest-cov\n</code></pre>"},{"location":"setups/python/#configure-vs-code-for-python","title":"Configure VS Code for Python","text":"<p>VS Code is a very popular code editor with powerful features, active community extensions, and gets useful updates frequently. It's by no means the only way to program with Python, but it's one of the most beginner friendly ways to work on a variety of projects.</p>"},{"location":"setups/python/#set-up-test-project","title":"Set up test project","text":"<p>Make a folder called something like <code>test_python</code> and a file inside it called <code>first_script.py</code></p> Bash<pre><code>mkdir test_python\ncd test_python\ntouch first_script.py\n</code></pre>"},{"location":"setups/python/#download-and-install-vs-code","title":"Download and Install VS Code","text":"<ul> <li>Download from your OS link </li> <li>Install with downloaded file</li> <li>Open VS Code for the first time</li> <li>If it won't work or can't open, try reading their getting started guides</li> </ul>"},{"location":"setups/python/#open-your-test-project-in-vs-code","title":"Open your test project in VS Code","text":"<ul> <li><code>File</code> -&gt; <code>Open Folder</code><ul> <li>Select the <code>test_python</code> folder</li> <li>If a \"Trust this workspace\" message pops up, hit agree / trust</li> <li>Open the <code>first_script.py</code> file<ul> <li>Write something like <code>print('Hello There')</code> and save the file</li> </ul> </li> </ul> </li> <li>Go back to your terminal and run <code>python first_script.py</code></li> </ul>"},{"location":"setups/python/#install-extensions-for-python","title":"Install Extensions for Python","text":"<p>In VS Code, open the extensions menu on the left (or with <code>ctrl+shift+x</code>) - Search for and install the following:     - Python (Microsoft) - Reload the window now (close and reopen VS Code) or install some more of the things below then reload (all are optional)</p> <p>Some other nice extensions (not necessary off the bat) - Python Specific     - Python Docstring Generator (Nils Werner)     - Even Better TOML (tamasfe) - General VS Code     - indent-rainbow (oderwat): Visualize deeply indented blocks more easily     - GitLens (Eric Amodio): Quickly check git history of files, branches, lines, etc. - Various File Types     - Markdown TOC (AlanWalk)     - markdownlint (David Anson)     - XML (Red Hat)     - SQL Formatter (adpyke)</p> <p>Bonus: For Emacs users, \"Awesome Emacs Keymap\" will get you most of the way to familiar text editing keybindings</p>"},{"location":"setups/python/#vs-code-set-python-env-and-formatter","title":"VS Code Set Python Env and Formatter","text":"<p>Recommended settings in VS Code</p> <ul> <li>Type <code>ctrl+shift+p</code> then type \"settings json\" and select the entry, add these key-value pairs to file<ul> <li>Edit the <code>CHANGEME:USERNAME</code> with your Windows user. (On Ubuntu based systems use <code>/home/username/miniconda3/bin/python</code>, similar for other Unix systems but with relevant user home directory)</li> </ul> </li> </ul> <p>note: you can instead use <code>ctrl+shift+p</code> and search \"settings ui\" (or use <code>ctrl+,</code>) and then search for each of the keynames below to explore what other settings you might like to try.</p> JSON<pre><code>{\n    \"python.pythonPath\": \"C:\\\\Users\\\\CHANGEME:USERNAME\\\\miniconda3\\\\python.exe\",\n    \"files.autoSave\": \"onFocusChange\",\n    \"editor.wordWrap\": \"on\",\n    \"editor.wordWrapColumn\": 88,\n    \"jupyter.alwaysTrustNotebooks\": true,\n    \"python.linting.flake8Enabled\": true,\n    \"python.linting.flake8Args\": [\n        \"--max-line-length=88\"\n    ],\n    \"python.analysis.typeCheckingMode\": \"basic\",\n    \"workbench.editorAssociations\": [\n        {\n            \"viewType\": \"jupyter.notebook.ipynb\",\n            \"filenamePattern\": \"*.ipynb\"\n        }\n    ],\n    \"python.linting.pylintEnabled\": false,\n    \"python.formatting.provider\": \"black\",\n    \"python.linting.ignorePatterns\": [\n        \".vscode/*.py\",\n        \"**/site-packages/**/*.py\",\n        \"venv/*.py\"\n    ],\n    \"python.testing.pytestEnabled\": true,\n    \"python.venvPath\": \"${workspaceFolder}\"\n}\n</code></pre>"},{"location":"setups/python/#learn-more","title":"Learn More","text":"<p>Feel free to try out / research the following (Python related):</p> <ul> <li>Typing error-filled python code and see how flake8 warns you</li> <li>Start your <code>.py</code> document with <code># %%</code> to make it a VS Code \"Python Interactive\" document (really awesome way to experiment with code blocks and debug small chunks)</li> <li>Use <code>alt+shift+f</code> or <code>ctrl+shift+p</code> and search 'Format Document' to format python according to <code>black</code> standard</li> <li>Text Editing and Searching Guide including stuff like using multiple cursors, search and replace over multiple files, auto save (which is included in my <code>json</code> settings above)</li> <li>Make a new virtual environment for your project with <code>python -m venv venv</code> or look into using <code>conda</code> to manage your environments</li> <li>Look into VS Code's live code sharing and git integrations to collaborate better</li> </ul> <p>Or expand your environment by setting up a few more tools (tangential to Python):</p> <ul> <li>Look into Emmet the auto completer for html files built into VS Code</li> <li>Install Node if you're going to be doing some web development or otherwise need <code>npm</code></li> <li>Install Docker if you want to explore modern container based deployment</li> <li>Find some good Python Reading (Ok one more Python recommendation; some of these are available for low-to-no cost from the authors)</li> </ul>"},{"location":"setups/ssh/","title":"SSH","text":"<p>For more, see github guides: https://docs.github.com/en/authentication/connecting-to-github-with-ssh/about-ssh</p>"},{"location":"setups/ssh/#create-ssh-key","title":"Create SSH Key","text":"<p>It's best to create new keys for new machines as opposed to copying an old key from one machine to another. Use a password if other people have access to your machine, otherwise feel free to leave blank.</p> Bash<pre><code>ssh-keygen -t ed25519 -f ~/.ssh/github_key\n</code></pre>"},{"location":"setups/ssh/#add-github-to-ssh-config","title":"Add Github to SSH Config","text":"<p>To make sure this key gets used when we try to authenticate to Github, we'll add the following entry to <code>~/.ssh/config</code>:</p> Text Only<pre><code>Host github.com\n    PreferredAuthentications publickey\n    IdentityFile ~/.ssh/github_key\n</code></pre> <p>To do this with a terminal editor use one of the following:</p> Bash<pre><code>nano ~/.ssh/config\nvim ~/.ssh/config\ned ~/.ssh/config\n</code></pre> <p>To do this with a GUI editor, you have to make sure the file exists first:</p> Bash<pre><code>touch ~/.ssh/config\nopen ~/.ssh/config\n</code></pre>"},{"location":"setups/ssh/#add-public-key-to-github","title":"Add Public Key to Github","text":"<p>Copy your public key to the clipboard with the following:</p> Bash<pre><code>pbcopy &lt; ~/.ssh/github_key.pub\n</code></pre>"},{"location":"setups/terminal/","title":"Terminal","text":"<p>If you're on MacOS zsh is the default shell in the default <code>Terminal</code> app.</p> <p>I recommend installing Oh My Zsh! for helpful tab completion and shortcuts for common tasks.</p>"},{"location":"setups/vps/","title":"VPS","text":""},{"location":"setups/vps/#cloud-providers","title":"Cloud Providers","text":"<p>Most cloud hosts have a basic VPS option.</p> <p>A small (1 cpu, 1 gb ram) shared machine should be around <code>$5</code> a month.</p> <ul> <li>Amazon AWS</li> <li>Google GCP</li> <li>Microsoft Azure</li> <li>Linode</li> <li>DigitalOcean</li> <li>Vultr</li> </ul>"},{"location":"setups/vps/#operating-system","title":"Operating System","text":"<p>Since it's a server, choose an OS lightweight and configurable enough for your needs.</p> <p>Debian is usually a dependable choice of Linux distribution.</p>"},{"location":"setups/vps/#security","title":"Security","text":"<p>Be sure to learn some aspects of how and why bots try to get into your server.</p> <p>Common hardening practices:</p> <ul> <li>Don't allow <code>root</code> login remotely</li> <li>Use a user with <code>sudo</code> priviliges if needed</li> <li>Don't allow remote logins with password (only SSH key)</li> <li>Use a system such as Universal Firewall <code>ufw</code> to block connections on all ports other than those necessary </li> <li>Usually 80 for HTTP webserver traffic</li> <li>Usually 443 for HTTPS webserver traffic</li> <li>Usually 22 for SSH connections<ul> <li>Many bots will check 22, so this can be configured</li> </ul> </li> <li>Use a system such as <code>fail2ban</code> to block repeated attempts from certain IP addresses</li> </ul> <p>See guides from:</p> <ul> <li>Linode</li> <li>Digital Ocean</li> </ul>"},{"location":"tags/","title":"Tags","text":"<p>Following is a list of used tags:</p>"},{"location":"tags/#api","title":"API","text":"<ul> <li>GraphQL</li> </ul>"},{"location":"tags/#backend","title":"Backend","text":"<ul> <li>GraphQL</li> <li>Javascript</li> <li>Node JS</li> </ul>"},{"location":"tags/#cicd","title":"CI/CD","text":"<ul> <li>CI/CD</li> <li>CircleCI</li> </ul>"},{"location":"tags/#circleci","title":"CircleCI","text":"<ul> <li>CircleCI</li> </ul>"},{"location":"tags/#database","title":"Database","text":"<ul> <li>GraphQL</li> </ul>"},{"location":"tags/#frontend","title":"Frontend","text":"<ul> <li>Gatsby</li> <li>Javascript</li> <li>React JS</li> </ul>"},{"location":"tags/#full-stack","title":"Full Stack","text":"<ul> <li>Node JS</li> </ul>"},{"location":"tags/#general","title":"General","text":"<ul> <li>Open Source</li> <li>Windows</li> </ul>"},{"location":"tags/#gitlab","title":"Gitlab","text":"<ul> <li>Gitlab</li> </ul>"},{"location":"tags/#graphql","title":"GraphQL","text":"<ul> <li>GraphQL</li> </ul>"},{"location":"tags/#heroku","title":"Heroku","text":"<ul> <li>Heroku</li> </ul>"},{"location":"tags/#js","title":"JS","text":"<ul> <li>React JS</li> </ul>"},{"location":"tags/#javascript","title":"Javascript","text":"<ul> <li>Javascript</li> </ul>"},{"location":"tags/#linux","title":"Linux","text":"<ul> <li>Linux</li> </ul>"},{"location":"tags/#machine-learning","title":"Machine Learning","text":"<ul> <li>Machine Learning</li> </ul>"},{"location":"tags/#markdown","title":"Markdown","text":"<ul> <li>Markdown</li> </ul>"},{"location":"tags/#node","title":"Node","text":"<ul> <li>Node JS</li> </ul>"},{"location":"tags/#os","title":"OS","text":"<ul> <li>Windows</li> </ul>"},{"location":"tags/#open-source","title":"Open Source","text":"<ul> <li>Open Source</li> </ul>"},{"location":"tags/#react","title":"React","text":"<ul> <li>Gatsby</li> <li>React JS</li> </ul>"},{"location":"tags/#ssr","title":"SSR","text":"<ul> <li>Gatsby</li> </ul>"},{"location":"tags/#web","title":"Web","text":"<ul> <li>Gatsby</li> <li>Javascript</li> <li>Node JS</li> <li>React JS</li> </ul>"},{"location":"tags/#windows","title":"Windows","text":"<ul> <li>Windows</li> </ul>"},{"location":"tags/#advanced","title":"advanced","text":"<ul> <li>Computer Vision Part 3: Train U-Net Architecture to predict UI segmentation</li> </ul>"},{"location":"tags/#async","title":"async","text":"<ul> <li>Checking 48 Mountain Weather Locations at Once</li> </ul>"},{"location":"tags/#beginner","title":"beginner","text":"<ul> <li>Bottle + HTMX vs Streamlit</li> <li>Computer Vision Part 1: Loading Images with Python</li> <li>CTE vs Subquery: Who gives a \ud83e\udd86uck?</li> <li>Gitlab Workflow Project Management</li> <li>How I Edit Text Files (VS Code Settings)</li> <li>My Favorite Python One Liner Sucks</li> <li>Share a website to the world in less than a day (Free for Students!)</li> <li>3 Ways to Run Python Code</li> <li>Streamlit Full Stack Part 1: Python Web App in One File</li> <li>The Terminal (/ Shell / Command Line / Console)</li> <li>Time Series Data Part 1: What is a Time Series?</li> <li>Git</li> </ul>"},{"location":"tags/#curl","title":"curl","text":"<ul> <li>curl</li> </ul>"},{"location":"tags/#darts","title":"darts","text":"<ul> <li>Time Series Data Part 2: Darts and Streamlit</li> <li>Time Series Data Part 3: Custom Model Tuning</li> <li>Time Series Data Part 4: A Full Stack Use Case</li> </ul>"},{"location":"tags/#data","title":"data","text":"<ul> <li>CTE vs Subquery: Who gives a \ud83e\udd86uck?</li> <li>Holy \ud83e\udd86uck! Fast Analysis with DuckDB + Pyarrow</li> </ul>"},{"location":"tags/#deployment","title":"deployment","text":"<ul> <li>Share a website to the world in less than a day (Free for Students!)</li> </ul>"},{"location":"tags/#drivendata","title":"drivendata","text":"<ul> <li>LightGBM Predicting Water Pump Functionality</li> </ul>"},{"location":"tags/#fastapi","title":"fastapi","text":"<ul> <li>Pipreqs FastAPI Server + Github Bot</li> </ul>"},{"location":"tags/#frontend_1","title":"frontend","text":"<ul> <li>Frontend</li> </ul>"},{"location":"tags/#full-stack_1","title":"full-stack","text":"<ul> <li>Frontend</li> </ul>"},{"location":"tags/#general-coding","title":"general-coding","text":"<ul> <li>Gitlab Workflow Project Management</li> <li>How I Edit Text Files (VS Code Settings)</li> </ul>"},{"location":"tags/#git","title":"git","text":"<ul> <li>Gitlab Workflow Project Management</li> <li>Git</li> </ul>"},{"location":"tags/#guide","title":"guide","text":"<ul> <li>Computer Vision Part 1: Loading Images with Python</li> <li>Computer Vision Part 2: Load Tagged Data With Pytorch</li> <li>Computer Vision Part 3: Train U-Net Architecture to predict UI segmentation</li> <li>Gitlab Workflow Project Management</li> <li>Share a website to the world in less than a day (Free for Students!)</li> </ul>"},{"location":"tags/#habits","title":"habits","text":"<ul> <li>How I Edit Text Files (VS Code Settings)</li> </ul>"},{"location":"tags/#images","title":"images","text":"<ul> <li>Computer Vision Part 1: Loading Images with Python</li> </ul>"},{"location":"tags/#intermediate","title":"intermediate","text":"<ul> <li>Holy \ud83e\udd86uck! Fast Analysis with DuckDB + Pyarrow</li> <li>LightGBM Predicting Water Pump Functionality</li> <li>Pipreqs FastAPI Server + Github Bot</li> <li>Python Form Generator (JSON to Streamlit Form!)</li> <li>Time Series Data Part 3: Custom Model Tuning</li> <li>Time Series Data Part 4: A Full Stack Use Case</li> <li>Checking 48 Mountain Weather Locations at Once</li> </ul>"},{"location":"tags/#machine-learning_1","title":"machine-learning","text":"<ul> <li>Computer Vision Part 2: Load Tagged Data With Pytorch</li> <li>Computer Vision Part 3: Train U-Net Architecture to predict UI segmentation</li> </ul>"},{"location":"tags/#meta","title":"meta","text":"<ul> <li>Brain Dump 2022</li> </ul>"},{"location":"tags/#moderate","title":"moderate","text":"<ul> <li>Computer Vision Part 2: Load Tagged Data With Pytorch</li> </ul>"},{"location":"tags/#networks","title":"networks","text":"<ul> <li>Local Network</li> <li>Local Server</li> </ul>"},{"location":"tags/#opencv","title":"opencv","text":"<ul> <li>Computer Vision Part 1: Loading Images with Python</li> <li>Custom Data Tagging with Python OpenCV</li> </ul>"},{"location":"tags/#projects","title":"projects","text":"<ul> <li>Custom Data Tagging with Python OpenCV</li> </ul>"},{"location":"tags/#python","title":"python","text":"<ul> <li>Bottle + HTMX vs Streamlit</li> <li>Computer Vision Part 1: Loading Images with Python</li> <li>Computer Vision Part 2: Load Tagged Data With Pytorch</li> <li>Computer Vision Part 3: Train U-Net Architecture to predict UI segmentation</li> <li>Holy \ud83e\udd86uck! Fast Analysis with DuckDB + Pyarrow</li> <li>LightGBM Predicting Water Pump Functionality</li> <li>My Favorite Python One Liner Sucks</li> <li>Pipreqs FastAPI Server + Github Bot</li> <li>Python Form Generator (JSON to Streamlit Form!)</li> <li>3 Ways to Run Python Code</li> <li>Streamlit Full Stack Part 1: Python Web App in One File</li> <li>Time Series Data Part 1: What is a Time Series?</li> <li>Time Series Data Part 2: Darts and Streamlit</li> <li>Time Series Data Part 3: Custom Model Tuning</li> <li>Time Series Data Part 4: A Full Stack Use Case</li> <li>Custom Data Tagging with Python OpenCV</li> <li>Checking 48 Mountain Weather Locations at Once</li> </ul>"},{"location":"tags/#pytorch","title":"pytorch","text":"<ul> <li>Computer Vision Part 1: Loading Images with Python</li> <li>Computer Vision Part 2: Load Tagged Data With Pytorch</li> <li>Computer Vision Part 3: Train U-Net Architecture to predict UI segmentation</li> </ul>"},{"location":"tags/#resources","title":"resources","text":"<ul> <li>Time Series</li> </ul>"},{"location":"tags/#sql","title":"sql","text":"<ul> <li>CTE vs Subquery: Who gives a \ud83e\udd86uck?</li> </ul>"},{"location":"tags/#streamlit","title":"streamlit","text":"<ul> <li>LightGBM Predicting Water Pump Functionality</li> <li>Python Form Generator (JSON to Streamlit Form!)</li> <li>Streamlit Full Stack Part 1: Python Web App in One File</li> <li>Time Series Data Part 2: Darts and Streamlit</li> <li>Time Series Data Part 3: Custom Model Tuning</li> <li>Time Series Data Part 4: A Full Stack Use Case</li> <li>Checking 48 Mountain Weather Locations at Once</li> </ul>"},{"location":"tags/#time-series","title":"time-series","text":"<ul> <li>Time Series Data Part 2: Darts and Streamlit</li> <li>Time Series Data Part 4: A Full Stack Use Case</li> </ul>"},{"location":"tags/#timeseries","title":"timeseries","text":"<ul> <li>Time Series Data Part 1: What is a Time Series?</li> <li>Time Series Data Part 3: Custom Model Tuning</li> <li>Time Series</li> </ul>"},{"location":"tags/#vgac","title":"vgac","text":"<ul> <li>Custom Data Tagging with Python OpenCV</li> </ul>"},{"location":"tags/#web_1","title":"web","text":"<ul> <li>Frontend</li> </ul>"},{"location":"tags/#web-dev","title":"web-dev","text":"<ul> <li>Share a website to the world in less than a day (Free for Students!)</li> <li>Custom Data Tagging with Python OpenCV</li> </ul>"}]}